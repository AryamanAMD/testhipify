diff --git a/src/samples/Samples/0_Introduction/clock_nvrtc/clock_kernel.cu.hip b/src/samples/Samples/0_Introduction/clock_nvrtc/clock_kernel.cu.hip
index e69de29..0ef5507 100644
--- a/src/samples/Samples/0_Introduction/clock_nvrtc/clock_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/clock_nvrtc/clock_kernel.cu.hip
@@ -0,0 +1,76 @@
+
+#include <hip/hip_runtime.h>
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * This example shows how to use the clock function to measure the performance
+ * of block of threads of a kernel accurately. Blocks are executed in parallel
+ * and out of order. Since there's no synchronization mechanism between blocks,
+ * we measure the clock once for each block. The clock samples are written to
+ * device memory.
+ */
+
+// This kernel computes a standard parallel reduction and evaluates the
+// time it takes to do that for each block. The timing results are stored
+// in device memory.
+
+extern "C" __global__ void timedReduction(const float *input, float *output,
+                                          clock_t *timer) {
+  // __shared__ float shared[2 * blockDim.x];
+  extern __shared__ float shared[];
+
+  const int tid = threadIdx.x;
+  const int bid = blockIdx.x;
+
+  if (tid == 0) timer[bid] = clock();
+
+  // Copy input.
+  shared[tid] = input[tid];
+  shared[tid + blockDim.x] = input[tid + blockDim.x];
+
+  // Perform reduction to find minimum.
+  for (int d = blockDim.x; d > 0; d /= 2) {
+    __syncthreads();
+
+    if (tid < d) {
+      float f0 = shared[tid];
+      float f1 = shared[tid + d];
+
+      if (f1 < f0) {
+        shared[tid] = f1;
+      }
+    }
+  }
+
+  // Write result.
+  if (tid == 0) output[bid] = shared[0];
+
+  __syncthreads();
+
+  if (tid == 0) timer[bid + gridDim.x] = clock();
+}
diff --git a/src/samples/Samples/0_Introduction/fp16ScalarProduct/fp16ScalarProduct.cu.hip b/src/samples/Samples/0_Introduction/fp16ScalarProduct/fp16ScalarProduct.cu.hip
index d97ec63..e69de29 100644
--- a/src/samples/Samples/0_Introduction/fp16ScalarProduct/fp16ScalarProduct.cu.hip
+++ b/src/samples/Samples/0_Introduction/fp16ScalarProduct/fp16ScalarProduct.cu.hip
@@ -1,217 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include "hip/hip_fp16.h"
-#include "helper_cuda.h"
-
-#include <cstdio>
-#include <cstdlib>
-#include <ctime>
-
-#define NUM_OF_BLOCKS 128
-#define NUM_OF_THREADS 128
-
-__forceinline__ __device__ void reduceInShared_intrinsics(half2 *const v) {
-  if (threadIdx.x < 64)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 64]);
-  __syncthreads();
-  if (threadIdx.x < 32)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 32]);
-  __syncthreads();
-  if (threadIdx.x < 16)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 16]);
-  __syncthreads();
-  if (threadIdx.x < 8)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 8]);
-  __syncthreads();
-  if (threadIdx.x < 4)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 4]);
-  __syncthreads();
-  if (threadIdx.x < 2)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 2]);
-  __syncthreads();
-  if (threadIdx.x < 1)
-    v[threadIdx.x] = __hadd2(v[threadIdx.x], v[threadIdx.x + 1]);
-  __syncthreads();
-}
-
-__forceinline__ __device__ void reduceInShared_native(half2 *const v) {
-  if (threadIdx.x < 64) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 64];
-  __syncthreads();
-  if (threadIdx.x < 32) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 32];
-  __syncthreads();
-  if (threadIdx.x < 16) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 16];
-  __syncthreads();
-  if (threadIdx.x < 8) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 8];
-  __syncthreads();
-  if (threadIdx.x < 4) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 4];
-  __syncthreads();
-  if (threadIdx.x < 2) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 2];
-  __syncthreads();
-  if (threadIdx.x < 1) v[threadIdx.x] = v[threadIdx.x] + v[threadIdx.x + 1];
-  __syncthreads();
-}
-
-__global__ void scalarProductKernel_intrinsics(half2 const *const a,
-                                               half2 const *const b,
-                                               float *const results,
-                                               size_t const size) {
-  const int stride = gridDim.x * blockDim.x;
-  __shared__ half2 shArray[NUM_OF_THREADS];
-
-  shArray[threadIdx.x] = __float2half2_rn(0.f);
-  half2 value = __float2half2_rn(0.f);
-
-  for (int i = threadIdx.x + blockDim.x + blockIdx.x; i < size; i += stride) {
-    value = __hfma2(a[i], b[i], value);
-  }
-
-  shArray[threadIdx.x] = value;
-  __syncthreads();
-  reduceInShared_intrinsics(shArray);
-
-  if (threadIdx.x == 0) {
-    half2 result = shArray[0];
-    float f_result = __low2float(result) + __high2float(result);
-    results[blockIdx.x] = f_result;
-  }
-}
-
-__global__ void scalarProductKernel_native(half2 const *const a,
-                                           half2 const *const b,
-                                           float *const results,
-                                           size_t const size) {
-  const int stride = gridDim.x * blockDim.x;
-  __shared__ half2 shArray[NUM_OF_THREADS];
-
-  half2 value(0.f, 0.f);
-  shArray[threadIdx.x] = value;
-
-  for (int i = threadIdx.x + blockDim.x + blockIdx.x; i < size; i += stride) {
-    value = a[i] * b[i] + value;
-  }
-
-  shArray[threadIdx.x] = value;
-  __syncthreads();
-  reduceInShared_native(shArray);
-
-  if (threadIdx.x == 0) {
-    half2 result = shArray[0];
-    float f_result = (float)result.y + (float)result.x;
-    results[blockIdx.x] = f_result;
-  }
-}
-
-void generateInput(half2 *a, size_t size) {
-  for (size_t i = 0; i < size; ++i) {
-    half2 temp;
-    temp.x = static_cast<float>(rand() % 4);
-    temp.y = static_cast<float>(rand() % 2);
-    a[i] = temp;
-  }
-}
-
-int main(int argc, char *argv[]) {
-  srand((unsigned int)time(NULL));
-  size_t size = NUM_OF_BLOCKS * NUM_OF_THREADS * 16;
-
-  half2 *vec[2];
-  half2 *devVec[2];
-
-  float *results;
-  float *devResults;
-
-  int devID = findCudaDevice(argc, (const char **)argv);
-
-  hipDeviceProp_t devProp;
-  HIPCHECK(hipGetDeviceProperties(&devProp, devID));
-
-  if (devProp.major < 5 || (devProp.major == 5 && devProp.minor < 3)) {
-    printf(
-        "ERROR: fp16ScalarProduct requires GPU devices with compute SM 5.3 or "
-        "higher.\n");
-    return EXIT_WAIVED;
-  }
-
-  for (int i = 0; i < 2; ++i) {
-    HIPCHECK(hipHostMalloc((void **)&vec[i], size * sizeof *vec[i]));
-    HIPCHECK(hipMalloc((void **)&devVec[i], size * sizeof *devVec[i]));
-  }
-
-  HIPCHECK(
-      hipHostMalloc((void **)&results, NUM_OF_BLOCKS * sizeof *results));
-  HIPCHECK(
-      hipMalloc((void **)&devResults, NUM_OF_BLOCKS * sizeof *devResults));
-
-  for (int i = 0; i < 2; ++i) {
-    generateInput(vec[i], size);
-    HIPCHECK(hipMemcpy(devVec[i], vec[i], size * sizeof *vec[i],
-                               hipMemcpyHostToDevice));
-  }
-
-  scalarProductKernel_native<<<NUM_OF_BLOCKS, NUM_OF_THREADS>>>(
-      devVec[0], devVec[1], devResults, size);
-
-  HIPCHECK(hipMemcpy(results, devResults,
-                             NUM_OF_BLOCKS * sizeof *results,
-                             hipMemcpyDeviceToHost));
-
-  float result_native = 0;
-  for (int i = 0; i < NUM_OF_BLOCKS; ++i) {
-    result_native += results[i];
-  }
-  printf("Result native operators\t: %f \n", result_native);
-
-  scalarProductKernel_intrinsics<<<NUM_OF_BLOCKS, NUM_OF_THREADS>>>(
-      devVec[0], devVec[1], devResults, size);
-
-  HIPCHECK(hipMemcpy(results, devResults,
-                             NUM_OF_BLOCKS * sizeof *results,
-                             hipMemcpyDeviceToHost));
-
-  float result_intrinsics = 0;
-  for (int i = 0; i < NUM_OF_BLOCKS; ++i) {
-    result_intrinsics += results[i];
-  }
-  printf("Result intrinsics\t: %f \n", result_intrinsics);
-
-  printf("&&&& fp16ScalarProduct %s\n",
-         (fabs(result_intrinsics - result_native) < 0.00001) ? "PASSED"
-                                                             : "FAILED");
-
-  for (int i = 0; i < 2; ++i) {
-    HIPCHECK(hipFree(devVec[i]));
-    HIPCHECK(hipHostFree(vec[i]));
-  }
-
-  HIPCHECK(hipFree(devResults));
-  HIPCHECK(hipHostFree(results));
-
-  return EXIT_SUCCESS;
-}
diff --git a/src/samples/Samples/0_Introduction/matrixMul/matrixMul.cu.hip b/src/samples/Samples/0_Introduction/matrixMul/matrixMul.cu.hip
index 575d702..e69de29 100644
--- a/src/samples/Samples/0_Introduction/matrixMul/matrixMul.cu.hip
+++ b/src/samples/Samples/0_Introduction/matrixMul/matrixMul.cu.hip
@@ -1,355 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/**
- * Matrix multiplication: C = A * B.
- * Host code.
- *
- * This sample implements matrix multiplication which makes use of shared memory
- * to ensure data reuse, the matrix multiplication is done using tiling approach.
- * It has been written for clarity of exposition to illustrate various CUDA programming
- * principles, not with the goal of providing the most performant generic kernel for matrix multiplication.
- * See also:
- * V. Volkov and J. Demmel, "Benchmarking GPUs to tune dense linear algebra,"
- * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),
- * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.
- */
-
-// System includes
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <assert.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-#include <hip/hip_runtime_api.h>
-
-// Helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-/**
- * Matrix multiplication (CUDA Kernel) on the device: C = A * B
- * wA is A's width and wB is B's width
- */
-template <int BLOCK_SIZE> __global__ void MatrixMulCUDA(float *C, float *A,
-    float *B, int wA,
-    int wB) {
-  // Block index
-  int bx = blockIdx.x;
-  int by = blockIdx.y;
-
-  // Thread index
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-
-  // Index of the first sub-matrix of A processed by the block
-  int aBegin = wA * BLOCK_SIZE * by;
-
-  // Index of the last sub-matrix of A processed by the block
-  int aEnd   = aBegin + wA - 1;
-
-  // Step size used to iterate through the sub-matrices of A
-  int aStep  = BLOCK_SIZE;
-
-  // Index of the first sub-matrix of B processed by the block
-  int bBegin = BLOCK_SIZE * bx;
-
-  // Step size used to iterate through the sub-matrices of B
-  int bStep  = BLOCK_SIZE * wB;
-
-  // Csub is used to store the element of the block sub-matrix
-  // that is computed by the thread
-  float Csub = 0;
-
-  // Loop over all the sub-matrices of A and B
-  // required to compute the block sub-matrix
-  for (int a = aBegin, b = bBegin;
-       a <= aEnd;
-       a += aStep, b += bStep) {
-    // Declaration of the shared memory array As used to
-    // store the sub-matrix of A
-    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
-
-    // Declaration of the shared memory array Bs used to
-    // store the sub-matrix of B
-    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
-
-    // Load the matrices from device memory
-    // to shared memory; each thread loads
-    // one element of each matrix
-    As[ty][tx] = A[a + wA * ty + tx];
-    Bs[ty][tx] = B[b + wB * ty + tx];
-
-    // Synchronize to make sure the matrices are loaded
-    __syncthreads();
-
-    // Multiply the two matrices together;
-    // each thread computes one element
-    // of the block sub-matrix
-#pragma unroll
-
-    for (int k = 0; k < BLOCK_SIZE; ++k) {
-      Csub += As[ty][k] * Bs[k][tx];
-    }
-
-    // Synchronize to make sure that the preceding
-    // computation is done before loading two new
-    // sub-matrices of A and B in the next iteration
-    __syncthreads();
-  }
-
-  // Write the block sub-matrix to device memory;
-  // each thread writes one element
-  int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
-  C[c + wB * ty + tx] = Csub;
-}
-
-void ConstantInit(float *data, int size, float val) {
-  for (int i = 0; i < size; ++i) {
-    data[i] = val;
-  }
-}
-
-/**
- * Run a simple test of matrix multiplication using CUDA
- */
-int MatrixMultiply(int argc, char **argv,
-                   int block_size, const dim3 &dimsA,
-                   const dim3 &dimsB) {
-  // Allocate host memory for matrices A and B
-  unsigned int size_A = dimsA.x * dimsA.y;
-  unsigned int mem_size_A = sizeof(float) * size_A;
-  float *h_A;
-  HIPCHECK(hipHostMalloc(&h_A, mem_size_A));
-  unsigned int size_B = dimsB.x * dimsB.y;
-  unsigned int mem_size_B = sizeof(float) * size_B;
-  float *h_B;
-  HIPCHECK(hipHostMalloc(&h_B, mem_size_B));
-  hipStream_t stream;
-
-  // Initialize host memory
-  const float valB = 0.01f;
-  ConstantInit(h_A, size_A, 1.0f);
-  ConstantInit(h_B, size_B, valB);
-
-  // Allocate device memory
-  float *d_A, *d_B, *d_C;
-
-  // Allocate host matrix C
-  dim3 dimsC(dimsB.x, dimsA.y, 1);
-  unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);
-  float *h_C;
-  HIPCHECK(hipHostMalloc(&h_C, mem_size_C));
-
-  if (h_C == NULL) {
-    fprintf(stderr, "Failed to allocate host matrix C!\n");
-    exit(EXIT_FAILURE);
-  }
-
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));
-  // Allocate CUDA events that we'll use for timing
-  hipEvent_t start, stop;
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  // copy host memory to device
-  HIPCHECK(
-      hipMemcpyAsync(d_A, h_A, mem_size_A, hipMemcpyHostToDevice, stream));
-  HIPCHECK(
-      hipMemcpyAsync(d_B, h_B, mem_size_B, hipMemcpyHostToDevice, stream));
-
-  // Setup execution parameters
-  dim3 threads(block_size, block_size);
-  dim3 grid(dimsB.x / threads.x, dimsA.y / threads.y);
-
-  // Create and start timer
-  printf("Computing result using CUDA Kernel...\n");
-
-  // Performs warmup operation using matrixMul CUDA kernel
-  if (block_size == 16) {
-    MatrixMulCUDA<16>
-        <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);
-  } else {
-    MatrixMulCUDA<32>
-        <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);
-  }
-
-  printf("done\n");
-  HIPCHECK(hipStreamSynchronize(stream));
-
-  // Record the start event
-  HIPCHECK(hipEventRecord(start, stream));
-
-  // Execute the kernel
-  int nIter = 300;
-
-  for (int j = 0; j < nIter; j++) {
-    if (block_size == 16) {
-      MatrixMulCUDA<16>
-          <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);
-    } else {
-      MatrixMulCUDA<32>
-          <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);
-    }
-  }
-
-  // Record the stop event
-  HIPCHECK(hipEventRecord(stop, stream));
-
-  // Wait for the stop event to complete
-  HIPCHECK(hipEventSynchronize(stop));
-
-  float msecTotal = 0.0f;
-  HIPCHECK(hipEventElapsedTime(&msecTotal, start, stop));
-
-  // Compute and print the performance
-  float msecPerMatrixMul = msecTotal / nIter;
-  double flopsPerMatrixMul = 2.0 * static_cast<double>(dimsA.x) *
-                             static_cast<double>(dimsA.y) *
-                             static_cast<double>(dimsB.x);
-  double gigaFlops =
-      (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul / 1000.0f);
-  printf(
-      "Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,"
-      " WorkgroupSize= %u threads/block\n",
-      gigaFlops, msecPerMatrixMul, flopsPerMatrixMul, threads.x * threads.y);
-
-  // Copy result from device to host
-  HIPCHECK(
-      hipMemcpyAsync(h_C, d_C, mem_size_C, hipMemcpyDeviceToHost, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-
-  printf("Checking computed result for correctness: ");
-  bool correct = true;
-
-  // test relative error by the formula
-  //     |<x, y>_cpu - <x,y>_gpu|/<|x|, |y|>  < eps
-  double eps = 1.e-6;  // machine zero
-
-  for (int i = 0; i < static_cast<int>(dimsC.x * dimsC.y); i++) {
-    double abs_err = fabs(h_C[i] - (dimsA.x * valB));
-    double dot_length = dimsA.x;
-    double abs_val = fabs(h_C[i]);
-    double rel_err = abs_err / abs_val / dot_length;
-
-    if (rel_err > eps) {
-      printf("Error! Matrix[%05d]=%.8f, ref=%.8f error term is > %E\n",
-             i, h_C[i], dimsA.x * valB, eps);
-      correct = false;
-    }
-  }
-
-  printf("%s\n", correct ? "Result = PASS" : "Result = FAIL");
-
-  // Clean up memory
-  HIPCHECK(hipHostFree(h_A));
-  HIPCHECK(hipHostFree(h_B));
-  HIPCHECK(hipHostFree(h_C));
-  HIPCHECK(hipFree(d_A));
-  HIPCHECK(hipFree(d_B));
-  HIPCHECK(hipFree(d_C));
-  HIPCHECK(hipEventDestroy(start));
-  HIPCHECK(hipEventDestroy(stop));
-  printf(
-      "\nNOTE: The CUDA Samples are not meant for performance "
-      "measurements. Results may vary when GPU Boost is enabled.\n");
-
-  if (correct) {
-    return EXIT_SUCCESS;
-  } else {
-    return EXIT_FAILURE;
-  }
-}
-
-
-/**
- * Program main
- */
-int main(int argc, char **argv) {
-  printf("[Matrix Multiply Using CUDA] - Starting...\n");
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
-      checkCmdLineFlag(argc, (const char **)argv, "?")) {
-    printf("Usage -device=n (n >= 0 for deviceID)\n");
-    printf("      -wA=WidthA -hA=HeightA (Width x Height of Matrix A)\n");
-    printf("      -wB=WidthB -hB=HeightB (Width x Height of Matrix B)\n");
-    printf("  Note: Outer matrix dimensions of A & B matrices" \
-           " must be equal.\n");
-
-    exit(EXIT_SUCCESS);
-  }
-
-  // This will pick the best possible CUDA capable device, otherwise
-  // override the device ID based on input provided at the command line
-  int dev = findCudaDevice(argc, (const char **)argv);
-
-  int block_size = 32;
-
-  dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);
-  dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);
-
-  // width of Matrix A
-  if (checkCmdLineFlag(argc, (const char **)argv, "wA")) {
-    dimsA.x = getCmdLineArgumentInt(argc, (const char **)argv, "wA");
-  }
-
-  // height of Matrix A
-  if (checkCmdLineFlag(argc, (const char **)argv, "hA")) {
-    dimsA.y = getCmdLineArgumentInt(argc, (const char **)argv, "hA");
-  }
-
-  // width of Matrix B
-  if (checkCmdLineFlag(argc, (const char **)argv, "wB")) {
-    dimsB.x = getCmdLineArgumentInt(argc, (const char **)argv, "wB");
-  }
-
-  // height of Matrix B
-  if (checkCmdLineFlag(argc, (const char **)argv, "hB")) {
-    dimsB.y = getCmdLineArgumentInt(argc, (const char **)argv, "hB");
-  }
-
-  if (dimsA.x != dimsB.y) {
-    printf("Error: outer matrix dimensions must be equal. (%d != %d)\n",
-           dimsA.x, dimsB.y);
-    exit(EXIT_FAILURE);
-  }
-
-  printf("MatrixA(%d,%d), MatrixB(%d,%d)\n", dimsA.x, dimsA.y,
-         dimsB.x, dimsB.y);
-
-  HIPCHECK(rocprofiler_start());
-  int matrix_result = MatrixMultiply(argc, argv, block_size, dimsA, dimsB);
-  HIPCHECK(rocprofiler_stop());
-
-  exit(matrix_result);
-}
diff --git a/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip b/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip
index e69de29..fdcb144 100644
--- a/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/matrixMulDrv/matrixMul_kernel.cu.hip
@@ -0,0 +1,129 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Matrix multiplication: C = A * B.
+ * Device code.
+ */
+
+#ifndef _MATRIXMUL_KERNEL_H_
+#define _MATRIXMUL_KERNEL_H_
+
+#include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
+
+#define AS(i, j) As[i][j]
+#define BS(i, j) Bs[i][j]
+
+////////////////////////////////////////////////////////////////////////////////
+//! Matrix multiplication on the device: C = A * B
+//! wA is A's width and wB is B's width
+////////////////////////////////////////////////////////////////////////////////
+template <int block_size, typename size_type>
+__device__ void matrixMul(float *C, float *A, float *B, size_type wA,
+                          size_type wB) {
+  // Block index
+  size_type bx = blockIdx.x;
+  size_type by = blockIdx.y;
+
+  // Thread index
+  size_type tx = threadIdx.x;
+  size_type ty = threadIdx.y;
+
+  // Index of the first sub-matrix of A processed by the block
+  size_type aBegin = wA * block_size * by;
+
+  // Index of the last sub-matrix of A processed by the block
+  size_type aEnd = aBegin + wA - 1;
+
+  // Step size used to iterate through the sub-matrices of A
+  size_type aStep = block_size;
+
+  // Index of the first sub-matrix of B processed by the block
+  size_type bBegin = block_size * bx;
+
+  // Step size used to iterate through the sub-matrices of B
+  size_type bStep = block_size * wB;
+
+  // Csub is used to store the element of the block sub-matrix
+  // that is computed by the thread
+  float Csub = 0;
+
+  // Loop over all the sub-matrices of A and B
+  // required to compute the block sub-matrix
+  for (size_type a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
+    // Declaration of the shared memory array As used to
+    // store the sub-matrix of A
+    __shared__ float As[block_size][block_size];
+
+    // Declaration of the shared memory array Bs used to
+    // store the sub-matrix of B
+    __shared__ float Bs[block_size][block_size];
+
+    // Load the matrices from device memory
+    // to shared memory; each thread loads
+    // one element of each matrix
+    AS(ty, tx) = A[a + wA * ty + tx];
+    BS(ty, tx) = B[b + wB * ty + tx];
+
+    // Synchronize to make sure the matrices are loaded
+    __syncthreads();
+
+    // Multiply the two matrices together;
+    // each thread computes one element
+    // of the block sub-matrix
+#pragma unroll
+
+    for (size_type k = 0; k < block_size; ++k) Csub += AS(ty, k) * BS(k, tx);
+
+    // Synchronize to make sure that the preceding
+    // computation is done before loading two new
+    // sub-matrices of A and B in the next iteration
+    __syncthreads();
+  }
+
+  // Write the block sub-matrix to device memory;
+  // each thread writes one element
+  size_type c = wB * block_size * by + block_size * bx;
+  C[c + wB * ty + tx] = Csub;
+}
+
+// C wrappers around our template kernel
+extern "C" __global__ void matrixMul_bs8_64bit(float *C, float *A, float *B,
+                                               size_t wA, size_t wB) {
+  matrixMul<8, size_t>(C, A, B, wA, wB);
+}
+extern "C" __global__ void matrixMul_bs16_64bit(float *C, float *A, float *B,
+                                                size_t wA, size_t wB) {
+  matrixMul<16, size_t>(C, A, B, wA, wB);
+}
+extern "C" __global__ void matrixMul_bs32_64bit(float *C, float *A, float *B,
+                                                size_t wA, size_t wB) {
+  matrixMul<32, size_t>(C, A, B, wA, wB);
+}
+
+#endif  // #ifndef _MATRIXMUL_KERNEL_H_
diff --git a/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip b/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip
index 7aa112f..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleAttributes/simpleAttributes.cu.hip
@@ -1,216 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-// includes CUDA
-#include <hip/hip_runtime.h>
-
-// includes, project
-#include <helper_cuda.h>
-#include <helper_functions.h>  // helper functions for SDK examples
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-void runTest(int argc, char **argv);
-
-hipAccessPolicyWindow initAccessPolicyWindow(void) {
-  hipAccessPolicyWindow accessPolicyWindow = {0};
-  accessPolicyWindow.base_ptr = (void *)0;
-  accessPolicyWindow.num_bytes = 0;
-  accessPolicyWindow.hitRatio = 0.f;
-  accessPolicyWindow.hitProp = hipAccessPropertyNormal;
-  accessPolicyWindow.missProp = hipAccessPropertyStreaming;
-  return accessPolicyWindow;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Simple test kernel for device functionality
-//! @param data  input data in global memory
-//! @param dataSize  input data size
-//! @param bigData  input bigData in global memory
-//! @param bigDataSize  input bigData size
-//! @param hitcount how many data access are done within block
-////////////////////////////////////////////////////////////////////////////////
-static __global__ void kernCacheSegmentTest(int *data, int dataSize, int *trash,
-                                            int bigDataSize, int hitCount) {
-  __shared__ unsigned int hit;
-  int row = blockIdx.y * blockDim.y + threadIdx.y;
-  int col = blockIdx.x * blockDim.x + threadIdx.x;
-  int tID = row * blockDim.y + col;
-  uint32_t psRand = tID;
-
-  atomicExch(&hit, 0);
-  __syncthreads();
-  while (hit < hitCount) {
-    psRand ^= psRand << 13;
-    psRand ^= psRand >> 17;
-    psRand ^= psRand << 5;
-
-    int idx = tID - psRand;
-    if (idx < 0) {
-      idx = -idx;
-    }
-
-    if ((tID % 2) == 0) {
-      data[psRand % dataSize] = data[psRand % dataSize] + data[idx % dataSize];
-    } else {
-      trash[psRand % bigDataSize] =
-          trash[psRand % bigDataSize] + trash[idx % bigDataSize];
-    }
-
-    atomicAdd(&hit, 1);
-  }
-}
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) { runTest(argc, argv); }
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run a simple test for CUDA
-////////////////////////////////////////////////////////////////////////////////
-void runTest(int argc, char **argv) {
-  bool bTestResult = true;
-  hipAccessPolicyWindow accessPolicyWindow;
-  hipDeviceProp_t deviceProp;
-  cudaStreamAttrValue streamAttrValue;
-  hipStream_t stream;
-  cudaStreamAttrID streamAttrID;
-  dim3 threads(32, 32);
-  int *dataDevicePointer;
-  int *dataHostPointer;
-  int dataSize;
-  int *bigDataDevicePointer;
-  int *bigDataHostPointer;
-  int bigDataSize;
-  StopWatchInterface *timer = 0;
-
-  printf("%s Starting...\n\n", argv[0]);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  int devID = findCudaDevice(argc, (const char **)argv);
-  sdkCreateTimer(&timer);
-  sdkStartTimer(&timer);
-  // Get device properties
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
-  dim3 blocks(deviceProp.maxGridSize[1], 1);
-
-  // Make sure device the l2 optimization
-  if (deviceProp.persistingL2CacheMaxSize == 0) {
-    printf(
-        "Waiving execution as device %d does not support persisting L2 "
-        "Caching\n",
-        devID);
-    exit(EXIT_WAIVED);
-  }
-
-  // Create stream to assiocate with window
-  HIPCHECK(hipStreamCreate(&stream));
-
-  // Set the amount of l2 cache that will be persisting to maximum the device
-  // can support
-  HIPCHECK(cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize,
-                                     deviceProp.persistingL2CacheMaxSize));
-
-  // Stream attribute to set
-  streamAttrID = cudaStreamAttributeAccessPolicyWindow;
-
-  // Default window
-  streamAttrValue.accessPolicyWindow = initAccessPolicyWindow();
-  accessPolicyWindow = initAccessPolicyWindow();
-
-  // Allocate size of both buffers
-  bigDataSize = (deviceProp.l2CacheSize * 4) / sizeof(int);
-  dataSize = (deviceProp.l2CacheSize / 4) / sizeof(int);
-
-  // Allocate data
-  HIPCHECK(hipHostMalloc(&dataHostPointer, dataSize * sizeof(int)));
-  HIPCHECK(
-      hipHostMalloc(&bigDataHostPointer, bigDataSize * sizeof(int)));
-
-  for (int i = 0; i < bigDataSize; ++i) {
-    if (i < dataSize) {
-      dataHostPointer[i] = i;
-    }
-
-    bigDataHostPointer[bigDataSize - i - 1] = i;
-  }
-
-  HIPCHECK(
-      hipMalloc((void **)&dataDevicePointer, dataSize * sizeof(int)));
-  HIPCHECK(
-      hipMalloc((void **)&bigDataDevicePointer, bigDataSize * sizeof(int)));
-  HIPCHECK(hipMemcpyAsync(dataDevicePointer, dataHostPointer,
-                                  dataSize * sizeof(int),
-                                  hipMemcpyHostToDevice, stream));
-  HIPCHECK(hipMemcpyAsync(bigDataDevicePointer, bigDataHostPointer,
-                                  bigDataSize * sizeof(int),
-                                  hipMemcpyHostToDevice, stream));
-
-  // Make a window for the buffer of interest
-  accessPolicyWindow.base_ptr = (void *)dataDevicePointer;
-  accessPolicyWindow.num_bytes = dataSize * sizeof(int);
-  accessPolicyWindow.hitRatio = 1.f;
-  accessPolicyWindow.hitProp = hipAccessPropertyPersisting;
-  accessPolicyWindow.missProp = hipAccessPropertyNormal;
-  streamAttrValue.accessPolicyWindow = accessPolicyWindow;
-
-  // Assign window to stream
-  HIPCHECK(
-      cudaStreamSetAttribute(stream, streamAttrID, &streamAttrValue));
-
-  // Demote any previous persisting lines
-  HIPCHECK(cudaCtxResetPersistingL2Cache());
-
-  HIPCHECK(hipStreamSynchronize(stream));
-  kernCacheSegmentTest<<<blocks, threads, 0, stream>>>(
-      dataDevicePointer, dataSize, bigDataDevicePointer, bigDataSize, 0xAFFFF);
-
-  HIPCHECK(hipStreamSynchronize(stream));
-  // check if kernel execution generated and error
-  getLastCudaError("Kernel execution failed");
-
-  // Free memory
-  HIPCHECK(hipHostFree(dataHostPointer));
-  HIPCHECK(hipHostFree(bigDataHostPointer));
-  HIPCHECK(hipFree(dataDevicePointer));
-  HIPCHECK(hipFree(bigDataDevicePointer));
-
-  sdkStopTimer(&timer);
-  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
-  sdkDeleteTimer(&timer);
-
-  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
index 0c4126e..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCUDA2GL/simpleCUDA2GL.cu.hip
@@ -1,64 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// Utilities and system includes
-
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-
-// clamp x to range [a, b]
-__device__ float clamp(float x, float a, float b) { return max(a, min(b, x)); }
-
-__device__ int clamp(int x, int a, int b) { return max(a, min(b, x)); }
-
-// convert floating point rgb color to 8-bit integer
-__device__ int rgbToInt(float r, float g, float b) {
-  r = clamp(r, 0.0f, 255.0f);
-  g = clamp(g, 0.0f, 255.0f);
-  b = clamp(b, 0.0f, 255.0f);
-  return (int(b) << 16) | (int(g) << 8) | int(r);
-}
-
-__global__ void cudaProcess(unsigned int *g_odata, int imgw) {
-  extern __shared__ uchar4 sdata[];
-
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-  int bw = blockDim.x;
-  int bh = blockDim.y;
-  int x = blockIdx.x * bw + tx;
-  int y = blockIdx.y * bh + ty;
-
-  uchar4 c4 = make_uchar4((x & 0x20) ? 100 : 0, 0, (y & 0x20) ? 100 : 0, 0);
-  g_odata[y * imgw + x] = rgbToInt(c4.z, c4.y, c4.x);
-}
-
-extern "C" void launch_cudaProcess(dim3 grid, dim3 block, int sbytes,
-                                   unsigned int *g_odata, int imgw) {
-  cudaProcess<<<grid, block, sbytes>>>(g_odata, imgw);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip b/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip
index 5a730b6..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleCubemapTexture/simpleCubemapTexture.cu.hip
@@ -1,270 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
-* This sample demonstrates how to use texture fetches from layered 2D textures
-* in CUDA C
-*
-* This sample first generates a 3D input data array for the layered texture
-* and the expected output. Then it starts CUDA C kernels, one for each layer,
-* which fetch their layer's texture data (using normalized texture coordinates)
-* transform it to the expected output, and write it to a 3D output data array.
-*/
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-// includes CUDA
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-static const char *sSDKname = "simpleCubemapTexture";
-
-// includes, kernels
-
-////////////////////////////////////////////////////////////////////////////////
-//! Transform a cubemap face of a linear buffe using cubemap texture lookups
-//! @param g_odata  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-__global__ void transformKernel(float *g_odata, int width,
-                                hipTextureObject_t tex) {
-  // calculate this thread's data point
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  // 0.5f offset and division are necessary to access the original data points
-  // in the texture (such that bilinear interpolation will not be activated).
-  // For details, see also CUDA Programming Guide, Appendix D
-
-  float u = ((x + 0.5f) / (float)width) * 2.f - 1.f;
-  float v = ((y + 0.5f) / (float)width) * 2.f - 1.f;
-
-  float cx, cy, cz;
-
-  for (unsigned int face = 0; face < 6; face++) {
-    // Layer 0 is positive X face
-    if (face == 0) {
-      cx = 1;
-      cy = -v;
-      cz = -u;
-    }
-    // Layer 1 is negative X face
-    else if (face == 1) {
-      cx = -1;
-      cy = -v;
-      cz = u;
-    }
-    // Layer 2 is positive Y face
-    else if (face == 2) {
-      cx = u;
-      cy = 1;
-      cz = v;
-    }
-    // Layer 3 is negative Y face
-    else if (face == 3) {
-      cx = u;
-      cy = -1;
-      cz = -v;
-    }
-    // Layer 4 is positive Z face
-    else if (face == 4) {
-      cx = u;
-      cy = -v;
-      cz = 1;
-    }
-    // Layer 4 is negative Z face
-    else if (face == 5) {
-      cx = -u;
-      cy = -v;
-      cz = -1;
-    }
-
-    // read from texture, do expected transformation and write to global memory
-    g_odata[face * width * width + y * width + x] =
-        -texCubemap<float>(tex, cx, cy, cz);
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  int devID = findCudaDevice(argc, (const char **)argv);
-
-  bool bResult = true;
-
-  // get number of SMs on this GPU
-  hipDeviceProp_t deviceProps;
-
-  HIPCHECK(hipGetDeviceProperties(&deviceProps, devID));
-  printf("CUDA device [%s] has %d Multi-Processors ", deviceProps.name,
-         deviceProps.multiProcessorCount);
-  printf("SM %d.%d\n", deviceProps.major, deviceProps.minor);
-
-  if (deviceProps.major < 2) {
-    printf(
-        "%s requires SM 2.0 or higher for support of Texture Arrays.  Test "
-        "will exit... \n",
-        sSDKname);
-
-    exit(EXIT_WAIVED);
-  }
-
-  // generate input data for layered texture
-  unsigned int width = 64, num_faces = 6, num_layers = 1;
-  unsigned int cubemap_size = width * width * num_faces;
-  unsigned int size = cubemap_size * num_layers * sizeof(float);
-  float *h_data = (float *)malloc(size);
-
-  for (int i = 0; i < (int)(cubemap_size * num_layers); i++) {
-    h_data[i] = (float)i;
-  }
-
-  // this is the expected transformation of the input data (the expected output)
-  float *h_data_ref = (float *)malloc(size);
-
-  for (unsigned int layer = 0; layer < num_layers; layer++) {
-    for (int i = 0; i < (int)(cubemap_size); i++) {
-      h_data_ref[layer * cubemap_size + i] =
-          -h_data[layer * cubemap_size + i] + layer;
-    }
-  }
-
-  // allocate device memory for result
-  float *d_data = NULL;
-  HIPCHECK(hipMalloc((void **)&d_data, size));
-
-  // allocate array and copy image data
-  hipChannelFormatDesc channelDesc =
-      hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
-  hipArray *cu_3darray;
-  //    HIPCHECK(cudaMalloc3DArray( &cu_3darray, &channelDesc,
-  //    make_cudaExtent(width, height, num_layers), cudaArrayLayered ));
-  HIPCHECK(hipMalloc3DArray(&cu_3darray, &channelDesc,
-                                    make_hipExtent(width, width, num_faces),
-                                    hipArrayCubemap));
-  hipMemcpy3DParms myparms = {0};
-  myparms.srcPos = make_hipPos(0, 0, 0);
-  myparms.dstPos = make_hipPos(0, 0, 0);
-  myparms.srcPtr =
-      make_hipPitchedPtr(h_data, width * sizeof(float), width, width);
-  myparms.dstArray = cu_3darray;
-  myparms.extent = make_hipExtent(width, width, num_faces);
-  myparms.kind = hipMemcpyHostToDevice;
-  HIPCHECK(hipMemcpy3D(&myparms));
-
-  hipTextureObject_t tex;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = cu_3darray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = true;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.addressMode[2] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
-
-  dim3 dimBlock(8, 8, 1);
-  dim3 dimGrid(width / dimBlock.x, width / dimBlock.y, 1);
-
-  printf(
-      "Covering Cubemap data array of %d~3 x %d: Grid size is %d x %d, each "
-      "block has 8 x 8 threads\n",
-      width, num_layers, dimGrid.x, dimGrid.y);
-
-  transformKernel<<<dimGrid, dimBlock>>>(d_data, width,
-                                         tex);  // warmup (for better timing)
-
-  // check if kernel execution generated an error
-  getLastCudaError("warmup Kernel execution failed");
-
-  HIPCHECK(hipDeviceSynchronize());
-
-  StopWatchInterface *timer = NULL;
-  sdkCreateTimer(&timer);
-  sdkStartTimer(&timer);
-
-  // execute the kernel
-  transformKernel<<<dimGrid, dimBlock, 0>>>(d_data, width, tex);
-
-  // check if kernel execution generated an error
-  getLastCudaError("Kernel execution failed");
-
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&timer);
-  printf("Processing time: %.3f msec\n", sdkGetTimerValue(&timer));
-  printf("%.2f Mtexlookups/sec\n",
-         (cubemap_size / (sdkGetTimerValue(&timer) / 1000.0f) / 1e6));
-  sdkDeleteTimer(&timer);
-
-  // allocate mem for the result on host side
-  float *h_odata = (float *)malloc(size);
-  // copy result from device to host
-  HIPCHECK(hipMemcpy(h_odata, d_data, size, hipMemcpyDeviceToHost));
-
-  // write regression file if necessary
-  if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
-    // write file for regression test
-    sdkWriteFile<float>("./data/regression.dat", h_odata, width * width, 0.0f,
-                        false);
-  } else {
-    printf("Comparing kernel output to expected data\n");
-
-#define MIN_EPSILON_ERROR 5e-3f
-    bResult =
-        compareData(h_odata, h_data_ref, cubemap_size, MIN_EPSILON_ERROR, 0.0f);
-  }
-
-  // cleanup memory
-  free(h_data);
-  free(h_data_ref);
-  free(h_odata);
-
-  HIPCHECK(hipDestroyTextureObject(tex));
-  HIPCHECK(hipFree(d_data));
-  HIPCHECK(hipFreeArray(cu_3darray));
-
-  exit(bResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleIPC/simpleIPC.cu.hip b/src/samples/Samples/0_Introduction/simpleIPC/simpleIPC.cu.hip
index e338d7a..b0b4730 100644
--- a/src/samples/Samples/0_Introduction/simpleIPC/simpleIPC.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleIPC/simpleIPC.cu.hip
@@ -32,6 +32,8 @@
 
 #include <hip/hip_runtime.h>
 #include <stdio.h>
+#include "rocprofiler.h"
+#include "HIPCHECK.h"
 #include <stdlib.h>
 #include <vector>
 #include "helper_cuda.h"
@@ -111,10 +113,10 @@ static void childProcess(int id) {
 
   printf("Process %d: Starting on device %d...\n", id, shm->devices[id]);
 
-  checkCudaErrors(hipSetDevice(shm->devices[id]));
-  checkCudaErrors(hipGetDeviceProperties(&prop, shm->devices[id]));
-  checkCudaErrors(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-  checkCudaErrors(hipOccupancyMaxActiveBlocksPerMultiprocessor(
+  HIPCHECK(hipSetDevice(shm->devices[id]));
+  HIPCHECK(hipGetDeviceProperties(&prop, shm->devices[id]));
+  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
+  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
       &blocks, simpleKernel, threads, 0));
   blocks *= prop.multiProcessorCount;
 
@@ -126,10 +128,10 @@ static void childProcess(int id) {
 
     // Notice, we don't need to explicitly enable peer access for
     // allocations on other devices.
-    checkCudaErrors(
+    HIPCHECK(
         hipIpcOpenMemHandle(&ptr, *(hipIpcMemHandle_t *)&shm->memHandle[i],
                              hipIpcMemLazyEnablePeerAccess));
-    checkCudaErrors(hipIpcOpenEventHandle(
+    HIPCHECK(hipIpcOpenEventHandle(
         &event, *(hipIpcEventHandle_t *)&shm->eventHandle[i]));
 
     ptrs.push_back(ptr);
@@ -145,13 +147,13 @@ static void childProcess(int id) {
   for (i = 0; i < procCount; i++) {
     size_t bufferId = (i + id) % procCount;
     // Wait for the buffer to be accessed to be ready
-    checkCudaErrors(hipStreamWaitEvent(stream, events[bufferId], 0));
+    HIPCHECK(hipStreamWaitEvent(stream, events[bufferId], 0));
     // Push a simple kernel on it
     simpleKernel<<<blocks, threads, 0, stream>>>((char *)ptrs[bufferId],
                                                  DATA_SIZE, id);
-    checkCudaErrors(hipGetLastError());
+    HIPCHECK(hipGetLastError());
     // Signal that this buffer is ready for the next consumer
-    checkCudaErrors(hipEventRecord(events[bufferId], stream));
+    HIPCHECK(hipEventRecord(events[bufferId], stream));
     // Wait for all my sibling processes to push this stage of their work
     // before proceeding to the next. This prevents siblings from racing
     // ahead and clobbering the recorded event or waiting on the wrong
@@ -163,11 +165,11 @@ static void childProcess(int id) {
   }
 
   // Now wait for my buffer to be ready so I can copy it locally and verify it
-  checkCudaErrors(hipStreamWaitEvent(stream, events[id], 0));
-  checkCudaErrors(hipMemcpyAsync(&verification_buffer[0], ptrs[id], DATA_SIZE,
+  HIPCHECK(hipStreamWaitEvent(stream, events[id], 0));
+  HIPCHECK(hipMemcpyAsync(&verification_buffer[0], ptrs[id], DATA_SIZE,
                                   hipMemcpyDeviceToHost, stream));
   // And wait for all the queued up work to complete
-  checkCudaErrors(hipStreamSynchronize(stream));
+  HIPCHECK(hipStreamSynchronize(stream));
 
   printf("Process %d: verifying...\n", id);
 
@@ -182,11 +184,11 @@ static void childProcess(int id) {
 
   // Clean up!
   for (i = 0; i < procCount; i++) {
-    checkCudaErrors(hipIpcCloseMemHandle(ptrs[i]));
-    checkCudaErrors(hipEventDestroy(events[i]));
+    HIPCHECK(hipIpcCloseMemHandle(ptrs[i]));
+    HIPCHECK(hipEventDestroy(events[i]));
   }
 
-  checkCudaErrors(hipStreamDestroy(stream));
+  HIPCHECK(hipStreamDestroy(stream));
 
   printf("Process %d complete!\n", id);
 }
@@ -199,7 +201,7 @@ static void parentProcess(char *app) {
   std::vector<hipEvent_t> events;
   std::vector<Process> processes;
 
-  checkCudaErrors(hipGetDeviceCount(&devCount));
+  HIPCHECK(hipGetDeviceCount(&devCount));
 
   if (sharedMemoryCreate(shmName, sizeof(*shm), &info) != 0) {
     printf("Failed to create shared memory slab\n");
@@ -215,7 +217,7 @@ static void parentProcess(char *app) {
   for (i = 0; i < devCount; i++) {
     bool allPeers = true;
     hipDeviceProp_t prop;
-    checkCudaErrors(hipGetDeviceProperties(&prop, i));
+    HIPCHECK(hipGetDeviceProperties(&prop, i));
 
     // CUDA IPC is only supported on devices with unified addressing
     if (!prop.unifiedAddressing) {
@@ -232,9 +234,9 @@ static void parentProcess(char *app) {
 
     for (int j = 0; j < shm->nprocesses; j++) {
       int canAccessPeerIJ, canAccessPeerJI;
-      checkCudaErrors(
+      HIPCHECK(
           hipDeviceCanAccessPeer(&canAccessPeerJI, shm->devices[j], i));
-      checkCudaErrors(
+      HIPCHECK(
           hipDeviceCanAccessPeer(&canAccessPeerIJ, i, shm->devices[j]));
       if (!canAccessPeerIJ || !canAccessPeerJI) {
         allPeers = false;
@@ -246,10 +248,10 @@ static void parentProcess(char *app) {
       // setup the peers for the device.  For systems that only allow 8
       // peers per GPU at a time, this acts to remove devices from CanAccessPeer
       for (int j = 0; j < shm->nprocesses; j++) {
-        checkCudaErrors(hipSetDevice(i));
-        checkCudaErrors(hipDeviceEnablePeerAccess(shm->devices[j], 0));
-        checkCudaErrors(hipSetDevice(shm->devices[j]));
-        checkCudaErrors(hipDeviceEnablePeerAccess(i, 0));
+        HIPCHECK(hipSetDevice(i));
+        HIPCHECK(hipDeviceEnablePeerAccess(shm->devices[j], 0));
+        HIPCHECK(hipSetDevice(shm->devices[j]));
+        HIPCHECK(hipDeviceEnablePeerAccess(i, 0));
       }
       shm->devices[shm->nprocesses++] = i;
       if (shm->nprocesses >= MAX_DEVICES) break;
@@ -272,13 +274,13 @@ static void parentProcess(char *app) {
     void *ptr = NULL;
     hipEvent_t event;
 
-    checkCudaErrors(hipSetDevice(shm->devices[i]));
-    checkCudaErrors(hipMalloc(&ptr, DATA_SIZE));
-    checkCudaErrors(
+    HIPCHECK(hipSetDevice(shm->devices[i]));
+    HIPCHECK(hipMalloc(&ptr, DATA_SIZE));
+    HIPCHECK(
         hipIpcGetMemHandle((hipIpcMemHandle_t *)&shm->memHandle[i], ptr));
-    checkCudaErrors(hipEventCreate(
+    HIPCHECK(hipEventCreate(
         &event, hipEventDisableTiming | hipEventInterprocess));
-    checkCudaErrors(hipIpcGetEventHandle(
+    HIPCHECK(hipIpcGetEventHandle(
         (hipIpcEventHandle_t *)&shm->eventHandle[i], event));
 
     ptrs.push_back(ptr);
@@ -311,10 +313,10 @@ static void parentProcess(char *app) {
 
   // Clean up!
   for (i = 0; i < shm->nprocesses; i++) {
-    checkCudaErrors(hipSetDevice(shm->devices[i]));
-    checkCudaErrors(hipEventSynchronize(events[i]));
-    checkCudaErrors(hipEventDestroy(events[i]));
-    checkCudaErrors(hipFree(ptrs[i]));
+    HIPCHECK(hipSetDevice(shm->devices[i]));
+    HIPCHECK(hipEventSynchronize(events[i]));
+    HIPCHECK(hipEventDestroy(events[i]));
+    HIPCHECK(hipFree(ptrs[i]));
   }
 
   sharedMemoryClose(&info);
@@ -333,3 +335,12 @@ int main(int argc, char **argv) {
   return EXIT_SUCCESS;
 #endif
 }
+__arm__) || defined(__aarch64__)
+  printf("Not supported on ARM\n");
+  return EXIT_WAIVED;
+#else
+  if (argc == 1) {
+    parentProcess(argv[0]);
+  } else {
+    childProcess(atoi(argv[1]));
+  }
diff --git a/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.cu.hip b/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.cu.hip
index a08cf9a..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleLayeredTexture/simpleLayeredTexture.cu.hip
@@ -1,218 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
-* This sample demonstrates how to use texture fetches from layered 2D textures
-* in CUDA C
-*
-* This sample first generates a 3D input data array for the layered texture
-* and the expected output. Then it starts CUDA C kernels, one for each layer,
-* which fetch their layer's texture data (using normalized texture coordinates)
-* transform it to the expected output, and write it to a 3D output data array.
-*/
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-// includes, kernels
-#include <hip/hip_runtime.h>
-
-// includes, project
-#include <helper_cuda.h>
-#include <helper_functions.h>  // helper for shared that are common to CUDA Samples
-
-static const char *sSDKname = "simpleLayeredTexture";
-
-////////////////////////////////////////////////////////////////////////////////
-//! Transform a layer of a layered 2D texture using texture lookups
-//! @param g_odata  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-__global__ void transformKernel(float *g_odata, int width, int height,
-                                int layer, hipTextureObject_t tex) {
-  // calculate this thread's data point
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  // 0.5f offset and division are necessary to access the original data points
-  // in the texture (such that bilinear interpolation will not be activated).
-  // For details, see also CUDA Programming Guide, Appendix D
-  float u = (x + 0.5f) / (float)width;
-  float v = (y + 0.5f) / (float)height;
-
-  // read from texture, do expected transformation and write to global memory
-  g_odata[layer * width * height + y * width + x] =
-      -tex2DLayered<float>(tex, u, v, layer) + layer;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  printf("[%s] - Starting...\n", sSDKname);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  int devID = findCudaDevice(argc, (const char **)argv);
-
-  bool bResult = true;
-
-  // get number of SMs on this GPU
-  hipDeviceProp_t deviceProps;
-
-  HIPCHECK(hipGetDeviceProperties(&deviceProps, devID));
-  printf("CUDA device [%s] has %d Multi-Processors ", deviceProps.name,
-         deviceProps.multiProcessorCount);
-  printf("SM %d.%d\n", deviceProps.major, deviceProps.minor);
-
-  // generate input data for layered texture
-  unsigned int width = 512, height = 512, num_layers = 5;
-  unsigned int size = width * height * num_layers * sizeof(float);
-  float *h_data = (float *)malloc(size);
-
-  for (unsigned int layer = 0; layer < num_layers; layer++)
-    for (int i = 0; i < (int)(width * height); i++) {
-      h_data[layer * width * height + i] = (float)i;
-    }
-
-  // this is the expected transformation of the input data (the expected output)
-  float *h_data_ref = (float *)malloc(size);
-
-  for (unsigned int layer = 0; layer < num_layers; layer++)
-    for (int i = 0; i < (int)(width * height); i++) {
-      h_data_ref[layer * width * height + i] =
-          -h_data[layer * width * height + i] + layer;
-    }
-
-  // allocate device memory for result
-  float *d_data = NULL;
-  HIPCHECK(hipMalloc((void **)&d_data, size));
-
-  // allocate array and copy image data
-  hipChannelFormatDesc channelDesc =
-      hipCreateChannelDesc(32, 0, 0, 0, hipChannelFormatKindFloat);
-  hipArray *cu_3darray;
-  HIPCHECK(hipMalloc3DArray(&cu_3darray, &channelDesc,
-                                    make_hipExtent(width, height, num_layers),
-                                    hipArrayLayered));
-  hipMemcpy3DParms myparms = {0};
-  myparms.srcPos = make_hipPos(0, 0, 0);
-  myparms.dstPos = make_hipPos(0, 0, 0);
-  myparms.srcPtr =
-      make_hipPitchedPtr(h_data, width * sizeof(float), width, height);
-  myparms.dstArray = cu_3darray;
-  myparms.extent = make_hipExtent(width, height, num_layers);
-  myparms.kind = hipMemcpyHostToDevice;
-  HIPCHECK(hipMemcpy3D(&myparms));
-
-  hipTextureObject_t tex;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = cu_3darray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = true;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
-
-  dim3 dimBlock(8, 8, 1);
-  dim3 dimGrid(width / dimBlock.x, height / dimBlock.y, 1);
-
-  printf(
-      "Covering 2D data array of %d x %d: Grid size is %d x %d, each block has "
-      "8 x 8 threads\n",
-      width, height, dimGrid.x, dimGrid.y);
-
-  transformKernel<<<dimGrid, dimBlock>>>(d_data, width, height, 0,
-                                         tex);  // warmup (for better timing)
-
-  // check if kernel execution generated an error
-  getLastCudaError("warmup Kernel execution failed");
-
-  HIPCHECK(hipDeviceSynchronize());
-
-  StopWatchInterface *timer = NULL;
-  sdkCreateTimer(&timer);
-  sdkStartTimer(&timer);
-
-  // execute the kernel
-  for (unsigned int layer = 0; layer < num_layers; layer++)
-    transformKernel<<<dimGrid, dimBlock, 0>>>(d_data, width, height, layer,
-                                              tex);
-
-  // check if kernel execution generated an error
-  getLastCudaError("Kernel execution failed");
-
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&timer);
-  printf("Processing time: %.3f msec\n", sdkGetTimerValue(&timer));
-  printf("%.2f Mtexlookups/sec\n",
-         (width * height * num_layers / (sdkGetTimerValue(&timer) / 1000.0f) /
-          1e6));
-  sdkDeleteTimer(&timer);
-
-  // allocate mem for the result on host side
-  float *h_odata = (float *)malloc(size);
-  // copy result from device to host
-  HIPCHECK(hipMemcpy(h_odata, d_data, size, hipMemcpyDeviceToHost));
-
-  // write regression file if necessary
-  if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
-    // write file for regression test
-    sdkWriteFile<float>("./data/regression.dat", h_odata, width * height, 0.0f,
-                        false);
-  } else {
-    printf("Comparing kernel output to expected data\n");
-
-#define MIN_EPSILON_ERROR 5e-3f
-    bResult = compareData(h_odata, h_data_ref, width * height * num_layers,
-                          MIN_EPSILON_ERROR, 0.0f);
-  }
-
-  // cleanup memory
-  free(h_data);
-  free(h_data_ref);
-  free(h_odata);
-
-  HIPCHECK(hipDestroyTextureObject(tex));
-  HIPCHECK(hipFree(d_data));
-  HIPCHECK(hipFreeArray(cu_3darray));
-
-  exit(bResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/0_Introduction/simpleMPI/simpleMPI.cu.hip b/src/samples/Samples/0_Introduction/simpleMPI/simpleMPI.cu.hip
index e69de29..b4baff3 100644
--- a/src/samples/Samples/0_Introduction/simpleMPI/simpleMPI.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleMPI/simpleMPI.cu.hip
@@ -0,0 +1,104 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/* Simple example demonstrating how to use MPI with CUDA
+*
+*  Generate some random numbers on one node.
+*  Dispatch them to all nodes.
+*  Compute their square root on each node's GPU.
+*  Compute the average of the results using MPI.
+*
+*  simpleMPI.cu: GPU part, compiled with nvcc
+*/
+
+
+#include <hip/hip_runtime.h>
+#include <iostream>
+using std::cerr;
+using std::endl;
+
+#include "simpleMPI.h"
+
+// Error handling macro
+#define CUDA_CHECK(call)                                                 \
+  if ((call) != hipSuccess) {                                           \
+    hipError_t err = hipGetLastError();                                \
+    cerr << "CUDA error calling \"" #call "\", code is " << err << endl; \
+    my_abort(err);                                                       \
+  }
+
+// Device code
+// Very simple GPU Kernel that computes square roots of input numbers
+__global__ void simpleMPIKernel(float *input, float *output) {
+  int tid = blockIdx.x * blockDim.x + threadIdx.x;
+  output[tid] = sqrt(input[tid]);
+}
+
+// Initialize an array with random data (between 0 and 1)
+void initData(float *data, int dataSize) {
+  for (int i = 0; i < dataSize; i++) {
+    data[i] = (float)rand() / RAND_MAX;
+  }
+}
+
+// CUDA computation on each node
+// No MPI here, only CUDA
+void computeGPU(float *hostData, int blockSize, int gridSize) {
+  int dataSize = blockSize * gridSize;
+
+  // Allocate data on GPU memory
+  float *deviceInputData = NULL;
+  CUDA_CHECK(hipMalloc((void **)&deviceInputData, dataSize * sizeof(float)));
+
+  float *deviceOutputData = NULL;
+  CUDA_CHECK(hipMalloc((void **)&deviceOutputData, dataSize * sizeof(float)));
+
+  // Copy to GPU memory
+  CUDA_CHECK(hipMemcpy(deviceInputData, hostData, dataSize * sizeof(float),
+                        hipMemcpyHostToDevice));
+
+  // Run kernel
+  simpleMPIKernel<<<gridSize, blockSize>>>(deviceInputData, deviceOutputData);
+
+  // Copy data back to CPU memory
+  CUDA_CHECK(hipMemcpy(hostData, deviceOutputData, dataSize * sizeof(float),
+                        hipMemcpyDeviceToHost));
+
+  // Free GPU memory
+  CUDA_CHECK(hipFree(deviceInputData));
+  CUDA_CHECK(hipFree(deviceOutputData));
+}
+
+float sum(float *data, int size) {
+  float accum = 0.f;
+
+  for (int i = 0; i < size; i++) {
+    accum += data[i];
+  }
+
+  return accum;
+}
diff --git a/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip b/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip
index 6489de2..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleMultiGPU/simpleMultiGPU.cu.hip
@@ -1,241 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This application demonstrates how to use the CUDA API to use multiple GPUs,
- * with an emphasis on simple illustration of the techniques (not on
- * performance).
- *
- * Note that in order to detect multiple GPUs in your system you have to disable
- * SLI in the nvidia control panel. Otherwise only one GPU is visible to the
- * application. On the other side, you can still extend your desktop to screens
- * attached to both GPUs.
- */
-
-// System includes
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <assert.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-#ifndef MAX
-#define MAX(a, b) (a > b ? a : b)
-#endif
-
-#include "simpleMultiGPU.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// Data configuration
-////////////////////////////////////////////////////////////////////////////////
-const int MAX_GPU_COUNT = 32;
-const int DATA_N = 1048576 * 32;
-
-////////////////////////////////////////////////////////////////////////////////
-// Simple reduction kernel.
-// Refer to the 'reduction' CUDA Sample describing
-// reduction optimization strategies
-////////////////////////////////////////////////////////////////////////////////
-__global__ static void reduceKernel(float *d_Result, float *d_Input, int N) {
-  const int tid = blockIdx.x * blockDim.x + threadIdx.x;
-  const int threadN = gridDim.x * blockDim.x;
-  float sum = 0;
-
-  for (int pos = tid; pos < N; pos += threadN) sum += d_Input[pos];
-
-  d_Result[tid] = sum;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  // Solver config
-  TGPUplan plan[MAX_GPU_COUNT];
-
-  // GPU reduction results
-  float h_SumGPU[MAX_GPU_COUNT];
-
-  float sumGPU;
-  double sumCPU, diff;
-
-  int i, j, gpuBase, GPU_N;
-
-  const int BLOCK_N = 32;
-  const int THREAD_N = 256;
-  const int ACCUM_N = BLOCK_N * THREAD_N;
-
-  printf("Starting simpleMultiGPU\n");
-  HIPCHECK(hipGetDeviceCount(&GPU_N));
-
-  if (GPU_N > MAX_GPU_COUNT) {
-    GPU_N = MAX_GPU_COUNT;
-  }
-
-  printf("CUDA-capable device count: %i\n", GPU_N);
-
-  printf("Generating input data...\n\n");
-
-  // Subdividing input data across GPUs
-  // Get data sizes for each GPU
-  for (i = 0; i < GPU_N; i++) {
-    plan[i].dataN = DATA_N / GPU_N;
-  }
-
-  // Take into account "odd" data sizes
-  for (i = 0; i < DATA_N % GPU_N; i++) {
-    plan[i].dataN++;
-  }
-
-  // Assign data ranges to GPUs
-  gpuBase = 0;
-
-  for (i = 0; i < GPU_N; i++) {
-    plan[i].h_Sum = h_SumGPU + i;
-    gpuBase += plan[i].dataN;
-  }
-
-  // Create streams for issuing GPU command asynchronously and allocate memory
-  // (GPU and System page-locked)
-  for (i = 0; i < GPU_N; i++) {
-    HIPCHECK(hipSetDevice(i));
-    HIPCHECK(hipStreamCreate(&plan[i].stream));
-    // Allocate memory
-    HIPCHECK(
-        hipMalloc((void **)&plan[i].d_Data, plan[i].dataN * sizeof(float)));
-    HIPCHECK(
-        hipMalloc((void **)&plan[i].d_Sum, ACCUM_N * sizeof(float)));
-    HIPCHECK(hipHostMalloc((void **)&plan[i].h_Sum_from_device,
-                                   ACCUM_N * sizeof(float)));
-    HIPCHECK(hipHostMalloc((void **)&plan[i].h_Data,
-                                   plan[i].dataN * sizeof(float)));
-
-    for (j = 0; j < plan[i].dataN; j++) {
-      plan[i].h_Data[j] = (float)rand() / (float)RAND_MAX;
-    }
-  }
-
-  // Start timing and compute on GPU(s)
-  printf("Computing with %d GPUs...\n", GPU_N);
-  // create and start timer
-  StopWatchInterface *timer = NULL;
-  sdkCreateTimer(&timer);
-
-  // start the timer
-  sdkStartTimer(&timer);
-
-  // Copy data to GPU, launch the kernel and copy data back. All asynchronously
-  for (i = 0; i < GPU_N; i++) {
-    // Set device
-    HIPCHECK(hipSetDevice(i));
-
-    // Copy input data from CPU
-    HIPCHECK(hipMemcpyAsync(plan[i].d_Data, plan[i].h_Data,
-                                    plan[i].dataN * sizeof(float),
-                                    hipMemcpyHostToDevice, plan[i].stream));
-
-    // Perform GPU computations
-    reduceKernel<<<BLOCK_N, THREAD_N, 0, plan[i].stream>>>(
-        plan[i].d_Sum, plan[i].d_Data, plan[i].dataN);
-    getLastCudaError("reduceKernel() execution failed.\n");
-
-    // Read back GPU results
-    HIPCHECK(hipMemcpyAsync(plan[i].h_Sum_from_device, plan[i].d_Sum,
-                                    ACCUM_N * sizeof(float),
-                                    hipMemcpyDeviceToHost, plan[i].stream));
-  }
-
-  // Process GPU results
-  for (i = 0; i < GPU_N; i++) {
-    float sum;
-
-    // Set device
-    HIPCHECK(hipSetDevice(i));
-
-    // Wait for all operations to finish
-    hipStreamSynchronize(plan[i].stream);
-
-    // Finalize GPU reduction for current subvector
-    sum = 0;
-
-    for (j = 0; j < ACCUM_N; j++) {
-      sum += plan[i].h_Sum_from_device[j];
-    }
-
-    *(plan[i].h_Sum) = (float)sum;
-
-    // Shut down this GPU
-    HIPCHECK(hipHostFree(plan[i].h_Sum_from_device));
-    HIPCHECK(hipFree(plan[i].d_Sum));
-    HIPCHECK(hipFree(plan[i].d_Data));
-    HIPCHECK(hipStreamDestroy(plan[i].stream));
-  }
-
-  sumGPU = 0;
-
-  for (i = 0; i < GPU_N; i++) {
-    sumGPU += h_SumGPU[i];
-  }
-
-  sdkStopTimer(&timer);
-  printf("  GPU Processing time: %f (ms)\n\n", sdkGetTimerValue(&timer));
-  sdkDeleteTimer(&timer);
-
-  // Compute on Host CPU
-  printf("Computing with Host CPU...\n\n");
-
-  sumCPU = 0;
-
-  for (i = 0; i < GPU_N; i++) {
-    for (j = 0; j < plan[i].dataN; j++) {
-      sumCPU += plan[i].h_Data[j];
-    }
-  }
-
-  // Compare GPU and CPU results
-  printf("Comparing GPU and Host CPU results...\n");
-  diff = fabs(sumCPU - sumGPU) / fabs(sumCPU);
-  printf("  GPU sum: %f\n  CPU sum: %f\n", sumGPU, sumCPU);
-  printf("  Relative difference: %E \n\n", diff);
-
-  // Cleanup and shutdown
-  for (i = 0; i < GPU_N; i++) {
-    HIPCHECK(hipSetDevice(i));
-    HIPCHECK(hipHostFree(plan[i].h_Data));
-  }
-
-  exit((diff < 1e-5) ? EXIT_SUCCESS : EXIT_FAILURE);
-}
-ice(i));
-    checkCudaErrors(hipHostFree(plan[i].h_Data));
-  }
diff --git a/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip b/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip
index 7768545..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleOccupancy/simpleOccupancy.cu.hip
@@ -1,244 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <iostream>
-#include <helper_cuda.h>  // helper functions for CUDA error check
-
-const int manualBlockSize = 32;
-
-////////////////////////////////////////////////////////////////////////////////
-// Test kernel
-//
-// This kernel squares each array element. Each thread addresses
-// himself with threadIdx and blockIdx, so that it can handle any
-// execution configuration, including anything the launch configurator
-// API suggests.
-////////////////////////////////////////////////////////////////////////////////
-__global__ void square(int *array, int arrayCount) {
-  extern __shared__ int dynamicSmem[];
-  int idx = threadIdx.x + blockIdx.x * blockDim.x;
-
-  if (idx < arrayCount) {
-    array[idx] *= array[idx];
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Potential occupancy calculator
-//
-// The potential occupancy is calculated according to the kernel and
-// execution configuration the user desires. Occupancy is defined in
-// terms of active blocks per multiprocessor, and the user can convert
-// it to other metrics.
-//
-// This wrapper routine computes the occupancy of kernel, and reports
-// it in terms of active warps / maximum warps per SM.
-////////////////////////////////////////////////////////////////////////////////
-static double reportPotentialOccupancy(void *kernel, int blockSize,
-                                       size_t dynamicSMem) {
-  int device;
-  hipDeviceProp_t prop;
-
-  int numBlocks;
-  int activeWarps;
-  int maxWarps;
-
-  double occupancy;
-
-  HIPCHECK(hipGetDevice(&device));
-  HIPCHECK(hipGetDeviceProperties(&prop, device));
-
-  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
-      &numBlocks, kernel, blockSize, dynamicSMem));
-
-  activeWarps = numBlocks * blockSize / prop.warpSize;
-  maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;
-
-  occupancy = (double)activeWarps / maxWarps;
-
-  return occupancy;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Occupancy-based launch configurator
-//
-// The launch configurator, cudaOccupancyMaxPotentialBlockSize and
-// cudaOccupancyMaxPotentialBlockSizeVariableSMem, suggests a block
-// size that achieves the best theoretical occupancy. It also returns
-// the minimum number of blocks needed to achieve the occupancy on the
-// whole device.
-//
-// This launch configurator is purely occupancy-based. It doesn't
-// translate directly to performance, but the suggestion should
-// nevertheless be a good starting point for further optimizations.
-//
-// This function configures the launch based on the "automatic"
-// argument, records the runtime, and reports occupancy and runtime.
-////////////////////////////////////////////////////////////////////////////////
-static int launchConfig(int *array, int arrayCount, bool automatic) {
-  int blockSize;
-  int minGridSize;
-  int gridSize;
-  size_t dynamicSMemUsage = 0;
-
-  hipEvent_t start;
-  hipEvent_t end;
-
-  float elapsedTime;
-
-  double potentialOccupancy;
-
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&end));
-
-  if (automatic) {
-    HIPCHECK(hipOccupancyMaxPotentialBlockSize(
-        &minGridSize, &blockSize, (void *)square, dynamicSMemUsage,
-        arrayCount));
-
-    std::cout << "Suggested block size: " << blockSize << std::endl
-              << "Minimum grid size for maximum occupancy: " << minGridSize
-              << std::endl;
-  } else {
-    // This block size is too small. Given limited number of
-    // active blocks per multiprocessor, the number of active
-    // threads will be limited, and thus unable to achieve maximum
-    // occupancy.
-    //
-    blockSize = manualBlockSize;
-  }
-
-  // Round up
-  //
-  gridSize = (arrayCount + blockSize - 1) / blockSize;
-
-  // Launch and profile
-  //
-  HIPCHECK(hipEventRecord(start));
-  square<<<gridSize, blockSize, dynamicSMemUsage>>>(array, arrayCount);
-  HIPCHECK(hipEventRecord(end));
-
-  HIPCHECK(hipDeviceSynchronize());
-
-  // Calculate occupancy
-  //
-  potentialOccupancy =
-      reportPotentialOccupancy((void *)square, blockSize, dynamicSMemUsage);
-
-  std::cout << "Potential occupancy: " << potentialOccupancy * 100 << "%"
-            << std::endl;
-
-  // Report elapsed time
-  //
-  HIPCHECK(hipEventElapsedTime(&elapsedTime, start, end));
-  std::cout << "Elapsed time: " << elapsedTime << "ms" << std::endl;
-
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// The test
-//
-// The test generates an array and squares it with a CUDA kernel, then
-// verifies the result.
-////////////////////////////////////////////////////////////////////////////////
-static int test(bool automaticLaunchConfig, const int count = 1000000) {
-  int *array;
-  int *dArray;
-  int size = count * sizeof(int);
-
-  array = new int[count];
-
-  for (int i = 0; i < count; i += 1) {
-    array[i] = i;
-  }
-
-  HIPCHECK(hipMalloc(&dArray, size));
-  HIPCHECK(hipMemcpy(dArray, array, size, hipMemcpyHostToDevice));
-
-  for (int i = 0; i < count; i += 1) {
-    array[i] = 0;
-  }
-
-  launchConfig(dArray, count, automaticLaunchConfig);
-
-  HIPCHECK(hipMemcpy(array, dArray, size, hipMemcpyDeviceToHost));
-  HIPCHECK(hipFree(dArray));
-
-  // Verify the return data
-  //
-  for (int i = 0; i < count; i += 1) {
-    if (array[i] != i * i) {
-      std::cout << "element " << i << " expected " << i * i << " actual "
-                << array[i] << std::endl;
-      return 1;
-    }
-  }
-  delete[] array;
-
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Sample Main
-//
-// The sample runs the test with manually configured launch and
-// automatically configured launch, and reports the occupancy and
-// performance.
-////////////////////////////////////////////////////////////////////////////////
-int main() {
-  int status;
-
-  std::cout << "starting Simple Occupancy" << std::endl << std::endl;
-
-  std::cout << "[ Manual configuration with " << manualBlockSize
-            << " threads per block ]" << std::endl;
-
-  status = test(false);
-  if (status) {
-    std::cerr << "Test failed\n" << std::endl;
-    return -1;
-  }
-
-  std::cout << std::endl;
-
-  std::cout << "[ Automatic, occupancy-based configuration ]" << std::endl;
-  status = test(true);
-  if (status) {
-    std::cerr << "Test failed\n" << std::endl;
-    return -1;
-  }
-
-  std::cout << std::endl;
-  std::cout << "Test PASSED\n" << std::endl;
-
-  return 0;
-}
-rn -1;
-  }
diff --git a/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.cu.hip b/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.cu.hip
index 2c7f68f..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleP2P/simpleP2P.cu.hip
@@ -1,276 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample demonstrates a combination of Peer-to-Peer (P2P) and
- * Unified Virtual Address Space (UVA) features new to SDK 4.0
- */
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-// CUDA includes
-#include <hip/hip_runtime.h>
-
-// includes, project
-#include <helper_cuda.h>
-#include <helper_functions.h>  // helper for shared that are common to CUDA Samples
-
-__global__ void SimpleKernel(float *src, float *dst) {
-  // Just a dummy kernel, doing enough for us to verify that everything
-  // worked
-  const int idx = blockIdx.x * blockDim.x + threadIdx.x;
-  dst[idx] = src[idx] * 2.0f;
-}
-
-inline bool IsAppBuiltAs64() { return sizeof(void *) == 8; }
-
-int main(int argc, char **argv) {
-  printf("[%s] - Starting...\n", argv[0]);
-
-  if (!IsAppBuiltAs64()) {
-    printf(
-        "%s is only supported with on 64-bit OSs and the application must be "
-        "built as a 64-bit target.  Test is being waived.\n",
-        argv[0]);
-    exit(EXIT_WAIVED);
-  }
-
-  // Number of GPUs
-  printf("Checking for multiple GPUs...\n");
-  int gpu_n;
-  HIPCHECK(hipGetDeviceCount(&gpu_n));
-  printf("CUDA-capable device count: %i\n", gpu_n);
-
-  if (gpu_n < 2) {
-    printf(
-        "Two or more GPUs with Peer-to-Peer access capability are required for "
-        "%s.\n",
-        argv[0]);
-    printf("Waiving test.\n");
-    exit(EXIT_WAIVED);
-  }
-
-  // Query device properties
-  hipDeviceProp_t prop[64];
-  int gpuid[2];  // we want to find the first two GPU's that can support P2P
-
-  for (int i = 0; i < gpu_n; i++) {
-    HIPCHECK(hipGetDeviceProperties(&prop[i], i));
-  }
-  // Check possibility for peer access
-  printf("\nChecking GPU(s) for support of peer to peer memory access...\n");
-
-  int can_access_peer;
-  int p2pCapableGPUs[2];  // We take only 1 pair of P2P capable GPUs
-  p2pCapableGPUs[0] = p2pCapableGPUs[1] = -1;
-
-  // Show all the combinations of supported P2P GPUs
-  for (int i = 0; i < gpu_n; i++) {
-    for (int j = 0; j < gpu_n; j++) {
-      if (i == j) {
-        continue;
-      }
-      HIPCHECK(hipDeviceCanAccessPeer(&can_access_peer, i, j));
-      printf("> Peer access from %s (GPU%d) -> %s (GPU%d) : %s\n", prop[i].name,
-             i, prop[j].name, j, can_access_peer ? "Yes" : "No");
-      if (can_access_peer && p2pCapableGPUs[0] == -1) {
-        p2pCapableGPUs[0] = i;
-        p2pCapableGPUs[1] = j;
-      }
-    }
-  }
-
-  if (p2pCapableGPUs[0] == -1 || p2pCapableGPUs[1] == -1) {
-    printf(
-        "Two or more GPUs with Peer-to-Peer access capability are required for "
-        "%s.\n",
-        argv[0]);
-    printf(
-        "Peer to Peer access is not available amongst GPUs in the system, "
-        "waiving test.\n");
-
-    exit(EXIT_WAIVED);
-  }
-
-  // Use first pair of p2p capable GPUs detected.
-  gpuid[0] = p2pCapableGPUs[0];
-  gpuid[1] = p2pCapableGPUs[1];
-
-  // Enable peer access
-  printf("Enabling peer access between GPU%d and GPU%d...\n", gpuid[0],
-         gpuid[1]);
-  HIPCHECK(hipSetDevice(gpuid[0]));
-  HIPCHECK(hipDeviceEnablePeerAccess(gpuid[1], 0));
-  HIPCHECK(hipSetDevice(gpuid[1]));
-  HIPCHECK(hipDeviceEnablePeerAccess(gpuid[0], 0));
-
-  // Allocate buffers
-  const size_t buf_size = 1024 * 1024 * 16 * sizeof(float);
-  printf("Allocating buffers (%iMB on GPU%d, GPU%d and CPU Host)...\n",
-         int(buf_size / 1024 / 1024), gpuid[0], gpuid[1]);
-  HIPCHECK(hipSetDevice(gpuid[0]));
-  float *g0;
-  HIPCHECK(hipMalloc(&g0, buf_size));
-  HIPCHECK(hipSetDevice(gpuid[1]));
-  float *g1;
-  HIPCHECK(hipMalloc(&g1, buf_size));
-  float *h0;
-  HIPCHECK(
-      hipHostMalloc(&h0, buf_size));  // Automatically portable with UVA
-
-  // Create CUDA event handles
-  printf("Creating event handles...\n");
-  hipEvent_t start_event, stop_event;
-  float time_memcpy;
-  int eventflags = hipEventBlockingSync;
-  HIPCHECK(hipEventCreateWithFlags(&start_event, eventflags));
-  HIPCHECK(hipEventCreateWithFlags(&stop_event, eventflags));
-
-  // P2P memcopy() benchmark
-  HIPCHECK(hipEventRecord(start_event, 0));
-
-  for (int i = 0; i < 100; i++) {
-    // With UVA we don't need to specify source and target devices, the
-    // runtime figures this out by itself from the pointers
-    // Ping-pong copy between GPUs
-    if (i % 2 == 0) {
-      HIPCHECK(hipMemcpy(g1, g0, buf_size, hipMemcpyDefault));
-    } else {
-      HIPCHECK(hipMemcpy(g0, g1, buf_size, hipMemcpyDefault));
-    }
-  }
-
-  HIPCHECK(hipEventRecord(stop_event, 0));
-  HIPCHECK(hipEventSynchronize(stop_event));
-  HIPCHECK(hipEventElapsedTime(&time_memcpy, start_event, stop_event));
-  printf("hipMemcpyPeer / hipMemcpy between GPU%d and GPU%d: %.2fGB/s\n",
-         gpuid[0], gpuid[1],
-         (1.0f / (time_memcpy / 1000.0f)) * ((100.0f * buf_size)) / 1024.0f /
-             1024.0f / 1024.0f);
-
-  // Prepare host buffer and copy to GPU 0
-  printf("Preparing host buffer and memcpy to GPU%d...\n", gpuid[0]);
-
-  for (int i = 0; i < buf_size / sizeof(float); i++) {
-    h0[i] = float(i % 4096);
-  }
-
-  HIPCHECK(hipSetDevice(gpuid[0]));
-  HIPCHECK(hipMemcpy(g0, h0, buf_size, hipMemcpyDefault));
-
-  // Kernel launch configuration
-  const dim3 threads(512, 1);
-  const dim3 blocks((buf_size / sizeof(float)) / threads.x, 1);
-
-  // Run kernel on GPU 1, reading input from the GPU 0 buffer, writing
-  // output to the GPU 1 buffer
-  printf(
-      "Run kernel on GPU%d, taking source data from GPU%d and writing to "
-      "GPU%d...\n",
-      gpuid[1], gpuid[0], gpuid[1]);
-  HIPCHECK(hipSetDevice(gpuid[1]));
-  SimpleKernel<<<blocks, threads>>>(g0, g1);
-
-  HIPCHECK(hipDeviceSynchronize());
-
-  // Run kernel on GPU 0, reading input from the GPU 1 buffer, writing
-  // output to the GPU 0 buffer
-  printf(
-      "Run kernel on GPU%d, taking source data from GPU%d and writing to "
-      "GPU%d...\n",
-      gpuid[0], gpuid[1], gpuid[0]);
-  HIPCHECK(hipSetDevice(gpuid[0]));
-  SimpleKernel<<<blocks, threads>>>(g1, g0);
-
-  HIPCHECK(hipDeviceSynchronize());
-
-  // Copy data back to host and verify
-  printf("Copy data back to host from GPU%d and verify results...\n", gpuid[0]);
-  HIPCHECK(hipMemcpy(h0, g0, buf_size, hipMemcpyDefault));
-
-  int error_count = 0;
-
-  for (int i = 0; i < buf_size / sizeof(float); i++) {
-    // Re-generate input data and apply 2x '* 2.0f' computation of both
-    // kernel runs
-    if (h0[i] != float(i % 4096) * 2.0f * 2.0f) {
-      printf("Verification error @ element %i: val = %f, ref = %f\n", i, h0[i],
-             (float(i % 4096) * 2.0f * 2.0f));
-
-      if (error_count++ > 10) {
-        break;
-      }
-    }
-  }
-
-  // Disable peer access (also unregisters memory for non-UVA cases)
-  printf("Disabling peer access...\n");
-  HIPCHECK(hipSetDevice(gpuid[0]));
-  HIPCHECK(hipDeviceDisablePeerAccess(gpuid[1]));
-  HIPCHECK(hipSetDevice(gpuid[1]));
-  HIPCHECK(hipDeviceDisablePeerAccess(gpuid[0]));
-
-  // Cleanup and shutdown
-  printf("Shutting down...\n");
-  HIPCHECK(hipEventDestroy(start_event));
-  HIPCHECK(hipEventDestroy(stop_event));
-  HIPCHECK(hipSetDevice(gpuid[0]));
-  HIPCHECK(hipFree(g0));
-  HIPCHECK(hipSetDevice(gpuid[1]));
-  HIPCHECK(hipFree(g1));
-  HIPCHECK(hipHostFree(h0));
-
-  for (int i = 0; i < gpu_n; i++) {
-    HIPCHECK(hipSetDevice(i));
-  }
-
-  if (error_count != 0) {
-    printf("Test failed!\n");
-    exit(EXIT_FAILURE);
-  } else {
-    printf("Test passed\n");
-    exit(EXIT_SUCCESS);
-  }
-}
-(g1));
-  checkCudaErrors(hipHostFree(h0));
-
-  for (int i = 0; i < gpu_n; i++) {
-    checkCudaErrors(hipSetDevice(i));
-  }
-
-  if (error_count != 0) {
-    printf("Test failed!\n");
-    exit(EXIT_FAILURE);
-  } else {
-    printf("Test passed\n");
-    exit(EXIT_SUCCESS);
-  }
diff --git a/src/samples/Samples/0_Introduction/simpleTemplates/simpleTemplates.cu.hip b/src/samples/Samples/0_Introduction/simpleTemplates/simpleTemplates.cu.hip
index f5d42e7..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleTemplates/simpleTemplates.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTemplates/simpleTemplates.cu.hip
@@ -1,266 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* This sample is a templatized version of the template project.
-* It also shows how to correctly templatize dynamically allocated shared
-* memory arrays.
-* Host code.
-*/
-
-// System includes
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <assert.h>
-#include <string.h>
-#include <math.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-#ifndef MAX
-#define MAX(a, b) (a > b ? a : b)
-#endif
-
-// includes, kernels
-#include "sharedmem.cuh"
-
-int g_TotalFailures = 0;
-
-////////////////////////////////////////////////////////////////////////////////
-//! Simple test kernel for device functionality
-//! @param g_idata  input data in global memory
-//! @param g_odata  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-template <class T>
-__global__ void testKernel(T *g_idata, T *g_odata) {
-  // Shared mem size is determined by the host app at run time
-  SharedMemory<T> smem;
-  T *sdata = smem.getPointer();
-
-  // access thread id
-  const unsigned int tid = threadIdx.x;
-  // access number of threads in this block
-  const unsigned int num_threads = blockDim.x;
-
-  // read in input data from global memory
-  sdata[tid] = g_idata[tid];
-  __syncthreads();
-
-  // perform some computations
-  sdata[tid] = (T)num_threads * sdata[tid];
-  __syncthreads();
-
-  // write data to global memory
-  g_odata[tid] = sdata[tid];
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-template <class T>
-void runTest(int argc, char **argv, int len);
-
-template <class T>
-void computeGold(T *reference, T *idata, const unsigned int len) {
-  const T T_len = static_cast<T>(len);
-
-  for (unsigned int i = 0; i < len; ++i) {
-    reference[i] = idata[i] * T_len;
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  printf("> runTest<float,32>\n");
-  runTest<float>(argc, argv, 32);
-  printf("> runTest<int,64>\n");
-  runTest<int>(argc, argv, 64);
-
-  printf("\n[simpleTemplates] -> Test Results: %d Failures\n", g_TotalFailures);
-
-  exit(g_TotalFailures == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
-}
-
-// To completely templatize runTest (below) with cutil, we need to use
-// template specialization to wrap up CUTIL's array comparison and file writing
-// functions for different types.
-
-// Here's the generic wrapper for cutCompare*
-template <class T>
-class ArrayComparator {
- public:
-  bool compare(const T *reference, T *data, unsigned int len) {
-    fprintf(stderr,
-            "Error: no comparison function implemented for this type\n");
-    return false;
-  }
-};
-
-// Here's the specialization for ints:
-template <>
-class ArrayComparator<int> {
- public:
-  bool compare(const int *reference, int *data, unsigned int len) {
-    return compareData(reference, data, len, 0.15f, 0.0f);
-  }
-};
-
-// Here's the specialization for floats:
-template <>
-class ArrayComparator<float> {
- public:
-  bool compare(const float *reference, float *data, unsigned int len) {
-    return compareData(reference, data, len, 0.15f, 0.15f);
-  }
-};
-
-// Here's the generic wrapper for cutWriteFile*
-template <class T>
-class ArrayFileWriter {
- public:
-  bool write(const char *filename, T *data, unsigned int len, float epsilon) {
-    fprintf(stderr,
-            "Error: no file write function implemented for this type\n");
-    return false;
-  }
-};
-
-// Here's the specialization for ints:
-template <>
-class ArrayFileWriter<int> {
- public:
-  bool write(const char *filename, int *data, unsigned int len, float epsilon) {
-    return sdkWriteFile(filename, data, len, epsilon, false);
-  }
-};
-
-// Here's the specialization for floats:
-template <>
-class ArrayFileWriter<float> {
- public:
-  bool write(const char *filename, float *data, unsigned int len,
-             float epsilon) {
-    return sdkWriteFile(filename, data, len, epsilon, false);
-  }
-};
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run a simple test for CUDA
-////////////////////////////////////////////////////////////////////////////////
-template <class T>
-void runTest(int argc, char **argv, int len) {
-  int devID;
-  hipDeviceProp_t deviceProps;
-
-  devID = findCudaDevice(argc, (const char **)argv);
-
-  // get number of SMs on this GPU
-  HIPCHECK(hipGetDeviceProperties(&deviceProps, devID));
-  printf("CUDA device [%s] has %d Multi-Processors\n", deviceProps.name,
-         deviceProps.multiProcessorCount);
-
-  // create and start timer
-  StopWatchInterface *timer = NULL;
-  sdkCreateTimer(&timer);
-
-  // start the timer
-  sdkStartTimer(&timer);
-
-  unsigned int num_threads = len;
-  unsigned int mem_size = sizeof(float) * num_threads;
-
-  // allocate host memory
-  T *h_idata = (T *)malloc(mem_size);
-
-  // initialize the memory
-  for (unsigned int i = 0; i < num_threads; ++i) {
-    h_idata[i] = (T)i;
-  }
-
-  // allocate device memory
-  T *d_idata;
-  HIPCHECK(hipMalloc((void **)&d_idata, mem_size));
-  // copy host memory to device
-  HIPCHECK(
-      hipMemcpy(d_idata, h_idata, mem_size, hipMemcpyHostToDevice));
-
-  // allocate device memory for result
-  T *d_odata;
-  HIPCHECK(hipMalloc((void **)&d_odata, mem_size));
-
-  // setup execution parameters
-  dim3 grid(1, 1, 1);
-  dim3 threads(num_threads, 1, 1);
-
-  // execute the kernel
-  testKernel<T><<<grid, threads, mem_size>>>(d_idata, d_odata);
-
-  // check if kernel execution generated and error
-  getLastCudaError("Kernel execution failed");
-
-  // allocate mem for the result on host side
-  T *h_odata = (T *)malloc(mem_size);
-  // copy result from device to host
-  HIPCHECK(hipMemcpy(h_odata, d_odata, sizeof(T) * num_threads,
-                             hipMemcpyDeviceToHost));
-
-  sdkStopTimer(&timer);
-  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
-  sdkDeleteTimer(&timer);
-
-  // compute reference solution
-  T *reference = (T *)malloc(mem_size);
-  computeGold<T>(reference, h_idata, num_threads);
-
-  ArrayComparator<T> comparator;
-  ArrayFileWriter<T> writer;
-
-  // check result
-  if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
-    // write file for regression test
-    writer.write("./data/regression.dat", h_odata, num_threads, 0.0f);
-  } else {
-    // custom output handling when no regression test running
-    // in this case check if the result is equivalent to the expected solution
-    bool res = comparator.compare(reference, h_odata, num_threads);
-    printf("Compare %s\n\n", (1 == res) ? "OK" : "MISMATCH");
-    g_TotalFailures += (1 != res);
-  }
-
-  // cleanup memory
-  free(h_idata);
-  free(h_odata);
-  free(reference);
-  HIPCHECK(hipFree(d_idata));
-  HIPCHECK(hipFree(d_odata));
-}
diff --git a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
index e69de29..0e22d08 100644
--- a/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTemplates_nvrtc/simpleTemplates_kernel.cu.hip
@@ -0,0 +1,72 @@
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+// includes, kernels
+
+#include <hip/hip_runtime.h>
+#include "sharedmem.cuh"
+
+////////////////////////////////////////////////////////////////////////////////
+//! Simple test kernel for device functionality
+//! @param g_idata  input data in global memory
+//! @param g_odata  output data in global memory
+////////////////////////////////////////////////////////////////////////////////
+
+template <class T>
+__device__ void testKernel(T *g_idata, T *g_odata) {
+  // Shared mem size is determined by the host app at run time
+  SharedMemory<T> smem;
+
+  T *sdata = smem.getPointer();
+
+  // access thread id
+  const unsigned int tid = threadIdx.x;
+
+  // access number of threads in this block
+  const unsigned int num_threads = blockDim.x;
+
+  // read in input data from global memory
+  sdata[tid] = g_idata[tid];
+
+  __syncthreads();
+
+  // perform some computations
+  sdata[tid] = (T)num_threads * sdata[tid];
+
+  __syncthreads();
+
+  // write data to global memory
+  g_odata[tid] = sdata[tid];
+}
+
+extern "C" __global__ void testFloat(float *p1, float *p2) {
+  testKernel<float>(p1, p2);
+}
+
+extern "C" __global__ void testInt(int *p1, int *p2) {
+  testKernel<int>(p1, p2);
+}
diff --git a/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip b/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip
index b66c37d..e69de29 100644
--- a/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/simpleTexture3D/simpleTexture3D_kernel.cu.hip
@@ -1,143 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _SIMPLETEXTURE3D_KERNEL_CU_
-#define _SIMPLETEXTURE3D_KERNEL_CU_
-
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-#include <helper_cuda.h>
-#include <helper_math.h>
-
-typedef unsigned int uint;
-typedef unsigned char uchar;
-
-hipArray *d_volumeArray = 0;
-hipTextureObject_t tex;  // 3D texture
-
-__global__ void d_render(uint *d_output, uint imageW, uint imageH, float w,
-                         hipTextureObject_t texObj) {
-  uint x = __umul24(blockIdx.x, blockDim.x) + threadIdx.x;
-  uint y = __umul24(blockIdx.y, blockDim.y) + threadIdx.y;
-
-  float u = x / (float)imageW;
-  float v = y / (float)imageH;
-  // read from 3D texture
-  float voxel = tex3D<float>(texObj, u, v, w);
-
-  if ((x < imageW) && (y < imageH)) {
-    // write output color
-    uint i = __umul24(y, imageW) + x;
-    d_output[i] = voxel * 255;
-  }
-}
-
-extern "C" void setTextureFilterMode(bool bLinearFilter) {
-  if (tex) {
-    HIPCHECK(hipDestroyTextureObject(tex));
-  }
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = d_volumeArray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = true;
-  texDescr.filterMode =
-      bLinearFilter ? hipFilterModeLinear : hipFilterModePoint;
-  ;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.addressMode[2] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
-}
-
-extern "C" void initCuda(const uchar *h_volume, hipExtent volumeSize) {
-  // create 3D array
-  hipChannelFormatDesc channelDesc = hipCreateChannelDesc<uchar>();
-  HIPCHECK(hipMalloc3DArray(&d_volumeArray, &channelDesc, volumeSize));
-
-  // copy data to 3D array
-  hipMemcpy3DParms copyParams = {0};
-  copyParams.srcPtr =
-      make_hipPitchedPtr((void *)h_volume, volumeSize.width * sizeof(uchar),
-                          volumeSize.width, volumeSize.height);
-  copyParams.dstArray = d_volumeArray;
-  copyParams.extent = volumeSize;
-  copyParams.kind = hipMemcpyHostToDevice;
-  HIPCHECK(hipMemcpy3D(&copyParams));
-
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = d_volumeArray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  // access with normalized texture coordinates
-  texDescr.normalizedCoords = true;
-  // linear interpolation
-  texDescr.filterMode = hipFilterModeLinear;
-  // wrap texture coordinates
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.addressMode[2] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(hipCreateTextureObject(&tex, &texRes, &texDescr, NULL));
-}
-
-extern "C" void render_kernel(dim3 gridSize, dim3 blockSize, uint *d_output,
-                              uint imageW, uint imageH, float w) {
-  d_render<<<gridSize, blockSize>>>(d_output, imageW, imageH, w, tex);
-}
-
-void cleanupCuda() {
-  if (tex) {
-    HIPCHECK(hipDestroyTextureObject(tex));
-  }
-  if (d_volumeArray) {
-    HIPCHECK(hipFreeArray(d_volumeArray));
-  }
-}
-
-#endif  // #ifndef _SIMPLETEXTURE3D_KERNEL_CU_
-
-
-#endif  // #ifndef _SIMPLETEXTURE3D_KERNEL_CU_
diff --git a/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip b/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
index 372cbd1..e69de29 100644
--- a/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
+++ b/src/samples/Samples/0_Introduction/systemWideAtomics/systemWideAtomics.cu.hip
@@ -1,342 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* A program demonstrating trivial use of system-wide atomics on migratable
- * memory.
- */
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-#include <math.h>
-#include <stdint.h>
-#include <cstdio>
-#include <ctime>
-
-#define min(a, b) (a) < (b) ? (a) : (b)
-#define max(a, b) (a) > (b) ? (a) : (b)
-
-#define LOOP_NUM 50
-
-__global__ void atomicKernel(int *atom_arr) {
-  unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;
-
-  for (int i = 0; i < LOOP_NUM; i++) {
-    // Atomic addition
-    atomicAdd_system(&atom_arr[0], 10);
-
-    // Atomic exchange
-    atomicExch_system(&atom_arr[1], tid);
-
-    // Atomic maximum
-    atomicMax_system(&atom_arr[2], tid);
-
-    // Atomic minimum
-    atomicMin_system(&atom_arr[3], tid);
-
-    // Atomic increment (modulo 17+1)
-    atomicInc_system((unsigned int *)&atom_arr[4], 17);
-
-    // Atomic decrement
-    atomicDec_system((unsigned int *)&atom_arr[5], 137);
-
-    // Atomic compare-and-swap
-    atomicCAS_system(&atom_arr[6], tid - 1, tid);
-
-    // Bitwise atomic instructions
-
-    // Atomic AND
-    atomicAnd_system(&atom_arr[7], 2 * tid + 7);
-
-    // Atomic OR
-    atomicOr_system(&atom_arr[8], 1 << tid);
-
-    // Atomic XOR
-    atomicXor_system(&atom_arr[9], tid);
-  }
-}
-
-void atomicKernel_CPU(int *atom_arr, int no_of_threads) {
-  for (int i = no_of_threads; i < 2 * no_of_threads; i++) {
-    for (int j = 0; j < LOOP_NUM; j++) {
-      // Atomic addition
-      __sync_fetch_and_add(&atom_arr[0], 10);
-
-      // Atomic exchange
-      __sync_lock_test_and_set(&atom_arr[1], i);
-
-      // Atomic maximum
-      int old, expected;
-      do {
-        expected = atom_arr[2];
-        old = __sync_val_compare_and_swap(&atom_arr[2], expected,
-                                          max(expected, i));
-      } while (old != expected);
-
-      // Atomic minimum
-      do {
-        expected = atom_arr[3];
-        old = __sync_val_compare_and_swap(&atom_arr[3], expected,
-                                          min(expected, i));
-      } while (old != expected);
-
-      // Atomic increment (modulo 17+1)
-      int limit = 17;
-      do {
-        expected = atom_arr[4];
-        old = __sync_val_compare_and_swap(
-            &atom_arr[4], expected, (expected >= limit) ? 0 : expected + 1);
-      } while (old != expected);
-
-      // Atomic decrement
-      limit = 137;
-      do {
-        expected = atom_arr[5];
-        old = __sync_val_compare_and_swap(
-            &atom_arr[5], expected,
-            ((expected == 0) || (expected > limit)) ? limit : expected - 1);
-      } while (old != expected);
-
-      // Atomic compare-and-swap
-      __sync_val_compare_and_swap(&atom_arr[6], i - 1, i);
-
-      // Bitwise atomic instructions
-
-      // Atomic AND
-      __sync_fetch_and_and(&atom_arr[7], 2 * i + 7);
-
-      // Atomic OR
-      __sync_fetch_and_or(&atom_arr[8], 1 << i);
-
-      // Atomic XOR
-      // 11th element should be 0xff
-      __sync_fetch_and_xor(&atom_arr[9], i);
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Compute reference data set
-//! Each element is multiplied with the number of threads / array length
-//! @param reference  reference data, computed but preallocated
-//! @param idata      input data as provided to device
-//! @param len        number of elements in reference / idata
-////////////////////////////////////////////////////////////////////////////////
-int verify(int *testData, const int len) {
-  int val = 0;
-
-  for (int i = 0; i < len * LOOP_NUM; ++i) {
-    val += 10;
-  }
-
-  if (val != testData[0]) {
-    printf("atomicAdd failed val = %d testData = %d\n", val, testData[0]);
-    return false;
-  }
-
-  val = 0;
-
-  bool found = false;
-
-  for (int i = 0; i < len; ++i) {
-    // second element should be a member of [0, len)
-    if (i == testData[1]) {
-      found = true;
-      break;
-    }
-  }
-
-  if (!found) {
-    printf("atomicExch failed\n");
-    return false;
-  }
-
-  val = -(1 << 8);
-
-  for (int i = 0; i < len; ++i) {
-    // third element should be len-1
-    val = max(val, i);
-  }
-
-  if (val != testData[2]) {
-    printf("atomicMax failed\n");
-    return false;
-  }
-
-  val = 1 << 8;
-
-  for (int i = 0; i < len; ++i) {
-    val = min(val, i);
-  }
-
-  if (val != testData[3]) {
-    printf("atomicMin failed\n");
-    return false;
-  }
-
-  int limit = 17;
-  val = 0;
-
-  for (int i = 0; i < len * LOOP_NUM; ++i) {
-    val = (val >= limit) ? 0 : val + 1;
-  }
-
-  if (val != testData[4]) {
-    printf("atomicInc failed\n");
-    return false;
-  }
-
-  limit = 137;
-  val = 0;
-
-  for (int i = 0; i < len * LOOP_NUM; ++i) {
-    val = ((val == 0) || (val > limit)) ? limit : val - 1;
-  }
-
-  if (val != testData[5]) {
-    printf("atomicDec failed\n");
-    return false;
-  }
-
-  found = false;
-
-  for (int i = 0; i < len; ++i) {
-    // seventh element should be a member of [0, len)
-    if (i == testData[6]) {
-      found = true;
-      break;
-    }
-  }
-
-  if (!found) {
-    printf("atomicCAS failed\n");
-    return false;
-  }
-
-  val = 0xff;
-
-  for (int i = 0; i < len; ++i) {
-    // 8th element should be 1
-    val &= (2 * i + 7);
-  }
-
-  if (val != testData[7]) {
-    printf("atomicAnd failed\n");
-    return false;
-  }
-
-  val = 0;
-
-  for (int i = 0; i < len; ++i) {
-    // 9th element should be 0xff
-    val |= (1 << i);
-  }
-
-  if (val != testData[8]) {
-    printf("atomicOr failed\n");
-    return false;
-  }
-
-  val = 0xff;
-
-  for (int i = 0; i < len; ++i) {
-    // 11th element should be 0xff
-    val ^= i;
-  }
-
-  if (val != testData[9]) {
-    printf("atomicXor failed\n");
-    return false;
-  }
-
-  return true;
-}
-
-int main(int argc, char **argv) {
-  // set device
-  hipDeviceProp_t device_prop;
-  int dev_id = findCudaDevice(argc, (const char **)argv);
-  HIPCHECK(hipGetDeviceProperties(&device_prop, dev_id));
-
-  if (!device_prop.managedMemory) {
-    // This samples requires being run on a device that supports Unified Memory
-    fprintf(stderr, "Unified Memory not supported on this device\n");
-    exit(EXIT_WAIVED);
-  }
-
-  if (device_prop.computeMode == hipComputeModeProhibited) {
-    // This sample requires being run with a default or process exclusive mode
-    fprintf(stderr,
-            "This sample requires a device in either default or process "
-            "exclusive mode\n");
-    exit(EXIT_WAIVED);
-  }
-
-  if (device_prop.major < 6) {
-    printf(
-        "%s: requires a minimum CUDA compute 6.0 capability, waiving "
-        "testing.\n",
-        argv[0]);
-    exit(EXIT_WAIVED);
-  }
-
-  unsigned int numThreads = 256;
-  unsigned int numBlocks = 64;
-  unsigned int numData = 10;
-
-  int *atom_arr;
-
-  if (device_prop.pageableMemoryAccess) {
-    printf("CAN access pageable memory\n");
-    atom_arr = (int *)malloc(sizeof(int) * numData);
-  } else {
-    printf("CANNOT access pageable memory\n");
-    HIPCHECK(hipMallocManaged(&atom_arr, sizeof(int) * numData));
-  }
-
-  for (unsigned int i = 0; i < numData; i++) atom_arr[i] = 0;
-
-  // To make the AND and XOR tests generate something other than 0...
-  atom_arr[7] = atom_arr[9] = 0xff;
-
-  atomicKernel<<<numBlocks, numThreads>>>(atom_arr);
-  atomicKernel_CPU(atom_arr, numBlocks * numThreads);
-
-  HIPCHECK(hipDeviceSynchronize());
-
-  // Compute & verify reference solution
-  int testResult = verify(atom_arr, 2 * numThreads * numBlocks);
-
-  if (device_prop.pageableMemoryAccess) {
-    free(atom_arr);
-  } else {
-    hipFree(atom_arr);
-  }
-
-  printf("systemWideAtomics completed, returned %s \n",
-         testResult ? "OK" : "ERROR!");
-  exit(testResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/0_Introduction/template/template.cu.hip b/src/samples/Samples/0_Introduction/template/template.cu.hip
index 4482e65..e69de29 100644
--- a/src/samples/Samples/0_Introduction/template/template.cu.hip
+++ b/src/samples/Samples/0_Introduction/template/template.cu.hip
@@ -1,167 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Template project which demonstrates the basics on how to setup a project
- * example application.
- * Host code.
- */
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-// includes CUDA
-#include <hip/hip_runtime.h>
-
-// includes, project
-#include <helper_cuda.h>
-#include <helper_functions.h> // helper functions for SDK examples
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-void runTest(int argc, char **argv);
-
-extern "C" void computeGold(float *reference, float *idata,
-                            const unsigned int len);
-
-////////////////////////////////////////////////////////////////////////////////
-//! Simple test kernel for device functionality
-//! @param g_idata  input data in global memory
-//! @param g_odata  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-__global__ void testKernel(float *g_idata, float *g_odata) {
-  // shared memory
-  // the size is determined by the host application
-  extern __shared__ float sdata[];
-
-  // access thread id
-  const unsigned int tid = threadIdx.x;
-  // access number of threads in this block
-  const unsigned int num_threads = blockDim.x;
-
-  // read in input data from global memory
-  sdata[tid] = g_idata[tid];
-  __syncthreads();
-
-  // perform some computations
-  sdata[tid] = (float)num_threads * sdata[tid];
-  __syncthreads();
-
-  // write data to global memory
-  g_odata[tid] = sdata[tid];
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) { runTest(argc, argv); }
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run a simple test for CUDA
-////////////////////////////////////////////////////////////////////////////////
-void runTest(int argc, char **argv) {
-  bool bTestResult = true;
-
-  printf("%s Starting...\n\n", argv[0]);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  int devID = findCudaDevice(argc, (const char **)argv);
-
-  StopWatchInterface *timer = 0;
-  sdkCreateTimer(&timer);
-  sdkStartTimer(&timer);
-
-  unsigned int num_threads = 32;
-  unsigned int mem_size = sizeof(float) * num_threads;
-
-  // allocate host memory
-  float *h_idata = (float *)malloc(mem_size);
-
-  // initalize the memory
-  for (unsigned int i = 0; i < num_threads; ++i) {
-    h_idata[i] = (float)i;
-  }
-
-  // allocate device memory
-  float *d_idata;
-  HIPCHECK(hipMalloc((void **)&d_idata, mem_size));
-  // copy host memory to device
-  HIPCHECK(
-      hipMemcpy(d_idata, h_idata, mem_size, hipMemcpyHostToDevice));
-
-  // allocate device memory for result
-  float *d_odata;
-  HIPCHECK(hipMalloc((void **)&d_odata, mem_size));
-
-  // setup execution parameters
-  dim3 grid(1, 1, 1);
-  dim3 threads(num_threads, 1, 1);
-
-  // execute the kernel
-  testKernel<<<grid, threads, mem_size>>>(d_idata, d_odata);
-
-  // check if kernel execution generated and error
-  getLastCudaError("Kernel execution failed");
-
-  // allocate mem for the result on host side
-  float *h_odata = (float *)malloc(mem_size);
-  // copy result from device to host
-  HIPCHECK(hipMemcpy(h_odata, d_odata, sizeof(float) * num_threads,
-                             hipMemcpyDeviceToHost));
-
-  sdkStopTimer(&timer);
-  printf("Processing time: %f (ms)\n", sdkGetTimerValue(&timer));
-  sdkDeleteTimer(&timer);
-
-  // compute reference solution
-  float *reference = (float *)malloc(mem_size);
-  computeGold(reference, h_idata, num_threads);
-
-  // check result
-  if (checkCmdLineFlag(argc, (const char **)argv, "regression")) {
-    // write file for regression test
-    sdkWriteFile("./data/regression.dat", h_odata, num_threads, 0.0f, false);
-  } else {
-    // custom output handling when no regression test running
-    // in this case check if the result is equivalent to the expected solution
-    bTestResult = compareData(reference, h_odata, num_threads, 0.0f, 0.0f);
-  }
-
-  // cleanup memory
-  free(h_idata);
-  free(h_odata);
-  free(reference);
-  HIPCHECK(hipFree(d_idata));
-  HIPCHECK(hipFree(d_odata));
-
-  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/0_Introduction/vectorAdd/vectorAdd.cu.hip b/src/samples/Samples/0_Introduction/vectorAdd/vectorAdd.cu.hip
index 7d24050..e69de29 100644
--- a/src/samples/Samples/0_Introduction/vectorAdd/vectorAdd.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAdd/vectorAdd.cu.hip
@@ -1,212 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/**
- * Vector addition: C = A + B.
- *
- * This sample is a very basic sample that implements element by element
- * vector addition. It is the same as the sample illustrating Chapter 2
- * of the programming guide with some additions like error checking.
- */
-
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-// For the CUDA runtime routines (prefixed with "cuda_")
-#include <hip/hip_runtime.h>
-
-#include <helper_cuda.h>
-/**
- * CUDA Kernel Device code
- *
- * Computes the vector addition of A and B into C. The 3 vectors have the same
- * number of elements numElements.
- */
-__global__ void vectorAdd(const float *A, const float *B, float *C,
-                          int numElements) {
-  int i = blockDim.x * blockIdx.x + threadIdx.x;
-
-  if (i < numElements) {
-    C[i] = A[i] + B[i] + 0.0f;
-  }
-}
-
-/**
- * Host main routine
- */
-int main(void) {
-  // Error code to check return values for CUDA calls
-  hipError_t err = hipSuccess;
-
-  // Print the vector length to be used, and compute its size
-  int numElements = 50000;
-  size_t size = numElements * sizeof(float);
-  printf("[Vector addition of %d elements]\n", numElements);
-
-  // Allocate the host input vector A
-  float *h_A = (float *)malloc(size);
-
-  // Allocate the host input vector B
-  float *h_B = (float *)malloc(size);
-
-  // Allocate the host output vector C
-  float *h_C = (float *)malloc(size);
-
-  // Verify that allocations succeeded
-  if (h_A == NULL || h_B == NULL || h_C == NULL) {
-    fprintf(stderr, "Failed to allocate host vectors!\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // Initialize the host input vectors
-  for (int i = 0; i < numElements; ++i) {
-    h_A[i] = rand() / (float)RAND_MAX;
-    h_B[i] = rand() / (float)RAND_MAX;
-  }
-
-  // Allocate the device input vector A
-  float *d_A = NULL;
-  err = hipMalloc((void **)&d_A, size);
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to allocate device vector A (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Allocate the device input vector B
-  float *d_B = NULL;
-  err = hipMalloc((void **)&d_B, size);
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to allocate device vector B (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Allocate the device output vector C
-  float *d_C = NULL;
-  err = hipMalloc((void **)&d_C, size);
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to allocate device vector C (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Copy the host input vectors A and B in host memory to the device input
-  // vectors in
-  // device memory
-  printf("Copy input data from the host memory to the CUDA device\n");
-  err = hipMemcpy(d_A, h_A, size, hipMemcpyHostToDevice);
-
-  if (err != hipSuccess) {
-    fprintf(stderr,
-            "Failed to copy vector A from host to device (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  err = hipMemcpy(d_B, h_B, size, hipMemcpyHostToDevice);
-
-  if (err != hipSuccess) {
-    fprintf(stderr,
-            "Failed to copy vector B from host to device (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Launch the Vector Add CUDA Kernel
-  int threadsPerBlock = 256;
-  int blocksPerGrid = (numElements + threadsPerBlock - 1) / threadsPerBlock;
-  printf("CUDA kernel launch with %d blocks of %d threads\n", blocksPerGrid,
-         threadsPerBlock);
-  vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);
-  err = hipGetLastError();
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to launch vectorAdd kernel (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Copy the device result vector in device memory to the host result vector
-  // in host memory.
-  printf("Copy output data from the CUDA device to the host memory\n");
-  err = hipMemcpy(h_C, d_C, size, hipMemcpyDeviceToHost);
-
-  if (err != hipSuccess) {
-    fprintf(stderr,
-            "Failed to copy vector C from device to host (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Verify that the result vector is correct
-  for (int i = 0; i < numElements; ++i) {
-    if (fabs(h_A[i] + h_B[i] - h_C[i]) > 1e-5) {
-      fprintf(stderr, "Result verification failed at element %d!\n", i);
-      exit(EXIT_FAILURE);
-    }
-  }
-
-  printf("Test PASSED\n");
-
-  // Free device global memory
-  err = hipFree(d_A);
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to free device vector A (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  err = hipFree(d_B);
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to free device vector B (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  err = hipFree(d_C);
-
-  if (err != hipSuccess) {
-    fprintf(stderr, "Failed to free device vector C (error code %s)!\n",
-            hipGetErrorString(err));
-    exit(EXIT_FAILURE);
-  }
-
-  // Free host memory
-  free(h_A);
-  free(h_B);
-  free(h_C);
-
-  printf("Done\n");
-  return 0;
-}
diff --git a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
index e69de29..c5919b7 100644
--- a/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
+++ b/src/samples/Samples/0_Introduction/vectorAdd_nvrtc/vectorAdd_kernel.cu.hip
@@ -0,0 +1,44 @@
+
+#include <hip/hip_runtime.h>
+/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *  * Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ *  * Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *  * Neither the name of NVIDIA CORPORATION nor the names of its
+ *    contributors may be used to endorse or promote products derived
+ *    from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
+ * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+ * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+ * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
+ * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/**
+ * CUDA Kernel Device code
+ *
+ * Computes the vector addition of A and B into C. The 3 vectors have the same
+ * number of elements numElements.
+ */
+
+extern "C" __global__ void vectorAdd(const float *A, const float *B, float *C,
+                                     int numElements) {
+  int i = blockDim.x * blockIdx.x + threadIdx.x;
+
+  if (i < numElements) {
+    C[i] = A[i] + B[i];
+  }
+}
diff --git a/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.cu.hip b/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.cu.hip
index b75032b..e69de29 100644
--- a/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.cu.hip
+++ b/src/samples/Samples/1_Utilities/bandwidthTest/bandwidthTest.cu.hip
@@ -1,969 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This is a simple test program to measure the memcopy bandwidth of the GPU.
- * It can measure device to device copy bandwidth, host to device copy bandwidth
- * for pageable and pinned memory, and device to host copy bandwidth for
- * pageable and pinned memory.
- *
- * Usage:
- * ./bandwidthTest [option]...
- */
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// includes
-#include <helper_cuda.h>  // helper functions for CUDA error checking and initialization
-#include <helper_functions.h>  // helper for shared functions common to CUDA Samples
-
-
-
-#include <cassert>
-#include <iostream>
-#include <memory>
-
-static const char *sSDKsample = "CUDA Bandwidth Test";
-
-// defines, project
-#define MEMCOPY_ITERATIONS 100
-#define DEFAULT_SIZE (32 * (1e6))      // 32 M
-#define DEFAULT_INCREMENT (4 * (1e6))  // 4 M
-#define CACHE_CLEAR_SIZE (16 * (1e6))  // 16 M
-
-// shmoo mode defines
-#define SHMOO_MEMSIZE_MAX (64 * (1e6))       // 64 M
-#define SHMOO_MEMSIZE_START (1e3)            // 1 KB
-#define SHMOO_INCREMENT_1KB (1e3)            // 1 KB
-#define SHMOO_INCREMENT_2KB (2 * 1e3)        // 2 KB
-#define SHMOO_INCREMENT_10KB (10 * (1e3))    // 10KB
-#define SHMOO_INCREMENT_100KB (100 * (1e3))  // 100 KB
-#define SHMOO_INCREMENT_1MB (1e6)            // 1 MB
-#define SHMOO_INCREMENT_2MB (2 * 1e6)        // 2 MB
-#define SHMOO_INCREMENT_4MB (4 * 1e6)        // 4 MB
-#define SHMOO_LIMIT_20KB (20 * (1e3))        // 20 KB
-#define SHMOO_LIMIT_50KB (50 * (1e3))        // 50 KB
-#define SHMOO_LIMIT_100KB (100 * (1e3))      // 100 KB
-#define SHMOO_LIMIT_1MB (1e6)                // 1 MB
-#define SHMOO_LIMIT_16MB (16 * 1e6)          // 16 MB
-#define SHMOO_LIMIT_32MB (32 * 1e6)          // 32 MB
-
-// CPU cache flush
-#define FLUSH_SIZE (256 * 1024 * 1024)
-char *flush_buf;
-
-// enums, project
-enum testMode { QUICK_MODE, RANGE_MODE, SHMOO_MODE };
-enum memcpyKind { DEVICE_TO_HOST, HOST_TO_DEVICE, DEVICE_TO_DEVICE };
-enum printMode { USER_READABLE, CSV };
-enum memoryMode { PINNED, PAGEABLE };
-
-const char *sMemoryCopyKind[] = {"Device to Host", "Host to Device",
-                                 "Device to Device", NULL};
-
-const char *sMemoryMode[] = {"PINNED", "PAGEABLE", NULL};
-
-// if true, use CPU based timing for everything
-static bool bDontUseGPUTiming;
-
-int *pArgc = NULL;
-char **pArgv = NULL;
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-int runTest(const int argc, const char **argv);
-void testBandwidth(unsigned int start, unsigned int end, unsigned int increment,
-                   testMode mode, memcpyKind kind, printMode printmode,
-                   memoryMode memMode, int startDevice, int endDevice, bool wc);
-void testBandwidthQuick(unsigned int size, memcpyKind kind, printMode printmode,
-                        memoryMode memMode, int startDevice, int endDevice,
-                        bool wc);
-void testBandwidthRange(unsigned int start, unsigned int end,
-                        unsigned int increment, memcpyKind kind,
-                        printMode printmode, memoryMode memMode,
-                        int startDevice, int endDevice, bool wc);
-void testBandwidthShmoo(memcpyKind kind, printMode printmode,
-                        memoryMode memMode, int startDevice, int endDevice,
-                        bool wc);
-float testDeviceToHostTransfer(unsigned int memSize, memoryMode memMode,
-                               bool wc);
-float testHostToDeviceTransfer(unsigned int memSize, memoryMode memMode,
-                               bool wc);
-float testDeviceToDeviceTransfer(unsigned int memSize);
-void printResultsReadable(unsigned int *memSizes, double *bandwidths,
-                          unsigned int count, memcpyKind kind,
-                          memoryMode memMode, int iNumDevs, bool wc);
-void printResultsCSV(unsigned int *memSizes, double *bandwidths,
-                     unsigned int count, memcpyKind kind, memoryMode memMode,
-                     int iNumDevs, bool wc);
-void printHelp(void);
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  pArgc = &argc;
-  pArgv = argv;
-
-  flush_buf = (char *)malloc(FLUSH_SIZE);
-
-  // set logfile name and start logs
-  printf("[%s] - Starting...\n", sSDKsample);
-
-  int iRetVal = runTest(argc, (const char **)argv);
-
-  if (iRetVal < 0) {
-    HIPCHECK(hipSetDevice(0));
-  }
-
-  // finish
-  printf("%s\n", (iRetVal == 0) ? "Result = PASS" : "Result = FAIL");
-
-  printf(
-      "\nNOTE: The CUDA Samples are not meant for performance measurements. "
-      "Results may vary when GPU Boost is enabled.\n");
-
-  free(flush_buf);
-
-  exit((iRetVal == 0) ? EXIT_SUCCESS : EXIT_FAILURE);
-}
-
-///////////////////////////////////////////////////////////////////////////////
-// Parse args, run the appropriate tests
-///////////////////////////////////////////////////////////////////////////////
-int runTest(const int argc, const char **argv) {
-  int start = DEFAULT_SIZE;
-  int end = DEFAULT_SIZE;
-  int startDevice = 0;
-  int endDevice = 0;
-  int increment = DEFAULT_INCREMENT;
-  testMode mode = QUICK_MODE;
-  bool htod = false;
-  bool dtoh = false;
-  bool dtod = false;
-  bool wc = false;
-  char *modeStr;
-  char *device = NULL;
-  printMode printmode = USER_READABLE;
-  char *memModeStr = NULL;
-  memoryMode memMode = PINNED;
-
-  // process command line args
-  if (checkCmdLineFlag(argc, argv, "help")) {
-    printHelp();
-    return 0;
-  }
-
-  if (checkCmdLineFlag(argc, argv, "csv")) {
-    printmode = CSV;
-  }
-
-  if (getCmdLineArgumentString(argc, argv, "memory", &memModeStr)) {
-    if (strcmp(memModeStr, "pageable") == 0) {
-      memMode = PAGEABLE;
-    } else if (strcmp(memModeStr, "pinned") == 0) {
-      memMode = PINNED;
-    } else {
-      printf("Invalid memory mode - valid modes are pageable or pinned\n");
-      printf("See --help for more information\n");
-      return -1000;
-    }
-  } else {
-    // default - pinned memory
-    memMode = PINNED;
-  }
-
-  if (getCmdLineArgumentString(argc, argv, "device", &device)) {
-    int deviceCount;
-    hipError_t error_id = hipGetDeviceCount(&deviceCount);
-
-    if (error_id != hipSuccess) {
-      printf("hipGetDeviceCount returned %d\n-> %s\n", (int)error_id,
-             hipGetErrorString(error_id));
-      exit(EXIT_FAILURE);
-    }
-
-    if (deviceCount == 0) {
-      printf("!!!!!No devices found!!!!!\n");
-      return -2000;
-    }
-
-    if (strcmp(device, "all") == 0) {
-      printf(
-          "\n!!!!!Cumulative Bandwidth to be computed from all the devices "
-          "!!!!!!\n\n");
-      startDevice = 0;
-      endDevice = deviceCount - 1;
-    } else {
-      startDevice = endDevice = atoi(device);
-
-      if (startDevice >= deviceCount || startDevice < 0) {
-        printf(
-            "\n!!!!!Invalid GPU number %d given hence default gpu %d will be "
-            "used !!!!!\n",
-            startDevice, 0);
-        startDevice = endDevice = 0;
-      }
-    }
-  }
-
-  printf("Running on...\n\n");
-
-  for (int currentDevice = startDevice; currentDevice <= endDevice;
-       currentDevice++) {
-    hipDeviceProp_t deviceProp;
-    hipError_t error_id = hipGetDeviceProperties(&deviceProp, currentDevice);
-
-    if (error_id == hipSuccess) {
-      printf(" Device %d: %s\n", currentDevice, deviceProp.name);
-
-      if (deviceProp.computeMode == hipComputeModeProhibited) {
-        fprintf(stderr,
-                "Error: device is running in <Compute Mode Prohibited>, no "
-                "threads can use ::cudaSetDevice().\n");
-        HIPCHECK(hipSetDevice(currentDevice));
-
-        exit(EXIT_FAILURE);
-      }
-    } else {
-      printf("hipGetDeviceProperties returned %d\n-> %s\n", (int)error_id,
-             hipGetErrorString(error_id));
-      HIPCHECK(hipSetDevice(currentDevice));
-
-      exit(EXIT_FAILURE);
-    }
-  }
-
-  if (getCmdLineArgumentString(argc, argv, "mode", &modeStr)) {
-    // figure out the mode
-    if (strcmp(modeStr, "quick") == 0) {
-      printf(" Quick Mode\n\n");
-      mode = QUICK_MODE;
-    } else if (strcmp(modeStr, "shmoo") == 0) {
-      printf(" Shmoo Mode\n\n");
-      mode = SHMOO_MODE;
-    } else if (strcmp(modeStr, "range") == 0) {
-      printf(" Range Mode\n\n");
-      mode = RANGE_MODE;
-    } else {
-      printf("Invalid mode - valid modes are quick, range, or shmoo\n");
-      printf("See --help for more information\n");
-      return -3000;
-    }
-  } else {
-    // default mode - quick
-    printf(" Quick Mode\n\n");
-    mode = QUICK_MODE;
-  }
-
-  if (checkCmdLineFlag(argc, argv, "htod")) {
-    htod = true;
-  }
-
-  if (checkCmdLineFlag(argc, argv, "dtoh")) {
-    dtoh = true;
-  }
-
-  if (checkCmdLineFlag(argc, argv, "dtod")) {
-    dtod = true;
-  }
-
-#if CUDART_VERSION >= 2020
-
-  if (checkCmdLineFlag(argc, argv, "wc")) {
-    wc = true;
-  }
-
-#endif
-
-  if (checkCmdLineFlag(argc, argv, "cputiming")) {
-    bDontUseGPUTiming = true;
-  }
-
-  if (!htod && !dtoh && !dtod) {
-    // default:  All
-    htod = true;
-    dtoh = true;
-    dtod = true;
-  }
-
-  if (RANGE_MODE == mode) {
-    if (checkCmdLineFlag(argc, (const char **)argv, "start")) {
-      start = getCmdLineArgumentInt(argc, argv, "start");
-
-      if (start <= 0) {
-        printf("Illegal argument - start must be greater than zero\n");
-        return -4000;
-      }
-    } else {
-      printf("Must specify a starting size in range mode\n");
-      printf("See --help for more information\n");
-      return -5000;
-    }
-
-    if (checkCmdLineFlag(argc, (const char **)argv, "end")) {
-      end = getCmdLineArgumentInt(argc, argv, "end");
-
-      if (end <= 0) {
-        printf("Illegal argument - end must be greater than zero\n");
-        return -6000;
-      }
-
-      if (start > end) {
-        printf("Illegal argument - start is greater than end\n");
-        return -7000;
-      }
-    } else {
-      printf("Must specify an end size in range mode.\n");
-      printf("See --help for more information\n");
-      return -8000;
-    }
-
-    if (checkCmdLineFlag(argc, argv, "increment")) {
-      increment = getCmdLineArgumentInt(argc, argv, "increment");
-
-      if (increment <= 0) {
-        printf("Illegal argument - increment must be greater than zero\n");
-        return -9000;
-      }
-    } else {
-      printf("Must specify an increment in user mode\n");
-      printf("See --help for more information\n");
-      return -10000;
-    }
-  }
-
-  if (htod) {
-    testBandwidth((unsigned int)start, (unsigned int)end,
-                  (unsigned int)increment, mode, HOST_TO_DEVICE, printmode,
-                  memMode, startDevice, endDevice, wc);
-  }
-
-  if (dtoh) {
-    testBandwidth((unsigned int)start, (unsigned int)end,
-                  (unsigned int)increment, mode, DEVICE_TO_HOST, printmode,
-                  memMode, startDevice, endDevice, wc);
-  }
-
-  if (dtod) {
-    testBandwidth((unsigned int)start, (unsigned int)end,
-                  (unsigned int)increment, mode, DEVICE_TO_DEVICE, printmode,
-                  memMode, startDevice, endDevice, wc);
-  }
-
-  // Ensure that we reset all CUDA Devices in question
-  for (int nDevice = startDevice; nDevice <= endDevice; nDevice++) {
-    hipSetDevice(nDevice);
-  }
-
-  return 0;
-}
-
-///////////////////////////////////////////////////////////////////////////////
-//  Run a bandwidth test
-///////////////////////////////////////////////////////////////////////////////
-void testBandwidth(unsigned int start, unsigned int end, unsigned int increment,
-                   testMode mode, memcpyKind kind, printMode printmode,
-                   memoryMode memMode, int startDevice, int endDevice,
-                   bool wc) {
-  switch (mode) {
-    case QUICK_MODE:
-      testBandwidthQuick(DEFAULT_SIZE, kind, printmode, memMode, startDevice,
-                         endDevice, wc);
-      break;
-
-    case RANGE_MODE:
-      testBandwidthRange(start, end, increment, kind, printmode, memMode,
-                         startDevice, endDevice, wc);
-      break;
-
-    case SHMOO_MODE:
-      testBandwidthShmoo(kind, printmode, memMode, startDevice, endDevice, wc);
-      break;
-
-    default:
-      break;
-  }
-}
-
-//////////////////////////////////////////////////////////////////////
-//  Run a quick mode bandwidth test
-//////////////////////////////////////////////////////////////////////
-void testBandwidthQuick(unsigned int size, memcpyKind kind, printMode printmode,
-                        memoryMode memMode, int startDevice, int endDevice,
-                        bool wc) {
-  testBandwidthRange(size, size, DEFAULT_INCREMENT, kind, printmode, memMode,
-                     startDevice, endDevice, wc);
-}
-
-///////////////////////////////////////////////////////////////////////
-//  Run a range mode bandwidth test
-//////////////////////////////////////////////////////////////////////
-void testBandwidthRange(unsigned int start, unsigned int end,
-                        unsigned int increment, memcpyKind kind,
-                        printMode printmode, memoryMode memMode,
-                        int startDevice, int endDevice, bool wc) {
-  // count the number of copies we're going to run
-  unsigned int count = 1 + ((end - start) / increment);
-
-  unsigned int *memSizes = (unsigned int *)malloc(count * sizeof(unsigned int));
-  double *bandwidths = (double *)malloc(count * sizeof(double));
-
-  // Before calculating the cumulative bandwidth, initialize bandwidths array to
-  // NULL
-  for (unsigned int i = 0; i < count; i++) {
-    bandwidths[i] = 0.0;
-  }
-
-  // Use the device asked by the user
-  for (int currentDevice = startDevice; currentDevice <= endDevice;
-       currentDevice++) {
-    hipSetDevice(currentDevice);
-
-    // run each of the copies
-    for (unsigned int i = 0; i < count; i++) {
-      memSizes[i] = start + i * increment;
-
-      switch (kind) {
-        case DEVICE_TO_HOST:
-          bandwidths[i] += testDeviceToHostTransfer(memSizes[i], memMode, wc);
-          break;
-
-        case HOST_TO_DEVICE:
-          bandwidths[i] += testHostToDeviceTransfer(memSizes[i], memMode, wc);
-          break;
-
-        case DEVICE_TO_DEVICE:
-          bandwidths[i] += testDeviceToDeviceTransfer(memSizes[i]);
-          break;
-      }
-    }
-  }  // Complete the bandwidth computation on all the devices
-
-  // print results
-  if (printmode == CSV) {
-    printResultsCSV(memSizes, bandwidths, count, kind, memMode,
-                    (1 + endDevice - startDevice), wc);
-  } else {
-    printResultsReadable(memSizes, bandwidths, count, kind, memMode,
-                         (1 + endDevice - startDevice), wc);
-  }
-
-  // clean up
-  free(memSizes);
-  free(bandwidths);
-}
-
-//////////////////////////////////////////////////////////////////////////////
-// Intense shmoo mode - covers a large range of values with varying increments
-//////////////////////////////////////////////////////////////////////////////
-void testBandwidthShmoo(memcpyKind kind, printMode printmode,
-                        memoryMode memMode, int startDevice, int endDevice,
-                        bool wc) {
-  // count the number of copies to make
-  unsigned int count =
-      1 + (SHMOO_LIMIT_20KB / SHMOO_INCREMENT_1KB) +
-      ((SHMOO_LIMIT_50KB - SHMOO_LIMIT_20KB) / SHMOO_INCREMENT_2KB) +
-      ((SHMOO_LIMIT_100KB - SHMOO_LIMIT_50KB) / SHMOO_INCREMENT_10KB) +
-      ((SHMOO_LIMIT_1MB - SHMOO_LIMIT_100KB) / SHMOO_INCREMENT_100KB) +
-      ((SHMOO_LIMIT_16MB - SHMOO_LIMIT_1MB) / SHMOO_INCREMENT_1MB) +
-      ((SHMOO_LIMIT_32MB - SHMOO_LIMIT_16MB) / SHMOO_INCREMENT_2MB) +
-      ((SHMOO_MEMSIZE_MAX - SHMOO_LIMIT_32MB) / SHMOO_INCREMENT_4MB);
-
-  unsigned int *memSizes = (unsigned int *)malloc(count * sizeof(unsigned int));
-  double *bandwidths = (double *)malloc(count * sizeof(double));
-
-  // Before calculating the cumulative bandwidth, initialize bandwidths array to
-  // NULL
-  for (unsigned int i = 0; i < count; i++) {
-    bandwidths[i] = 0.0;
-  }
-
-  // Use the device asked by the user
-  for (int currentDevice = startDevice; currentDevice <= endDevice;
-       currentDevice++) {
-    hipSetDevice(currentDevice);
-    // Run the shmoo
-    int iteration = 0;
-    unsigned int memSize = 0;
-
-    while (memSize <= SHMOO_MEMSIZE_MAX) {
-      if (memSize < SHMOO_LIMIT_20KB) {
-        memSize += SHMOO_INCREMENT_1KB;
-      } else if (memSize < SHMOO_LIMIT_50KB) {
-        memSize += SHMOO_INCREMENT_2KB;
-      } else if (memSize < SHMOO_LIMIT_100KB) {
-        memSize += SHMOO_INCREMENT_10KB;
-      } else if (memSize < SHMOO_LIMIT_1MB) {
-        memSize += SHMOO_INCREMENT_100KB;
-      } else if (memSize < SHMOO_LIMIT_16MB) {
-        memSize += SHMOO_INCREMENT_1MB;
-      } else if (memSize < SHMOO_LIMIT_32MB) {
-        memSize += SHMOO_INCREMENT_2MB;
-      } else {
-        memSize += SHMOO_INCREMENT_4MB;
-      }
-
-      memSizes[iteration] = memSize;
-
-      switch (kind) {
-        case DEVICE_TO_HOST:
-          bandwidths[iteration] +=
-              testDeviceToHostTransfer(memSizes[iteration], memMode, wc);
-          break;
-
-        case HOST_TO_DEVICE:
-          bandwidths[iteration] +=
-              testHostToDeviceTransfer(memSizes[iteration], memMode, wc);
-          break;
-
-        case DEVICE_TO_DEVICE:
-          bandwidths[iteration] +=
-              testDeviceToDeviceTransfer(memSizes[iteration]);
-          break;
-      }
-
-      iteration++;
-      printf(".");
-      fflush(0);
-    }
-  }  // Complete the bandwidth computation on all the devices
-
-  // print results
-  printf("\n");
-
-  if (CSV == printmode) {
-    printResultsCSV(memSizes, bandwidths, count, kind, memMode,
-                    (1 + endDevice - startDevice), wc);
-  } else {
-    printResultsReadable(memSizes, bandwidths, count, kind, memMode,
-                         (1 + endDevice - startDevice), wc);
-  }
-
-  // clean up
-  free(memSizes);
-  free(bandwidths);
-}
-
-///////////////////////////////////////////////////////////////////////////////
-//  test the bandwidth of a device to host memcopy of a specific size
-///////////////////////////////////////////////////////////////////////////////
-float testDeviceToHostTransfer(unsigned int memSize, memoryMode memMode,
-                               bool wc) {
-  StopWatchInterface *timer = NULL;
-  float elapsedTimeInMs = 0.0f;
-  float bandwidthInGBs = 0.0f;
-  unsigned char *h_idata = NULL;
-  unsigned char *h_odata = NULL;
-  hipEvent_t start, stop;
-
-  sdkCreateTimer(&timer);
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-
-  // allocate host memory
-  if (PINNED == memMode) {
-  // pinned memory mode - use special function to get OS-pinned memory
-#if CUDART_VERSION >= 2020
-    HIPCHECK(hipHostAlloc((void **)&h_idata, memSize,
-                                  (wc) ? hipHostMallocWriteCombined : 0));
-    HIPCHECK(hipHostAlloc((void **)&h_odata, memSize,
-                                  (wc) ? hipHostMallocWriteCombined : 0));
-#else
-    HIPCHECK(hipHostMalloc((void **)&h_idata, memSize));
-    HIPCHECK(hipHostMalloc((void **)&h_odata, memSize));
-#endif
-  } else {
-    // pageable memory mode - use malloc
-    h_idata = (unsigned char *)malloc(memSize);
-    h_odata = (unsigned char *)malloc(memSize);
-
-    if (h_idata == 0 || h_odata == 0) {
-      fprintf(stderr, "Not enough memory avaialable on host to run test!\n");
-      exit(EXIT_FAILURE);
-    }
-  }
-
-  // initialize the memory
-  for (unsigned int i = 0; i < memSize / sizeof(unsigned char); i++) {
-    h_idata[i] = (unsigned char)(i & 0xff);
-  }
-
-  // allocate device memory
-  unsigned char *d_idata;
-  HIPCHECK(hipMalloc((void **)&d_idata, memSize));
-
-  // initialize the device memory
-  HIPCHECK(
-      hipMemcpy(d_idata, h_idata, memSize, hipMemcpyHostToDevice));
-
-  // copy data from GPU to Host
-  if (PINNED == memMode) {
-    if (bDontUseGPUTiming) sdkStartTimer(&timer);
-    HIPCHECK(hipEventRecord(start, 0));
-    for (unsigned int i = 0; i < MEMCOPY_ITERATIONS; i++) {
-      HIPCHECK(hipMemcpyAsync(h_odata, d_idata, memSize,
-                                      hipMemcpyDeviceToHost, 0));
-    }
-    HIPCHECK(hipEventRecord(stop, 0));
-    HIPCHECK(hipDeviceSynchronize());
-    HIPCHECK(hipEventElapsedTime(&elapsedTimeInMs, start, stop));
-    if (bDontUseGPUTiming) {
-      sdkStopTimer(&timer);
-      elapsedTimeInMs = sdkGetTimerValue(&timer);
-      sdkResetTimer(&timer);
-    }
-  } else {
-    elapsedTimeInMs = 0;
-    for (unsigned int i = 0; i < MEMCOPY_ITERATIONS; i++) {
-      sdkStartTimer(&timer);
-      HIPCHECK(
-          hipMemcpy(h_odata, d_idata, memSize, hipMemcpyDeviceToHost));
-      sdkStopTimer(&timer);
-      elapsedTimeInMs += sdkGetTimerValue(&timer);
-      sdkResetTimer(&timer);
-      memset(flush_buf, i, FLUSH_SIZE);
-    }
-  }
-
-  // calculate bandwidth in GB/s
-  double time_s = elapsedTimeInMs / 1e3;
-  bandwidthInGBs = (memSize * (float)MEMCOPY_ITERATIONS) / (double)1e9;
-  bandwidthInGBs = bandwidthInGBs / time_s;
-  // clean up memory
-  HIPCHECK(hipEventDestroy(stop));
-  HIPCHECK(hipEventDestroy(start));
-  sdkDeleteTimer(&timer);
-
-  if (PINNED == memMode) {
-    HIPCHECK(hipHostFree(h_idata));
-    HIPCHECK(hipHostFree(h_odata));
-  } else {
-    free(h_idata);
-    free(h_odata);
-  }
-
-  HIPCHECK(hipFree(d_idata));
-
-  return bandwidthInGBs;
-}
-
-///////////////////////////////////////////////////////////////////////////////
-//! test the bandwidth of a host to device memcopy of a specific size
-///////////////////////////////////////////////////////////////////////////////
-float testHostToDeviceTransfer(unsigned int memSize, memoryMode memMode,
-                               bool wc) {
-  StopWatchInterface *timer = NULL;
-  float elapsedTimeInMs = 0.0f;
-  float bandwidthInGBs = 0.0f;
-  hipEvent_t start, stop;
-  sdkCreateTimer(&timer);
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-
-  // allocate host memory
-  unsigned char *h_odata = NULL;
-
-  if (PINNED == memMode) {
-#if CUDART_VERSION >= 2020
-    // pinned memory mode - use special function to get OS-pinned memory
-    HIPCHECK(hipHostAlloc((void **)&h_odata, memSize,
-                                  (wc) ? hipHostMallocWriteCombined : 0));
-#else
-    // pinned memory mode - use special function to get OS-pinned memory
-    HIPCHECK(hipHostMalloc((void **)&h_odata, memSize));
-#endif
-  } else {
-    // pageable memory mode - use malloc
-    h_odata = (unsigned char *)malloc(memSize);
-
-    if (h_odata == 0) {
-      fprintf(stderr, "Not enough memory available on host to run test!\n");
-      exit(EXIT_FAILURE);
-    }
-  }
-
-  unsigned char *h_cacheClear1 = (unsigned char *)malloc(CACHE_CLEAR_SIZE);
-  unsigned char *h_cacheClear2 = (unsigned char *)malloc(CACHE_CLEAR_SIZE);
-
-  if (h_cacheClear1 == 0 || h_cacheClear2 == 0) {
-    fprintf(stderr, "Not enough memory available on host to run test!\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // initialize the memory
-  for (unsigned int i = 0; i < memSize / sizeof(unsigned char); i++) {
-    h_odata[i] = (unsigned char)(i & 0xff);
-  }
-
-  for (unsigned int i = 0; i < CACHE_CLEAR_SIZE / sizeof(unsigned char); i++) {
-    h_cacheClear1[i] = (unsigned char)(i & 0xff);
-    h_cacheClear2[i] = (unsigned char)(0xff - (i & 0xff));
-  }
-
-  // allocate device memory
-  unsigned char *d_idata;
-  HIPCHECK(hipMalloc((void **)&d_idata, memSize));
-
-  // copy host memory to device memory
-  if (PINNED == memMode) {
-    if (bDontUseGPUTiming) sdkStartTimer(&timer);
-    HIPCHECK(hipEventRecord(start, 0));
-    for (unsigned int i = 0; i < MEMCOPY_ITERATIONS; i++) {
-      HIPCHECK(hipMemcpyAsync(d_idata, h_odata, memSize,
-                                      hipMemcpyHostToDevice, 0));
-    }
-    HIPCHECK(hipEventRecord(stop, 0));
-    HIPCHECK(hipDeviceSynchronize());
-    HIPCHECK(hipEventElapsedTime(&elapsedTimeInMs, start, stop));
-    if (bDontUseGPUTiming) {
-      sdkStopTimer(&timer);
-      elapsedTimeInMs = sdkGetTimerValue(&timer);
-      sdkResetTimer(&timer);
-    }
-  } else {
-    elapsedTimeInMs = 0;
-    for (unsigned int i = 0; i < MEMCOPY_ITERATIONS; i++) {
-      sdkStartTimer(&timer);
-      HIPCHECK(
-          hipMemcpy(d_idata, h_odata, memSize, hipMemcpyHostToDevice));
-      sdkStopTimer(&timer);
-      elapsedTimeInMs += sdkGetTimerValue(&timer);
-      sdkResetTimer(&timer);
-      memset(flush_buf, i, FLUSH_SIZE);
-    }
-  }
-
-  // calculate bandwidth in GB/s
-  double time_s = elapsedTimeInMs / 1e3;
-  bandwidthInGBs = (memSize * (float)MEMCOPY_ITERATIONS) / (double)1e9;
-  bandwidthInGBs = bandwidthInGBs / time_s;
-  // clean up memory
-  HIPCHECK(hipEventDestroy(stop));
-  HIPCHECK(hipEventDestroy(start));
-  sdkDeleteTimer(&timer);
-
-  if (PINNED == memMode) {
-    HIPCHECK(hipHostFree(h_odata));
-  } else {
-    free(h_odata);
-  }
-
-  free(h_cacheClear1);
-  free(h_cacheClear2);
-  HIPCHECK(hipFree(d_idata));
-
-  return bandwidthInGBs;
-}
-
-///////////////////////////////////////////////////////////////////////////////
-//! test the bandwidth of a device to device memcopy of a specific size
-///////////////////////////////////////////////////////////////////////////////
-float testDeviceToDeviceTransfer(unsigned int memSize) {
-  StopWatchInterface *timer = NULL;
-  float elapsedTimeInMs = 0.0f;
-  float bandwidthInGBs = 0.0f;
-  hipEvent_t start, stop;
-
-  sdkCreateTimer(&timer);
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-
-  // allocate host memory
-  unsigned char *h_idata = (unsigned char *)malloc(memSize);
-
-  if (h_idata == 0) {
-    fprintf(stderr, "Not enough memory avaialable on host to run test!\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // initialize the host memory
-  for (unsigned int i = 0; i < memSize / sizeof(unsigned char); i++) {
-    h_idata[i] = (unsigned char)(i & 0xff);
-  }
-
-  // allocate device memory
-  unsigned char *d_idata;
-  HIPCHECK(hipMalloc((void **)&d_idata, memSize));
-  unsigned char *d_odata;
-  HIPCHECK(hipMalloc((void **)&d_odata, memSize));
-
-  // initialize memory
-  HIPCHECK(
-      hipMemcpy(d_idata, h_idata, memSize, hipMemcpyHostToDevice));
-
-  // run the memcopy
-  sdkStartTimer(&timer);
-  HIPCHECK(hipEventRecord(start, 0));
-
-  for (unsigned int i = 0; i < MEMCOPY_ITERATIONS; i++) {
-    HIPCHECK(
-        hipMemcpy(d_odata, d_idata, memSize, hipMemcpyDeviceToDevice));
-  }
-
-  HIPCHECK(hipEventRecord(stop, 0));
-
-  // Since device to device memory copies are non-blocking,
-  // cudaDeviceSynchronize() is required in order to get
-  // proper timing.
-  HIPCHECK(hipDeviceSynchronize());
-
-  // get the total elapsed time in ms
-  sdkStopTimer(&timer);
-  HIPCHECK(hipEventElapsedTime(&elapsedTimeInMs, start, stop));
-
-  if (bDontUseGPUTiming) {
-    elapsedTimeInMs = sdkGetTimerValue(&timer);
-  }
-
-  // calculate bandwidth in GB/s
-  double time_s = elapsedTimeInMs / 1e3;
-  bandwidthInGBs = (2.0f * memSize * (float)MEMCOPY_ITERATIONS) / (double)1e9;
-  bandwidthInGBs = bandwidthInGBs / time_s;
-
-  // clean up memory
-  sdkDeleteTimer(&timer);
-  free(h_idata);
-  HIPCHECK(hipEventDestroy(stop));
-  HIPCHECK(hipEventDestroy(start));
-  HIPCHECK(hipFree(d_idata));
-  HIPCHECK(hipFree(d_odata));
-
-  return bandwidthInGBs;
-}
-
-/////////////////////////////////////////////////////////
-// print results in an easily read format
-////////////////////////////////////////////////////////
-void printResultsReadable(unsigned int *memSizes, double *bandwidths,
-                          unsigned int count, memcpyKind kind,
-                          memoryMode memMode, int iNumDevs, bool wc) {
-  printf(" %s Bandwidth, %i Device(s)\n", sMemoryCopyKind[kind], iNumDevs);
-  printf(" %s Memory Transfers\n", sMemoryMode[memMode]);
-
-  if (wc) {
-    printf(" Write-Combined Memory Writes are Enabled");
-  }
-
-  printf("   Transfer Size (Bytes)\tBandwidth(GB/s)\n");
-  unsigned int i;
-
-  for (i = 0; i < (count - 1); i++) {
-    printf("   %u\t\t\t%s%.1f\n", memSizes[i],
-           (memSizes[i] < 10000) ? "\t" : "", bandwidths[i]);
-  }
-
-  printf("   %u\t\t\t%s%.1f\n\n", memSizes[i],
-         (memSizes[i] < 10000) ? "\t" : "", bandwidths[i]);
-}
-
-///////////////////////////////////////////////////////////////////////////
-// print results in a database format
-///////////////////////////////////////////////////////////////////////////
-void printResultsCSV(unsigned int *memSizes, double *bandwidths,
-                     unsigned int count, memcpyKind kind, memoryMode memMode,
-                     int iNumDevs, bool wc) {
-  std::string sConfig;
-
-  // log config information
-  if (kind == DEVICE_TO_DEVICE) {
-    sConfig += "D2D";
-  } else {
-    if (kind == DEVICE_TO_HOST) {
-      sConfig += "D2H";
-    } else if (kind == HOST_TO_DEVICE) {
-      sConfig += "H2D";
-    }
-
-    if (memMode == PAGEABLE) {
-      sConfig += "-Paged";
-    } else if (memMode == PINNED) {
-      sConfig += "-Pinned";
-
-      if (wc) {
-        sConfig += "-WriteCombined";
-      }
-    }
-  }
-
-  unsigned int i;
-  double dSeconds = 0.0;
-
-  for (i = 0; i < count; i++) {
-    dSeconds = (double)memSizes[i] / (bandwidths[i] * (double)(1e9));
-    printf(
-        "bandwidthTest-%s, Bandwidth = %.1f GB/s, Time = %.5f s, Size = %u "
-        "bytes, NumDevsUsed = %d\n",
-        sConfig.c_str(), bandwidths[i], dSeconds, memSizes[i], iNumDevs);
-  }
-}
-
-///////////////////////////////////////////////////////////////////////////
-// Print help screen
-///////////////////////////////////////////////////////////////////////////
-void printHelp(void) {
-  printf("Usage:  bandwidthTest [OPTION]...\n");
-  printf(
-      "Test the bandwidth for device to host, host to device, and device to "
-      "device transfers\n");
-  printf("\n");
-  printf(
-      "Example:  measure the bandwidth of device to host pinned memory copies "
-      "in the range 1024 Bytes to 102400 Bytes in 1024 Byte increments\n");
-  printf(
-      "./bandwidthTest --memory=pinned --mode=range --start=1024 --end=102400 "
-      "--increment=1024 --dtoh\n");
-
-  printf("\n");
-  printf("Options:\n");
-  printf("--help\tDisplay this help menu\n");
-  printf("--csv\tPrint results as a CSV\n");
-  printf("--device=[deviceno]\tSpecify the device device to be used\n");
-  printf("  all - compute cumulative bandwidth on all the devices\n");
-  printf("  0,1,2,...,n - Specify any particular device to be used\n");
-  printf("--memory=[MEMMODE]\tSpecify which memory mode to use\n");
-  printf("  pageable - pageable memory\n");
-  printf("  pinned   - non-pageable system memory\n");
-  printf("--mode=[MODE]\tSpecify the mode to use\n");
-  printf("  quick - performs a quick measurement\n");
-  printf("  range - measures a user-specified range of values\n");
-  printf("  shmoo - performs an intense shmoo of a large range of values\n");
-
-  printf("--htod\tMeasure host to device transfers\n");
-  printf("--dtoh\tMeasure device to host transfers\n");
-  printf("--dtod\tMeasure device to device transfers\n");
-#if CUDART_VERSION >= 2020
-  printf("--wc\tAllocate pinned memory as write-combined\n");
-#endif
-  printf("--cputiming\tForce CPU-based timing always\n");
-
-  printf("Range mode options\n");
-  printf("--start=[SIZE]\tStarting transfer size in bytes\n");
-  printf("--end=[SIZE]\tEnding transfer size in bytes\n");
-  printf("--increment=[SIZE]\tIncrement size in bytes\n");
-}
diff --git a/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip b/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip
index 741c83f..e69de29 100644
--- a/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip
+++ b/src/samples/Samples/1_Utilities/topologyQuery/topologyQuery.cu.hip
@@ -1,84 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample demonstrates how to use query information on the current system
- * topology using a SDK 8.0 API.
- */
-
-// includes CUDA
-#include <hip/hip_runtime.h>
-
-// includes, project
-#include <helper_cuda.h>
-#include <helper_functions.h>  // helper for shared that are common to CUDA Samples
-
-int main(int argc, char **argv) {
-  int deviceCount = 0;
-  HIPCHECK(hipGetDeviceCount(&deviceCount));
-
-  // Enumerates Device <-> Device links
-  for (int device1 = 0; device1 < deviceCount; device1++) {
-    for (int device2 = 0; device2 < deviceCount; device2++) {
-      if (device1 == device2) continue;
-
-      int perfRank = 0;
-      int atomicSupported = 0;
-      int accessSupported = 0;
-
-      HIPCHECK(hipDeviceGetP2PAttribute(
-          &accessSupported, hipDevP2PAttrAccessSupported, device1, device2));
-      HIPCHECK(hipDeviceGetP2PAttribute(
-          &perfRank, hipDevP2PAttrPerformanceRank, device1, device2));
-      HIPCHECK(hipDeviceGetP2PAttribute(
-          &atomicSupported, hipDevP2PAttrNativeAtomicSupported, device1,
-          device2));
-
-      if (accessSupported) {
-        std::cout << "GPU" << device1 << " <-> GPU" << device2 << ":"
-                  << std::endl;
-        std::cout << "  * Atomic Supported: "
-                  << (atomicSupported ? "yes" : "no") << std::endl;
-        std::cout << "  * Perf Rank: " << perfRank << std::endl;
-      }
-    }
-  }
-
-  // Enumerates Device <-> Host links
-  for (int device = 0; device < deviceCount; device++) {
-    int atomicSupported = 0;
-    HIPCHECK(hipDeviceGetAttribute(
-        &atomicSupported, hipDeviceAttributeHostNativeAtomicSupported, device));
-    std::cout << "GPU" << device << " <-> CPU:" << std::endl;
-    std::cout << "  * Atomic Supported: " << (atomicSupported ? "yes" : "no")
-              << std::endl;
-  }
-
-  return 0;
-}
-  << std::endl;
-  }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/convolutionSeparable/convolutionSeparable.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/convolutionSeparable/convolutionSeparable.cu.hip
index 4742ee7..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/convolutionSeparable/convolutionSeparable.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/convolutionSeparable/convolutionSeparable.cu.hip
@@ -1,216 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <helper_cuda.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include "convolutionSeparable_common.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// Convolution kernel storage
-////////////////////////////////////////////////////////////////////////////////
-__constant__ float c_Kernel[KERNEL_LENGTH];
-
-extern "C" void setConvolutionKernel(float *h_Kernel) {
-  hipMemcpyToSymbol(HIP_SYMBOL(c_Kernel), h_Kernel, KERNEL_LENGTH * sizeof(float));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Row convolution filter
-////////////////////////////////////////////////////////////////////////////////
-#define ROWS_BLOCKDIM_X 16
-#define ROWS_BLOCKDIM_Y 4
-#define ROWS_RESULT_STEPS 8
-#define ROWS_HALO_STEPS 1
-
-__global__ void convolutionRowsKernel(float *d_Dst, float *d_Src, int imageW,
-                                      int imageH, int pitch) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ float
-      s_Data[ROWS_BLOCKDIM_Y][(ROWS_RESULT_STEPS + 2 * ROWS_HALO_STEPS) *
-                              ROWS_BLOCKDIM_X];
-
-  // Offset to the left halo edge
-  const int baseX =
-      (blockIdx.x * ROWS_RESULT_STEPS - ROWS_HALO_STEPS) * ROWS_BLOCKDIM_X +
-      threadIdx.x;
-  const int baseY = blockIdx.y * ROWS_BLOCKDIM_Y + threadIdx.y;
-
-  d_Src += baseY * pitch + baseX;
-  d_Dst += baseY * pitch + baseX;
-
-// Load main data
-#pragma unroll
-
-  for (int i = ROWS_HALO_STEPS; i < ROWS_HALO_STEPS + ROWS_RESULT_STEPS; i++) {
-    s_Data[threadIdx.y][threadIdx.x + i * ROWS_BLOCKDIM_X] =
-        d_Src[i * ROWS_BLOCKDIM_X];
-  }
-
-// Load left halo
-#pragma unroll
-
-  for (int i = 0; i < ROWS_HALO_STEPS; i++) {
-    s_Data[threadIdx.y][threadIdx.x + i * ROWS_BLOCKDIM_X] =
-        (baseX >= -i * ROWS_BLOCKDIM_X) ? d_Src[i * ROWS_BLOCKDIM_X] : 0;
-  }
-
-// Load right halo
-#pragma unroll
-
-  for (int i = ROWS_HALO_STEPS + ROWS_RESULT_STEPS;
-       i < ROWS_HALO_STEPS + ROWS_RESULT_STEPS + ROWS_HALO_STEPS; i++) {
-    s_Data[threadIdx.y][threadIdx.x + i * ROWS_BLOCKDIM_X] =
-        (imageW - baseX > i * ROWS_BLOCKDIM_X) ? d_Src[i * ROWS_BLOCKDIM_X] : 0;
-  }
-
-  // Compute and store results
-  cg::sync(cta);
-#pragma unroll
-
-  for (int i = ROWS_HALO_STEPS; i < ROWS_HALO_STEPS + ROWS_RESULT_STEPS; i++) {
-    float sum = 0;
-
-#pragma unroll
-
-    for (int j = -KERNEL_RADIUS; j <= KERNEL_RADIUS; j++) {
-      sum += c_Kernel[KERNEL_RADIUS - j] *
-             s_Data[threadIdx.y][threadIdx.x + i * ROWS_BLOCKDIM_X + j];
-    }
-
-    d_Dst[i * ROWS_BLOCKDIM_X] = sum;
-  }
-}
-
-extern "C" void convolutionRowsGPU(float *d_Dst, float *d_Src, int imageW,
-                                   int imageH) {
-  assert(ROWS_BLOCKDIM_X * ROWS_HALO_STEPS >= KERNEL_RADIUS);
-  assert(imageW % (ROWS_RESULT_STEPS * ROWS_BLOCKDIM_X) == 0);
-  assert(imageH % ROWS_BLOCKDIM_Y == 0);
-
-  dim3 blocks(imageW / (ROWS_RESULT_STEPS * ROWS_BLOCKDIM_X),
-              imageH / ROWS_BLOCKDIM_Y);
-  dim3 threads(ROWS_BLOCKDIM_X, ROWS_BLOCKDIM_Y);
-
-  convolutionRowsKernel<<<blocks, threads>>>(d_Dst, d_Src, imageW, imageH,
-                                             imageW);
-  getLastCudaError("convolutionRowsKernel() execution failed\n");
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Column convolution filter
-////////////////////////////////////////////////////////////////////////////////
-#define COLUMNS_BLOCKDIM_X 16
-#define COLUMNS_BLOCKDIM_Y 8
-#define COLUMNS_RESULT_STEPS 8
-#define COLUMNS_HALO_STEPS 1
-
-__global__ void convolutionColumnsKernel(float *d_Dst, float *d_Src, int imageW,
-                                         int imageH, int pitch) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ float s_Data[COLUMNS_BLOCKDIM_X][(COLUMNS_RESULT_STEPS +
-                                               2 * COLUMNS_HALO_STEPS) *
-                                                  COLUMNS_BLOCKDIM_Y +
-                                              1];
-
-  // Offset to the upper halo edge
-  const int baseX = blockIdx.x * COLUMNS_BLOCKDIM_X + threadIdx.x;
-  const int baseY = (blockIdx.y * COLUMNS_RESULT_STEPS - COLUMNS_HALO_STEPS) *
-                        COLUMNS_BLOCKDIM_Y +
-                    threadIdx.y;
-  d_Src += baseY * pitch + baseX;
-  d_Dst += baseY * pitch + baseX;
-
-// Main data
-#pragma unroll
-
-  for (int i = COLUMNS_HALO_STEPS;
-       i < COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS; i++) {
-    s_Data[threadIdx.x][threadIdx.y + i * COLUMNS_BLOCKDIM_Y] =
-        d_Src[i * COLUMNS_BLOCKDIM_Y * pitch];
-  }
-
-// Upper halo
-#pragma unroll
-
-  for (int i = 0; i < COLUMNS_HALO_STEPS; i++) {
-    s_Data[threadIdx.x][threadIdx.y + i * COLUMNS_BLOCKDIM_Y] =
-        (baseY >= -i * COLUMNS_BLOCKDIM_Y)
-            ? d_Src[i * COLUMNS_BLOCKDIM_Y * pitch]
-            : 0;
-  }
-
-// Lower halo
-#pragma unroll
-
-  for (int i = COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS;
-       i < COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS + COLUMNS_HALO_STEPS;
-       i++) {
-    s_Data[threadIdx.x][threadIdx.y + i * COLUMNS_BLOCKDIM_Y] =
-        (imageH - baseY > i * COLUMNS_BLOCKDIM_Y)
-            ? d_Src[i * COLUMNS_BLOCKDIM_Y * pitch]
-            : 0;
-  }
-
-  // Compute and store results
-  cg::sync(cta);
-#pragma unroll
-
-  for (int i = COLUMNS_HALO_STEPS;
-       i < COLUMNS_HALO_STEPS + COLUMNS_RESULT_STEPS; i++) {
-    float sum = 0;
-#pragma unroll
-
-    for (int j = -KERNEL_RADIUS; j <= KERNEL_RADIUS; j++) {
-      sum += c_Kernel[KERNEL_RADIUS - j] *
-             s_Data[threadIdx.x][threadIdx.y + i * COLUMNS_BLOCKDIM_Y + j];
-    }
-
-    d_Dst[i * COLUMNS_BLOCKDIM_Y * pitch] = sum;
-  }
-}
-
-extern "C" void convolutionColumnsGPU(float *d_Dst, float *d_Src, int imageW,
-                                      int imageH) {
-  assert(COLUMNS_BLOCKDIM_Y * COLUMNS_HALO_STEPS >= KERNEL_RADIUS);
-  assert(imageW % COLUMNS_BLOCKDIM_X == 0);
-  assert(imageH % (COLUMNS_RESULT_STEPS * COLUMNS_BLOCKDIM_Y) == 0);
-
-  dim3 blocks(imageW / COLUMNS_BLOCKDIM_X,
-              imageH / (COLUMNS_RESULT_STEPS * COLUMNS_BLOCKDIM_Y));
-  dim3 threads(COLUMNS_BLOCKDIM_X, COLUMNS_BLOCKDIM_Y);
-
-  convolutionColumnsKernel<<<blocks, threads>>>(d_Dst, d_Src, imageW, imageH,
-                                                imageW);
-  getLastCudaError("convolutionColumnsKernel() execution failed\n");
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip
index 36cdb93..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/convolutionTexture/convolutionTexture.cu.hip
@@ -1,166 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-#include <helper_cuda.h>
-
-#include "convolutionTexture_common.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// GPU-specific defines
-////////////////////////////////////////////////////////////////////////////////
-// Maps to a single instruction on G8x / G9x / G10x
-#define IMAD(a, b, c) (__mul24((a), (b)) + (c))
-
-// Use unrolled innermost convolution loop
-#define UNROLL_INNER 1
-
-// Round a / b to nearest higher integer value
-inline int iDivUp(int a, int b) { return (a % b != 0) ? (a / b + 1) : (a / b); }
-
-// Align a to nearest higher multiple of b
-inline int iAlignUp(int a, int b) { return (a % b != 0) ? (a - a % b + b) : a; }
-
-////////////////////////////////////////////////////////////////////////////////
-// Convolution kernel and input array storage
-////////////////////////////////////////////////////////////////////////////////
-__constant__ float c_Kernel[KERNEL_LENGTH];
-
-extern "C" void setConvolutionKernel(float *h_Kernel) {
-  hipMemcpyToSymbol(HIP_SYMBOL(c_Kernel), h_Kernel, KERNEL_LENGTH * sizeof(float));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Loop unrolling templates, needed for best performance
-////////////////////////////////////////////////////////////////////////////////
-template <int i>
-__device__ float convolutionRow(float x, float y, hipTextureObject_t texSrc) {
-  return tex2D<float>(texSrc, x + (float)(KERNEL_RADIUS - i), y) * c_Kernel[i] +
-         convolutionRow<i - 1>(x, y, texSrc);
-}
-
-template <>
-__device__ float convolutionRow<-1>(float x, float y,
-                                    hipTextureObject_t texSrc) {
-  return 0;
-}
-
-template <int i>
-__device__ float convolutionColumn(float x, float y,
-                                   hipTextureObject_t texSrc) {
-  return tex2D<float>(texSrc, x, y + (float)(KERNEL_RADIUS - i)) * c_Kernel[i] +
-         convolutionColumn<i - 1>(x, y, texSrc);
-}
-
-template <>
-__device__ float convolutionColumn<-1>(float x, float y,
-                                       hipTextureObject_t texSrc) {
-  return 0;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Row convolution filter
-////////////////////////////////////////////////////////////////////////////////
-__global__ void convolutionRowsKernel(float *d_Dst, int imageW, int imageH,
-                                      hipTextureObject_t texSrc) {
-  const int ix = IMAD(blockDim.x, blockIdx.x, threadIdx.x);
-  const int iy = IMAD(blockDim.y, blockIdx.y, threadIdx.y);
-  const float x = (float)ix + 0.5f;
-  const float y = (float)iy + 0.5f;
-
-  if (ix >= imageW || iy >= imageH) {
-    return;
-  }
-
-  float sum = 0;
-
-#if (UNROLL_INNER)
-  sum = convolutionRow<2 * KERNEL_RADIUS>(x, y, texSrc);
-#else
-
-  for (int k = -KERNEL_RADIUS; k <= KERNEL_RADIUS; k++) {
-    sum += tex2D<float>(texSrc, x + (float)k, y) * c_Kernel[KERNEL_RADIUS - k];
-  }
-
-#endif
-
-  d_Dst[IMAD(iy, imageW, ix)] = sum;
-}
-
-extern "C" void convolutionRowsGPU(float *d_Dst, hipArray *a_Src, int imageW,
-                                   int imageH, hipTextureObject_t texSrc) {
-  dim3 threads(16, 12);
-  dim3 blocks(iDivUp(imageW, threads.x), iDivUp(imageH, threads.y));
-
-  convolutionRowsKernel<<<blocks, threads>>>(d_Dst, imageW, imageH, texSrc);
-  getLastCudaError("convolutionRowsKernel() execution failed\n");
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Column convolution filter
-////////////////////////////////////////////////////////////////////////////////
-__global__ void convolutionColumnsKernel(float *d_Dst, int imageW, int imageH,
-                                         hipTextureObject_t texSrc) {
-  const int ix = IMAD(blockDim.x, blockIdx.x, threadIdx.x);
-  const int iy = IMAD(blockDim.y, blockIdx.y, threadIdx.y);
-  const float x = (float)ix + 0.5f;
-  const float y = (float)iy + 0.5f;
-
-  if (ix >= imageW || iy >= imageH) {
-    return;
-  }
-
-  float sum = 0;
-
-#if (UNROLL_INNER)
-  sum = convolutionColumn<2 * KERNEL_RADIUS>(x, y, texSrc);
-#else
-
-  for (int k = -KERNEL_RADIUS; k <= KERNEL_RADIUS; k++) {
-    sum += tex2D<float>(texSrc, x, y + (float)k) * c_Kernel[KERNEL_RADIUS - k];
-  }
-
-#endif
-
-  d_Dst[IMAD(iy, imageW, ix)] = sum;
-}
-
-extern "C" void convolutionColumnsGPU(float *d_Dst, hipArray *a_Src,
-                                      int imageW, int imageH,
-                                      hipTextureObject_t texSrc) {
-  dim3 threads(16, 12);
-  dim3 blocks(iDivUp(imageW, threads.x), iDivUp(imageH, threads.y));
-
-  convolutionColumnsKernel<<<blocks, threads>>>(d_Dst, imageW, imageH, texSrc);
-  getLastCudaError("convolutionColumnsKernel() execution failed\n");
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip
index b92e2a2..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/dct8x8/dct8x8.cu.hip
@@ -1,671 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/**
-**************************************************************************
-* \file dct8x8.cu
-* \brief Contains entry point, wrappers to host and device code and benchmark.
-*
-* This sample implements forward and inverse Discrete Cosine Transform to blocks
-* of image pixels (of 8x8 size), as in JPEG standard. The typical work flow is
-*as
-* follows:
-* 1. Run CPU version (Host code) and measure execution time;
-* 2. Run CUDA version (Device code) and measure execution time;
-* 3. Output execution timings and calculate CUDA speedup.
-*/
-
-
-#include <hip/hip_runtime.h>
-#include "Common.h"
-#include "DCT8x8_Gold.h"
-#include "BmpUtil.h"
-
-/**
-*  The number of DCT kernel calls
-*/
-#define BENCHMARK_SIZE 10
-
-/**
-*  The PSNR values over this threshold indicate images equality
-*/
-#define PSNR_THRESHOLD_EQUAL 40
-
-// includes kernels
-#include "dct8x8_kernel1.cuh"
-#include "dct8x8_kernel2.cuh"
-#include "dct8x8_kernel_short.cuh"
-#include "dct8x8_kernel_quantization.cuh"
-
-/**
-**************************************************************************
-*  Wrapper function for 1st gold version of DCT, quantization and IDCT
-*implementations
-*
-* \param ImgSrc         [IN] - Source byte image plane
-* \param ImgDst         [IN] - Quantized result byte image plane
-* \param Stride         [IN] - Stride for both source and result planes
-* \param Size           [IN] - Size of both planes
-*
-* \return Execution time in milliseconds
-*/
-float WrapperGold1(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
-  // allocate float buffers for DCT and other data
-  int StrideF;
-  float *ImgF1 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
-  float *ImgF2 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
-
-  // convert source image to float representation
-  CopyByte2Float(ImgSrc, Stride, ImgF1, StrideF, Size);
-  AddFloatPlane(-128.0f, ImgF1, StrideF, Size);
-
-  // create and start CUDA timer
-  StopWatchInterface *timerGold = 0;
-  sdkCreateTimer(&timerGold);
-  sdkResetTimer(&timerGold);
-
-  // perform block-wise DCT processing and benchmarking
-  for (int i = 0; i < BENCHMARK_SIZE; i++) {
-    sdkStartTimer(&timerGold);
-    computeDCT8x8Gold1(ImgF1, ImgF2, StrideF, Size);
-    sdkStopTimer(&timerGold);
-  }
-
-  // stop and destroy CUDA timer
-  float TimerGoldSpan = sdkGetAverageTimerValue(&timerGold);
-  sdkDeleteTimer(&timerGold);
-
-  // perform quantization
-  quantizeGoldFloat(ImgF2, StrideF, Size);
-
-  // perform block-wise IDCT processing
-  computeIDCT8x8Gold1(ImgF2, ImgF1, StrideF, Size);
-
-  // convert image back to byte representation
-  AddFloatPlane(128.0f, ImgF1, StrideF, Size);
-  CopyFloat2Byte(ImgF1, StrideF, ImgDst, Stride, Size);
-
-  // free float buffers
-  FreePlane(ImgF1);
-  FreePlane(ImgF2);
-
-  // return time taken by the operation
-  return TimerGoldSpan;
-}
-
-/**
-**************************************************************************
-*  Wrapper function for 2nd gold version of DCT, quantization and IDCT
-*implementations
-*
-* \param ImgSrc         [IN] - Source byte image plane
-* \param ImgDst         [IN] - Quantized result byte image plane
-* \param Stride         [IN] - Stride for both source and result planes
-* \param Size           [IN] - Size of both planes
-*
-* \return Execution time in milliseconds
-*/
-float WrapperGold2(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
-  // allocate float buffers for DCT and other data
-  int StrideF;
-  float *ImgF1 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
-  float *ImgF2 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
-
-  // convert source image to float representation
-  CopyByte2Float(ImgSrc, Stride, ImgF1, StrideF, Size);
-  AddFloatPlane(-128.0f, ImgF1, StrideF, Size);
-
-  // create and start CUDA timer
-  StopWatchInterface *timerGold = 0;
-  sdkCreateTimer(&timerGold);
-  sdkResetTimer(&timerGold);
-
-  // perform block-wise DCT processing and benchmarking
-  for (int i = 0; i < BENCHMARK_SIZE; i++) {
-    sdkStartTimer(&timerGold);
-    computeDCT8x8Gold2(ImgF1, ImgF2, StrideF, Size);
-    sdkStopTimer(&timerGold);
-  }
-
-  // stop and destroy CUDA timer
-  float TimerGoldSpan = sdkGetAverageTimerValue(&timerGold);
-  sdkDeleteTimer(&timerGold);
-
-  // perform quantization
-  quantizeGoldFloat(ImgF2, StrideF, Size);
-
-  // perform block-wise IDCT processing
-  computeIDCT8x8Gold2(ImgF2, ImgF1, StrideF, Size);
-
-  // convert image back to byte representation
-  AddFloatPlane(128.0f, ImgF1, StrideF, Size);
-  CopyFloat2Byte(ImgF1, StrideF, ImgDst, Stride, Size);
-
-  // free float buffers
-  FreePlane(ImgF1);
-  FreePlane(ImgF2);
-
-  // return time taken by the operation
-  return TimerGoldSpan;
-}
-
-/**
-**************************************************************************
-*  Wrapper function for 1st CUDA version of DCT, quantization and IDCT
-*implementations
-*
-* \param ImgSrc         [IN] - Source byte image plane
-* \param ImgDst         [IN] - Quantized result byte image plane
-* \param Stride         [IN] - Stride for both source and result planes
-* \param Size           [IN] - Size of both planes
-*
-* \return Execution time in milliseconds
-*/
-float WrapperCUDA1(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
-  // prepare channel format descriptor for passing texture into kernels
-  hipChannelFormatDesc floattex = hipCreateChannelDesc<float>();
-
-  // allocate device memory
-  hipArray *Src;
-  float *Dst;
-  size_t DstStride;
-  HIPCHECK(hipMallocArray(&Src, &floattex, Size.width, Size.height));
-  HIPCHECK(hipMallocPitch((void **)(&Dst), &DstStride,
-                                  Size.width * sizeof(float), Size.height));
-  DstStride /= sizeof(float);
-
-  // convert source image to float representation
-  int ImgSrcFStride;
-  float *ImgSrcF = MallocPlaneFloat(Size.width, Size.height, &ImgSrcFStride);
-  CopyByte2Float(ImgSrc, Stride, ImgSrcF, ImgSrcFStride, Size);
-  AddFloatPlane(-128.0f, ImgSrcF, ImgSrcFStride, Size);
-
-  // copy from host memory to device
-  HIPCHECK(hipMemcpy2DToArray(
-      Src, 0, 0, ImgSrcF, ImgSrcFStride * sizeof(float),
-      Size.width * sizeof(float), Size.height, hipMemcpyHostToDevice));
-
-  // setup execution parameters
-  dim3 threads(BLOCK_SIZE, BLOCK_SIZE);
-  dim3 grid(Size.width / BLOCK_SIZE, Size.height / BLOCK_SIZE);
-
-  // create and start CUDA timer
-  StopWatchInterface *timerCUDA = 0;
-  sdkCreateTimer(&timerCUDA);
-  sdkResetTimer(&timerCUDA);
-
-  // execute DCT kernel and benchmark
-  hipTextureObject_t TexSrc;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = Src;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&TexSrc, &texRes, &texDescr, NULL));
-
-  for (int i = 0; i < BENCHMARK_SIZE; i++) {
-    sdkStartTimer(&timerCUDA);
-    CUDAkernel1DCT<<<grid, threads>>>(Dst, (int)DstStride, 0, 0, TexSrc);
-    HIPCHECK(hipDeviceSynchronize());
-    sdkStopTimer(&timerCUDA);
-  }
-
-  getLastCudaError("Kernel execution failed");
-
-  // finalize CUDA timer
-  float TimerCUDASpan = sdkGetAverageTimerValue(&timerCUDA);
-  sdkDeleteTimer(&timerCUDA);
-
-  // execute Quantization kernel
-  CUDAkernelQuantizationFloat<<<grid, threads>>>(Dst, (int)DstStride);
-  getLastCudaError("Kernel execution failed");
-
-  // copy quantized coefficients from host memory to device array
-  HIPCHECK(hipMemcpy2DToArray(Src, 0, 0, Dst, DstStride * sizeof(float),
-                                      Size.width * sizeof(float), Size.height,
-                                      hipMemcpyDeviceToDevice));
-
-  // execute IDCT kernel
-  CUDAkernel1IDCT<<<grid, threads>>>(Dst, (int)DstStride, 0, 0, TexSrc);
-  getLastCudaError("Kernel execution failed");
-
-  // copy quantized image block to host
-  HIPCHECK(hipMemcpy2D(
-      ImgSrcF, ImgSrcFStride * sizeof(float), Dst, DstStride * sizeof(float),
-      Size.width * sizeof(float), Size.height, hipMemcpyDeviceToHost));
-
-  // convert image back to byte representation
-  AddFloatPlane(128.0f, ImgSrcF, ImgSrcFStride, Size);
-  CopyFloat2Byte(ImgSrcF, ImgSrcFStride, ImgDst, Stride, Size);
-
-  // clean up memory
-  HIPCHECK(hipDestroyTextureObject(TexSrc));
-  HIPCHECK(hipFreeArray(Src));
-  HIPCHECK(hipFree(Dst));
-  FreePlane(ImgSrcF);
-
-  // return time taken by the operation
-  return TimerCUDASpan;
-}
-
-/**
-**************************************************************************
-*  Wrapper function for 2nd CUDA version of DCT, quantization and IDCT
-*implementations
-*
-* \param ImgSrc         [IN] - Source byte image plane
-* \param ImgDst         [IN] - Quantized result byte image plane
-* \param Stride         [IN] - Stride for both source and result planes
-* \param Size           [IN] - Size of both planes
-*
-* \return Execution time in milliseconds
-*/
-
-float WrapperCUDA2(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
-  // allocate host buffers for DCT and other data
-  int StrideF;
-  float *ImgF1 = MallocPlaneFloat(Size.width, Size.height, &StrideF);
-
-  // convert source image to float representation
-  CopyByte2Float(ImgSrc, Stride, ImgF1, StrideF, Size);
-  AddFloatPlane(-128.0f, ImgF1, StrideF, Size);
-
-  // allocate device memory
-  float *src, *dst;
-  size_t DeviceStride;
-  HIPCHECK(hipMallocPitch((void **)&src, &DeviceStride,
-                                  Size.width * sizeof(float), Size.height));
-  HIPCHECK(hipMallocPitch((void **)&dst, &DeviceStride,
-                                  Size.width * sizeof(float), Size.height));
-  DeviceStride /= sizeof(float);
-
-  // copy from host memory to device
-  HIPCHECK(hipMemcpy2D(
-      src, DeviceStride * sizeof(float), ImgF1, StrideF * sizeof(float),
-      Size.width * sizeof(float), Size.height, hipMemcpyHostToDevice));
-
-  // create and start CUDA timer
-  StopWatchInterface *timerCUDA = 0;
-  sdkCreateTimer(&timerCUDA);
-
-  // setup execution parameters
-  dim3 GridFullWarps(Size.width / KER2_BLOCK_WIDTH,
-                     Size.height / KER2_BLOCK_HEIGHT, 1);
-  dim3 ThreadsFullWarps(8, KER2_BLOCK_WIDTH / 8, KER2_BLOCK_HEIGHT / 8);
-
-  // perform block-wise DCT processing and benchmarking
-  const int numIterations = 100;
-
-  for (int i = -1; i < numIterations; i++) {
-    if (i == 0) {
-      HIPCHECK(hipDeviceSynchronize());
-      sdkResetTimer(&timerCUDA);
-      sdkStartTimer(&timerCUDA);
-    }
-
-    CUDAkernel2DCT<<<GridFullWarps, ThreadsFullWarps>>>(dst, src,
-                                                        (int)DeviceStride);
-    getLastCudaError("Kernel execution failed");
-  }
-
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&timerCUDA);
-
-  // finalize timing of CUDA Kernels
-  float avgTime = (float)sdkGetTimerValue(&timerCUDA) / (float)numIterations;
-  sdkDeleteTimer(&timerCUDA);
-  printf("%f MPix/s //%f ms\n",
-         (1E-6 * (float)Size.width * (float)Size.height) / (1E-3 * avgTime),
-         avgTime);
-
-  // setup execution parameters for quantization
-  dim3 ThreadsSmallBlocks(BLOCK_SIZE, BLOCK_SIZE);
-  dim3 GridSmallBlocks(Size.width / BLOCK_SIZE, Size.height / BLOCK_SIZE);
-
-  // execute Quantization kernel
-  CUDAkernelQuantizationFloat<<<GridSmallBlocks, ThreadsSmallBlocks>>>(
-      dst, (int)DeviceStride);
-  getLastCudaError("Kernel execution failed");
-
-  // perform block-wise IDCT processing
-  CUDAkernel2IDCT<<<GridFullWarps, ThreadsFullWarps>>>(src, dst,
-                                                       (int)DeviceStride);
-  HIPCHECK(hipDeviceSynchronize());
-  getLastCudaError("Kernel execution failed");
-
-  // copy quantized image block to host
-  HIPCHECK(hipMemcpy2D(
-      ImgF1, StrideF * sizeof(float), src, DeviceStride * sizeof(float),
-      Size.width * sizeof(float), Size.height, hipMemcpyDeviceToHost));
-
-  // convert image back to byte representation
-  AddFloatPlane(128.0f, ImgF1, StrideF, Size);
-  CopyFloat2Byte(ImgF1, StrideF, ImgDst, Stride, Size);
-
-  // clean up memory
-  HIPCHECK(hipFree(dst));
-  HIPCHECK(hipFree(src));
-  FreePlane(ImgF1);
-
-  // return time taken by the operation
-  return avgTime;
-}
-
-/**
-**************************************************************************
-*  Wrapper function for short CUDA version of DCT, quantization and IDCT
-*implementations
-*
-* \param ImgSrc         [IN] - Source byte image plane
-* \param ImgDst         [IN] - Quantized result byte image plane
-* \param Stride         [IN] - Stride for both source and result planes
-* \param Size           [IN] - Size of both planes
-*
-* \return Execution time in milliseconds
-*/
-float WrapperCUDAshort(byte *ImgSrc, byte *ImgDst, int Stride, ROI Size) {
-  // allocate host buffers for DCT and other data
-  int StrideS;
-  short *ImgS1 = MallocPlaneShort(Size.width, Size.height, &StrideS);
-
-  // convert source image to short representation centered at 128
-  for (int i = 0; i < Size.height; i++) {
-    for (int j = 0; j < Size.width; j++) {
-      ImgS1[i * StrideS + j] = (short)ImgSrc[i * Stride + j] - 128;
-    }
-  }
-
-  // allocate device memory
-  short *SrcDst;
-  size_t DeviceStride;
-  HIPCHECK(hipMallocPitch((void **)(&SrcDst), &DeviceStride,
-                                  Size.width * sizeof(short), Size.height));
-  DeviceStride /= sizeof(short);
-
-  // copy from host memory to device
-  HIPCHECK(hipMemcpy2D(
-      SrcDst, DeviceStride * sizeof(short), ImgS1, StrideS * sizeof(short),
-      Size.width * sizeof(short), Size.height, hipMemcpyHostToDevice));
-
-  // create and start CUDA timer
-  StopWatchInterface *timerLibJpeg = 0;
-  sdkCreateTimer(&timerLibJpeg);
-  sdkResetTimer(&timerLibJpeg);
-
-  // setup execution parameters
-  dim3 GridShort(Size.width / KERS_BLOCK_WIDTH, Size.height / KERS_BLOCK_HEIGHT,
-                 1);
-  dim3 ThreadsShort(8, KERS_BLOCK_WIDTH / 8, KERS_BLOCK_HEIGHT / 8);
-
-  // perform block-wise DCT processing and benchmarking
-  sdkStartTimer(&timerLibJpeg);
-  CUDAkernelShortDCT<<<GridShort, ThreadsShort>>>(SrcDst, (int)DeviceStride);
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&timerLibJpeg);
-  getLastCudaError("Kernel execution failed");
-
-  // stop and destroy CUDA timer
-  float TimerLibJpegSpan16b = sdkGetAverageTimerValue(&timerLibJpeg);
-  sdkDeleteTimer(&timerLibJpeg);
-
-  // setup execution parameters for quantization
-  dim3 ThreadsSmallBlocks(BLOCK_SIZE, BLOCK_SIZE);
-  dim3 GridSmallBlocks(Size.width / BLOCK_SIZE, Size.height / BLOCK_SIZE);
-
-  // execute Quantization kernel
-  CUDAkernelQuantizationShort<<<GridSmallBlocks, ThreadsSmallBlocks>>>(
-      SrcDst, (int)DeviceStride);
-  getLastCudaError("Kernel execution failed");
-
-  // perform block-wise IDCT processing
-  CUDAkernelShortIDCT<<<GridShort, ThreadsShort>>>(SrcDst, (int)DeviceStride);
-  HIPCHECK(hipDeviceSynchronize());
-  getLastCudaError("Kernel execution failed");
-
-  // copy quantized image block to host
-  HIPCHECK(hipMemcpy2D(
-      ImgS1, StrideS * sizeof(short), SrcDst, DeviceStride * sizeof(short),
-      Size.width * sizeof(short), Size.height, hipMemcpyDeviceToHost));
-
-  // convert image back to byte representation
-  for (int i = 0; i < Size.height; i++) {
-    for (int j = 0; j < Size.width; j++) {
-      ImgDst[i * Stride + j] = clamp_0_255(ImgS1[i * StrideS + j] + 128);
-    }
-  }
-
-  // free float buffers
-  HIPCHECK(hipFree(SrcDst));
-  FreePlane(ImgS1);
-
-  // return time taken by the operation
-  return TimerLibJpegSpan16b;
-}
-
-/**
-**************************************************************************
-*  Program entry point
-*
-* \param argc       [IN] - Number of command-line arguments
-* \param argv       [IN] - Array of command-line arguments
-*
-* \return Status code
-*/
-
-int main(int argc, char **argv) {
-  //
-  // Sample initialization
-  //
-  printf("%s Starting...\n\n", argv[0]);
-
-  // initialize CUDA
-  findCudaDevice(argc, (const char **)argv);
-
-  // source and results image filenames
-  char SampleImageFname[] = "teapot512.bmp";
-  char SampleImageFnameResGold1[] = "teapot512_gold1.bmp";
-  char SampleImageFnameResGold2[] = "teapot512_gold2.bmp";
-  char SampleImageFnameResCUDA1[] = "teapot512_cuda1.bmp";
-  char SampleImageFnameResCUDA2[] = "teapot512_cuda2.bmp";
-  char SampleImageFnameResCUDAshort[] = "teapot512_cuda_short.bmp";
-
-  char *pSampleImageFpath = sdkFindFilePath(SampleImageFname, argv[0]);
-
-  if (pSampleImageFpath == NULL) {
-    printf("dct8x8 could not locate Sample Image <%s>\nExiting...\n",
-           pSampleImageFpath);
-    exit(EXIT_FAILURE);
-  }
-
-  // preload image (acquire dimensions)
-  int ImgWidth, ImgHeight;
-  ROI ImgSize;
-  int res = PreLoadBmp(pSampleImageFpath, &ImgWidth, &ImgHeight);
-  ImgSize.width = ImgWidth;
-  ImgSize.height = ImgHeight;
-
-  // CONSOLE INFORMATION: saying hello to user
-  printf("CUDA sample DCT/IDCT implementation\n");
-  printf("===================================\n");
-  printf("Loading test image: %s... ", SampleImageFname);
-
-  if (res) {
-    printf("\nError: Image file not found or invalid!\n");
-    exit(EXIT_FAILURE);
-    return 1;
-  }
-
-  // check image dimensions are multiples of BLOCK_SIZE
-  if (ImgWidth % BLOCK_SIZE != 0 || ImgHeight % BLOCK_SIZE != 0) {
-    printf("\nError: Input image dimensions must be multiples of 8!\n");
-    exit(EXIT_FAILURE);
-    return 1;
-  }
-
-  printf("[%d x %d]... ", ImgWidth, ImgHeight);
-
-  // allocate image buffers
-  int ImgStride;
-  byte *ImgSrc = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
-  byte *ImgDstGold1 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
-  byte *ImgDstGold2 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
-  byte *ImgDstCUDA1 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
-  byte *ImgDstCUDA2 = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
-  byte *ImgDstCUDAshort = MallocPlaneByte(ImgWidth, ImgHeight, &ImgStride);
-
-  // load sample image
-  LoadBmpAsGray(pSampleImageFpath, ImgStride, ImgSize, ImgSrc);
-
-  //
-  // RUNNING WRAPPERS
-  //
-
-  // compute Gold 1 version of DCT/quantization/IDCT
-  printf("Success\nRunning Gold 1 (CPU) version... ");
-  float TimeGold1 = WrapperGold1(ImgSrc, ImgDstGold1, ImgStride, ImgSize);
-
-  // compute Gold 2 version of DCT/quantization/IDCT
-  printf("Success\nRunning Gold 2 (CPU) version... ");
-  float TimeGold2 = WrapperGold2(ImgSrc, ImgDstGold2, ImgStride, ImgSize);
-
-  // compute CUDA 1 version of DCT/quantization/IDCT
-  printf("Success\nRunning CUDA 1 (GPU) version... ");
-  float TimeCUDA1 = WrapperCUDA1(ImgSrc, ImgDstCUDA1, ImgStride, ImgSize);
-
-  // compute CUDA 2 version of DCT/quantization/IDCT
-  printf("Success\nRunning CUDA 2 (GPU) version... ");
-  float TimeCUDA2 = WrapperCUDA2(ImgSrc, ImgDstCUDA2, ImgStride, ImgSize);
-
-  // compute CUDA short version of DCT/quantization/IDCT
-  printf("Success\nRunning CUDA short (GPU) version... ");
-  float TimeCUDAshort =
-      WrapperCUDAshort(ImgSrc, ImgDstCUDAshort, ImgStride, ImgSize);
-  //
-  // Execution statistics, result saving and validation
-  //
-
-  // dump result of Gold 1 processing
-  printf("Success\nDumping result to %s... ", SampleImageFnameResGold1);
-  DumpBmpAsGray(SampleImageFnameResGold1, ImgDstGold1, ImgStride, ImgSize);
-
-  // dump result of Gold 2 processing
-  printf("Success\nDumping result to %s... ", SampleImageFnameResGold2);
-  DumpBmpAsGray(SampleImageFnameResGold2, ImgDstGold2, ImgStride, ImgSize);
-
-  // dump result of CUDA 1 processing
-  printf("Success\nDumping result to %s... ", SampleImageFnameResCUDA1);
-  DumpBmpAsGray(SampleImageFnameResCUDA1, ImgDstCUDA1, ImgStride, ImgSize);
-
-  // dump result of CUDA 2 processing
-  printf("Success\nDumping result to %s... ", SampleImageFnameResCUDA2);
-  DumpBmpAsGray(SampleImageFnameResCUDA2, ImgDstCUDA2, ImgStride, ImgSize);
-
-  // dump result of CUDA short processing
-  printf("Success\nDumping result to %s... ", SampleImageFnameResCUDAshort);
-  DumpBmpAsGray(SampleImageFnameResCUDAshort, ImgDstCUDAshort, ImgStride,
-                ImgSize);
-  // print speed info
-  printf("Success\n");
-
-  printf("Processing time (CUDA 1)    : %f ms \n", TimeCUDA1);
-  printf("Processing time (CUDA 2)    : %f ms \n", TimeCUDA2);
-  printf("Processing time (CUDA short): %f ms \n", TimeCUDAshort);
-
-  // calculate PSNR between each pair of images
-  float PSNR_Src_DstGold1 =
-      CalculatePSNR(ImgSrc, ImgDstGold1, ImgStride, ImgSize);
-  float PSNR_Src_DstGold2 =
-      CalculatePSNR(ImgSrc, ImgDstGold2, ImgStride, ImgSize);
-  float PSNR_Src_DstCUDA1 =
-      CalculatePSNR(ImgSrc, ImgDstCUDA1, ImgStride, ImgSize);
-  float PSNR_Src_DstCUDA2 =
-      CalculatePSNR(ImgSrc, ImgDstCUDA2, ImgStride, ImgSize);
-  float PSNR_Src_DstCUDAshort =
-      CalculatePSNR(ImgSrc, ImgDstCUDAshort, ImgStride, ImgSize);
-  float PSNR_DstGold1_DstCUDA1 =
-      CalculatePSNR(ImgDstGold1, ImgDstCUDA1, ImgStride, ImgSize);
-  float PSNR_DstGold2_DstCUDA2 =
-      CalculatePSNR(ImgDstGold2, ImgDstCUDA2, ImgStride, ImgSize);
-  float PSNR_DstGold2_DstCUDA16b =
-      CalculatePSNR(ImgDstGold2, ImgDstCUDAshort, ImgStride, ImgSize);
-
-  printf("PSNR Original    <---> CPU(Gold 1)    : %f\n", PSNR_Src_DstGold1);
-  printf("PSNR Original    <---> CPU(Gold 2)    : %f\n", PSNR_Src_DstGold2);
-  printf("PSNR Original    <---> GPU(CUDA 1)    : %f\n", PSNR_Src_DstCUDA1);
-  printf("PSNR Original    <---> GPU(CUDA 2)    : %f\n", PSNR_Src_DstCUDA2);
-  printf("PSNR Original    <---> GPU(CUDA short): %f\n", PSNR_Src_DstCUDAshort);
-  printf("PSNR CPU(Gold 1) <---> GPU(CUDA 1)    : %f\n",
-         PSNR_DstGold1_DstCUDA1);
-  printf("PSNR CPU(Gold 2) <---> GPU(CUDA 2)    : %f\n",
-         PSNR_DstGold2_DstCUDA2);
-  printf("PSNR CPU(Gold 2) <---> GPU(CUDA short): %f\n",
-         PSNR_DstGold2_DstCUDA16b);
-
-  bool bTestResult = (PSNR_DstGold1_DstCUDA1 > PSNR_THRESHOLD_EQUAL &&
-                      PSNR_DstGold2_DstCUDA2 > PSNR_THRESHOLD_EQUAL &&
-                      PSNR_DstGold2_DstCUDA16b > PSNR_THRESHOLD_EQUAL);
-
-  //
-  // Finalization
-  //
-
-  // release byte planes
-  FreePlane(ImgSrc);
-  FreePlane(ImgDstGold1);
-  FreePlane(ImgDstGold2);
-  FreePlane(ImgDstCUDA1);
-  FreePlane(ImgDstCUDA2);
-  FreePlane(ImgDstCUDAshort);
-
-  // finalize
-  printf("\nTest Summary...\n");
-
-  if (!bTestResult) {
-    printf("Test failed!\n");
-    exit(EXIT_FAILURE);
-  }
-
-  printf("Test passed\n");
-  exit(EXIT_SUCCESS);
-}
-finalize
-  printf("\nTest Summary...\n");
-
-  if (!bTestResult) {
-    printf("Test failed!\n");
-    exit(EXIT_FAILURE);
-  }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip
index e0ace59..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_large.cu.hip
@@ -1,381 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Computation of eigenvalues of a large symmetric, tridiagonal matrix */
-
-// includes, system
-
-#include <hip/hip_runtime.h>
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-#include <float.h>
-
-// includes, project
-#include "helper_functions.h"
-#include "helper_cuda.h"
-#include "config.h"
-#include "structs.h"
-#include "util.h"
-#include "matlab.h"
-
-#include "bisect_large.cuh"
-
-// includes, kernels
-#include "bisect_kernel_large.cuh"
-#include "bisect_kernel_large_onei.cuh"
-#include "bisect_kernel_large_multi.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-//! Initialize variables and memory for result
-//! @param  result handles to memory
-//! @param  matrix_size  size of the matrix
-////////////////////////////////////////////////////////////////////////////////
-void initResultDataLargeMatrix(ResultDataLarge &result,
-                               const unsigned int mat_size) {
-  // helper variables to initialize memory
-  unsigned int zero = 0;
-  unsigned int mat_size_f = sizeof(float) * mat_size;
-  unsigned int mat_size_ui = sizeof(unsigned int) * mat_size;
-
-  float *tempf = (float *)malloc(mat_size_f);
-  unsigned int *tempui = (unsigned int *)malloc(mat_size_ui);
-
-  for (unsigned int i = 0; i < mat_size; ++i) {
-    tempf[i] = 0.0f;
-    tempui[i] = 0;
-  }
-
-  // number of intervals containing only one eigenvalue after the first step
-  HIPCHECK(hipMalloc((void **)&result.g_num_one, sizeof(unsigned int)));
-  HIPCHECK(hipMemcpy(result.g_num_one, &zero, sizeof(unsigned int),
-                             hipMemcpyHostToDevice));
-
-  // number of (thread) blocks of intervals with multiple eigenvalues after
-  // the first iteration
-  HIPCHECK(
-      hipMalloc((void **)&result.g_num_blocks_mult, sizeof(unsigned int)));
-  HIPCHECK(hipMemcpy(result.g_num_blocks_mult, &zero,
-                             sizeof(unsigned int), hipMemcpyHostToDevice));
-
-  HIPCHECK(hipMalloc((void **)&result.g_left_one, mat_size_f));
-  HIPCHECK(hipMalloc((void **)&result.g_right_one, mat_size_f));
-  HIPCHECK(hipMalloc((void **)&result.g_pos_one, mat_size_ui));
-
-  HIPCHECK(hipMalloc((void **)&result.g_left_mult, mat_size_f));
-  HIPCHECK(hipMalloc((void **)&result.g_right_mult, mat_size_f));
-  HIPCHECK(hipMalloc((void **)&result.g_left_count_mult, mat_size_ui));
-  HIPCHECK(hipMalloc((void **)&result.g_right_count_mult, mat_size_ui));
-
-  HIPCHECK(
-      hipMemcpy(result.g_left_one, tempf, mat_size_f, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_right_one, tempf, mat_size_f,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_pos_one, tempui, mat_size_ui,
-                             hipMemcpyHostToDevice));
-
-  HIPCHECK(hipMemcpy(result.g_left_mult, tempf, mat_size_f,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_right_mult, tempf, mat_size_f,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_left_count_mult, tempui, mat_size_ui,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_right_count_mult, tempui, mat_size_ui,
-                             hipMemcpyHostToDevice));
-
-  HIPCHECK(hipMalloc((void **)&result.g_blocks_mult, mat_size_ui));
-  HIPCHECK(hipMemcpy(result.g_blocks_mult, tempui, mat_size_ui,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMalloc((void **)&result.g_blocks_mult_sum, mat_size_ui));
-  HIPCHECK(hipMemcpy(result.g_blocks_mult_sum, tempui, mat_size_ui,
-                             hipMemcpyHostToDevice));
-
-  HIPCHECK(hipMalloc((void **)&result.g_lambda_mult, mat_size_f));
-  HIPCHECK(hipMemcpy(result.g_lambda_mult, tempf, mat_size_f,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMalloc((void **)&result.g_pos_mult, mat_size_ui));
-  HIPCHECK(hipMemcpy(result.g_pos_mult, tempf, mat_size_ui,
-                             hipMemcpyHostToDevice));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Cleanup result memory
-//! @param result  handles to memory
-////////////////////////////////////////////////////////////////////////////////
-void cleanupResultDataLargeMatrix(ResultDataLarge &result) {
-  HIPCHECK(hipFree(result.g_num_one));
-  HIPCHECK(hipFree(result.g_num_blocks_mult));
-  HIPCHECK(hipFree(result.g_left_one));
-  HIPCHECK(hipFree(result.g_right_one));
-  HIPCHECK(hipFree(result.g_pos_one));
-  HIPCHECK(hipFree(result.g_left_mult));
-  HIPCHECK(hipFree(result.g_right_mult));
-  HIPCHECK(hipFree(result.g_left_count_mult));
-  HIPCHECK(hipFree(result.g_right_count_mult));
-  HIPCHECK(hipFree(result.g_blocks_mult));
-  HIPCHECK(hipFree(result.g_blocks_mult_sum));
-  HIPCHECK(hipFree(result.g_lambda_mult));
-  HIPCHECK(hipFree(result.g_pos_mult));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run the kernels to compute the eigenvalues for large matrices
-//! @param  input   handles to input data
-//! @param  result  handles to result data
-//! @param  mat_size  matrix size
-//! @param  precision  desired precision of eigenvalues
-//! @param  lg  lower limit of Gerschgorin interval
-//! @param  ug  upper limit of Gerschgorin interval
-//! @param  iterations  number of iterations (for timing)
-////////////////////////////////////////////////////////////////////////////////
-void computeEigenvaluesLargeMatrix(const InputData &input,
-                                   const ResultDataLarge &result,
-                                   const unsigned int mat_size,
-                                   const float precision, const float lg,
-                                   const float ug,
-                                   const unsigned int iterations) {
-  dim3 blocks(1, 1, 1);
-  dim3 threads(MAX_THREADS_BLOCK, 1, 1);
-
-  StopWatchInterface *timer_step1 = NULL;
-  StopWatchInterface *timer_step2_one = NULL;
-  StopWatchInterface *timer_step2_mult = NULL;
-  StopWatchInterface *timer_total = NULL;
-  sdkCreateTimer(&timer_step1);
-  sdkCreateTimer(&timer_step2_one);
-  sdkCreateTimer(&timer_step2_mult);
-  sdkCreateTimer(&timer_total);
-
-  sdkStartTimer(&timer_total);
-
-  // do for multiple iterations to improve timing accuracy
-  for (unsigned int iter = 0; iter < iterations; ++iter) {
-    sdkStartTimer(&timer_step1);
-    bisectKernelLarge<<<blocks, threads>>>(
-        input.g_a, input.g_b, mat_size, lg, ug, 0, mat_size, precision,
-        result.g_num_one, result.g_num_blocks_mult, result.g_left_one,
-        result.g_right_one, result.g_pos_one, result.g_left_mult,
-        result.g_right_mult, result.g_left_count_mult,
-        result.g_right_count_mult, result.g_blocks_mult,
-        result.g_blocks_mult_sum);
-
-    getLastCudaError("Kernel launch failed.");
-    HIPCHECK(hipDeviceSynchronize());
-    sdkStopTimer(&timer_step1);
-
-    // get the number of intervals containing one eigenvalue after the first
-    // processing step
-    unsigned int num_one_intervals;
-    HIPCHECK(hipMemcpy(&num_one_intervals, result.g_num_one,
-                               sizeof(unsigned int), hipMemcpyDeviceToHost));
-
-    dim3 grid_onei;
-    grid_onei.x = getNumBlocksLinear(num_one_intervals, MAX_THREADS_BLOCK);
-    dim3 threads_onei;
-    // use always max number of available threads to better balance load times
-    // for matrix data
-    threads_onei.x = MAX_THREADS_BLOCK;
-
-    // compute eigenvalues for intervals that contained only one eigenvalue
-    // after the first processing step
-    sdkStartTimer(&timer_step2_one);
-
-    bisectKernelLarge_OneIntervals<<<grid_onei, threads_onei>>>(
-        input.g_a, input.g_b, mat_size, num_one_intervals, result.g_left_one,
-        result.g_right_one, result.g_pos_one, precision);
-
-    getLastCudaError("bisectKernelLarge_OneIntervals() FAILED.");
-    HIPCHECK(hipDeviceSynchronize());
-    sdkStopTimer(&timer_step2_one);
-
-    // process intervals that contained more than one eigenvalue after
-    // the first processing step
-
-    // get the number of blocks of intervals that contain, in total when
-    // each interval contains only one eigenvalue, not more than
-    // MAX_THREADS_BLOCK threads
-    unsigned int num_blocks_mult = 0;
-    HIPCHECK(hipMemcpy(&num_blocks_mult, result.g_num_blocks_mult,
-                               sizeof(unsigned int), hipMemcpyDeviceToHost));
-
-    // setup the execution environment
-    dim3 grid_mult(num_blocks_mult, 1, 1);
-    dim3 threads_mult(MAX_THREADS_BLOCK, 1, 1);
-
-    sdkStartTimer(&timer_step2_mult);
-
-    bisectKernelLarge_MultIntervals<<<grid_mult, threads_mult>>>(
-        input.g_a, input.g_b, mat_size, result.g_blocks_mult,
-        result.g_blocks_mult_sum, result.g_left_mult, result.g_right_mult,
-        result.g_left_count_mult, result.g_right_count_mult,
-        result.g_lambda_mult, result.g_pos_mult, precision);
-
-    getLastCudaError("bisectKernelLarge_MultIntervals() FAILED.");
-    HIPCHECK(hipDeviceSynchronize());
-    sdkStopTimer(&timer_step2_mult);
-  }
-
-  sdkStopTimer(&timer_total);
-
-  printf("Average time step 1: %f ms\n",
-         sdkGetTimerValue(&timer_step1) / (float)iterations);
-  printf("Average time step 2, one intervals: %f ms\n",
-         sdkGetTimerValue(&timer_step2_one) / (float)iterations);
-  printf("Average time step 2, mult intervals: %f ms\n",
-         sdkGetTimerValue(&timer_step2_mult) / (float)iterations);
-
-  printf("Average time TOTAL: %f ms\n",
-         sdkGetTimerValue(&timer_total) / (float)iterations);
-
-  sdkDeleteTimer(&timer_step1);
-  sdkDeleteTimer(&timer_step2_one);
-  sdkDeleteTimer(&timer_step2_mult);
-  sdkDeleteTimer(&timer_total);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Process the result, that is obtain result from device and do simple sanity
-//! checking
-//! @param  input   handles to input data
-//! @param  result  handles to result data
-//! @param  mat_size  matrix size
-//! @param  filename  output filename
-////////////////////////////////////////////////////////////////////////////////
-bool processResultDataLargeMatrix(const InputData &input,
-                                  const ResultDataLarge &result,
-                                  const unsigned int mat_size,
-                                  const char *filename,
-                                  const unsigned int user_defined,
-                                  char *exec_path) {
-  bool bCompareResult = false;
-  const unsigned int mat_size_ui = sizeof(unsigned int) * mat_size;
-  const unsigned int mat_size_f = sizeof(float) * mat_size;
-
-  // copy data from intervals that contained more than one eigenvalue after
-  // the first processing step
-  float *lambda_mult = (float *)malloc(sizeof(float) * mat_size);
-  HIPCHECK(hipMemcpy(lambda_mult, result.g_lambda_mult,
-                             sizeof(float) * mat_size, hipMemcpyDeviceToHost));
-  unsigned int *pos_mult =
-      (unsigned int *)malloc(sizeof(unsigned int) * mat_size);
-  HIPCHECK(hipMemcpy(pos_mult, result.g_pos_mult,
-                             sizeof(unsigned int) * mat_size,
-                             hipMemcpyDeviceToHost));
-
-  unsigned int *blocks_mult_sum =
-      (unsigned int *)malloc(sizeof(unsigned int) * mat_size);
-  HIPCHECK(hipMemcpy(blocks_mult_sum, result.g_blocks_mult_sum,
-                             sizeof(unsigned int) * mat_size,
-                             hipMemcpyDeviceToHost));
-
-  unsigned int num_one_intervals;
-  HIPCHECK(hipMemcpy(&num_one_intervals, result.g_num_one,
-                             sizeof(unsigned int), hipMemcpyDeviceToHost));
-
-  unsigned int sum_blocks_mult = mat_size - num_one_intervals;
-
-  // copy data for intervals that contained one eigenvalue after the first
-  // processing step
-  float *left_one = (float *)malloc(mat_size_f);
-  float *right_one = (float *)malloc(mat_size_f);
-  unsigned int *pos_one = (unsigned int *)malloc(mat_size_ui);
-  HIPCHECK(hipMemcpy(left_one, result.g_left_one, mat_size_f,
-                             hipMemcpyDeviceToHost));
-  HIPCHECK(hipMemcpy(right_one, result.g_right_one, mat_size_f,
-                             hipMemcpyDeviceToHost));
-  HIPCHECK(hipMemcpy(pos_one, result.g_pos_one, mat_size_ui,
-                             hipMemcpyDeviceToHost));
-
-  // extract eigenvalues
-  float *eigenvals = (float *)malloc(mat_size_f);
-
-  // singleton intervals generated in the second step
-  for (unsigned int i = 0; i < sum_blocks_mult; ++i) {
-    eigenvals[pos_mult[i] - 1] = lambda_mult[i];
-  }
-
-  // singleton intervals generated in the first step
-  unsigned int index = 0;
-
-  for (unsigned int i = 0; i < num_one_intervals; ++i, ++index) {
-    eigenvals[pos_one[i] - 1] = left_one[i];
-  }
-
-  if (1 == user_defined) {
-    // store result
-    writeTridiagSymMatlab(filename, input.a, input.b + 1, eigenvals, mat_size);
-    // getLastCudaError( sdkWriteFilef( filename, eigenvals, mat_size, 0.0f));
-
-    printf("User requests non-default argument(s), skipping self-check!\n");
-    bCompareResult = true;
-  } else {
-    // compare with reference solution
-
-    float *reference = NULL;
-    unsigned int input_data_size = 0;
-
-    char *ref_path = sdkFindFilePath("reference.dat", exec_path);
-    assert(NULL != ref_path);
-    sdkReadFile(ref_path, &reference, &input_data_size, false);
-    assert(input_data_size == mat_size);
-
-    // there's an imprecision of Sturm count computation which makes an
-    // additional offset necessary
-    float tolerance = 1.0e-5f + 5.0e-6f;
-
-    if (sdkCompareL2fe(reference, eigenvals, mat_size, tolerance) == true) {
-      bCompareResult = true;
-    } else {
-      bCompareResult = false;
-    }
-
-    free(ref_path);
-    free(reference);
-  }
-
-  freePtr(eigenvals);
-  freePtr(lambda_mult);
-  freePtr(pos_mult);
-  freePtr(blocks_mult_sum);
-  freePtr(left_one);
-  freePtr(right_one);
-  freePtr(pos_one);
-
-  return bCompareResult;
-}
-e, eigenvals, mat_size, tolerance) == true) {
-      bCompareResult = true;
-    } else {
-      bCompareResult = false;
-    }
-
-    free(ref_path);
-    free(reference);
-  }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip
index 1ba4622..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/bisect_small.cu.hip
@@ -1,186 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Computation of eigenvalues of a small symmetric, tridiagonal matrix */
-
-// includes, system
-
-#include <hip/hip_runtime.h>
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-#include <float.h>
-
-// includes, project
-#include "helper_functions.h"
-#include "helper_cuda.h"
-#include "config.h"
-#include "structs.h"
-#include "matlab.h"
-
-// includes, kernels
-#include "bisect_kernel_small.cuh"
-
-// includes, file
-#include "bisect_small.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-//! Determine eigenvalues for matrices smaller than MAX_SMALL_MATRIX
-//! @param TimingIterations  number of iterations for timing
-//! @param  input  handles to input data of kernel
-//! @param  result handles to result of kernel
-//! @param  mat_size  matrix size
-//! @param  lg  lower limit of Gerschgorin interval
-//! @param  ug  upper limit of Gerschgorin interval
-//! @param  precision  desired precision of eigenvalues
-//! @param  iterations  number of iterations for timing
-////////////////////////////////////////////////////////////////////////////////
-void computeEigenvaluesSmallMatrix(const InputData &input,
-                                   ResultDataSmall &result,
-                                   const unsigned int mat_size, const float lg,
-                                   const float ug, const float precision,
-                                   const unsigned int iterations) {
-  StopWatchInterface *timer = NULL;
-  sdkCreateTimer(&timer);
-  sdkStartTimer(&timer);
-
-  for (unsigned int i = 0; i < iterations; ++i) {
-    dim3 blocks(1, 1, 1);
-    dim3 threads(MAX_THREADS_BLOCK_SMALL_MATRIX, 1, 1);
-
-    bisectKernel<<<blocks, threads>>>(input.g_a, input.g_b, mat_size,
-                                      result.g_left, result.g_right,
-                                      result.g_left_count, result.g_right_count,
-                                      lg, ug, 0, mat_size, precision);
-  }
-
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&timer);
-  getLastCudaError("Kernel launch failed");
-  printf("Average time: %f ms (%i iterations)\n",
-         sdkGetTimerValue(&timer) / (float)iterations, iterations);
-
-  sdkDeleteTimer(&timer);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Initialize variables and memory for the result for small matrices
-//! @param result  handles to the necessary memory
-//! @param  mat_size  matrix_size
-////////////////////////////////////////////////////////////////////////////////
-void initResultSmallMatrix(ResultDataSmall &result,
-                           const unsigned int mat_size) {
-  result.mat_size_f = sizeof(float) * mat_size;
-  result.mat_size_ui = sizeof(unsigned int) * mat_size;
-
-  result.eigenvalues = (float *)malloc(result.mat_size_f);
-
-  // helper variables
-  result.zero_f = (float *)malloc(result.mat_size_f);
-  result.zero_ui = (unsigned int *)malloc(result.mat_size_ui);
-
-  for (unsigned int i = 0; i < mat_size; ++i) {
-    result.zero_f[i] = 0.0f;
-    result.zero_ui[i] = 0;
-
-    result.eigenvalues[i] = 0.0f;
-  }
-
-  HIPCHECK(hipMalloc((void **)&result.g_left, result.mat_size_f));
-  HIPCHECK(hipMalloc((void **)&result.g_right, result.mat_size_f));
-
-  HIPCHECK(
-      hipMalloc((void **)&result.g_left_count, result.mat_size_ui));
-  HIPCHECK(
-      hipMalloc((void **)&result.g_right_count, result.mat_size_ui));
-
-  // initialize result memory
-  HIPCHECK(hipMemcpy(result.g_left, result.zero_f, result.mat_size_f,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_right, result.zero_f, result.mat_size_f,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_right_count, result.zero_ui,
-                             result.mat_size_ui, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(result.g_left_count, result.zero_ui,
-                             result.mat_size_ui, hipMemcpyHostToDevice));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Cleanup memory and variables for result for small matrices
-//! @param  result  handle to variables
-////////////////////////////////////////////////////////////////////////////////
-void cleanupResultSmallMatrix(ResultDataSmall &result) {
-  freePtr(result.eigenvalues);
-  freePtr(result.zero_f);
-  freePtr(result.zero_ui);
-
-  HIPCHECK(hipFree(result.g_left));
-  HIPCHECK(hipFree(result.g_right));
-  HIPCHECK(hipFree(result.g_left_count));
-  HIPCHECK(hipFree(result.g_right_count));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Process the result obtained on the device, that is transfer to host and
-//! perform basic sanity checking
-//! @param  input  handles to input data
-//! @param  result  handles to result data
-//! @param  mat_size   matrix size
-//! @param  filename  output filename
-////////////////////////////////////////////////////////////////////////////////
-void processResultSmallMatrix(const InputData &input,
-                              const ResultDataSmall &result,
-                              const unsigned int mat_size,
-                              const char *filename) {
-  const unsigned int mat_size_f = sizeof(float) * mat_size;
-  const unsigned int mat_size_ui = sizeof(unsigned int) * mat_size;
-
-  // copy data back to host
-  float *left = (float *)malloc(mat_size_f);
-  unsigned int *left_count = (unsigned int *)malloc(mat_size_ui);
-
-  HIPCHECK(
-      hipMemcpy(left, result.g_left, mat_size_f, hipMemcpyDeviceToHost));
-  HIPCHECK(hipMemcpy(left_count, result.g_left_count, mat_size_ui,
-                             hipMemcpyDeviceToHost));
-
-  float *eigenvalues = (float *)malloc(mat_size_f);
-
-  for (unsigned int i = 0; i < mat_size; ++i) {
-    eigenvalues[left_count[i]] = left[i];
-  }
-
-  // save result in matlab format
-  writeTridiagSymMatlab(filename, input.a, input.b + 1, eigenvalues, mat_size);
-
-  freePtr(left);
-  freePtr(left_count);
-  freePtr(eigenvalues);
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/main.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/main.cu.hip
index b1334c2..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/main.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/eigenvalues/main.cu.hip
@@ -1,329 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Computation of eigenvalues of symmetric, tridiagonal matrix using
- * bisection.
- */
-
-// includes, system
-
-#include <hip/hip_runtime.h>
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-#include <float.h>
-#include <assert.h>
-
-// includes, project
-#include <helper_functions.h>
-#include <helper_cuda.h>
-#include "config.h"
-#include "structs.h"
-#include "matlab.h"
-#include "util.h"
-#include "gerschgorin.h"
-
-#include "bisect_small.cuh"
-#include "bisect_large.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-bool runTest(int argc, char **argv);
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  bool bQAResults = false;
-
-  printf("Starting eigenvalues\n");
-
-  bQAResults = runTest(argc, argv);
-  printf("Test %s\n", bQAResults ? "Succeeded!" : "Failed!");
-
-  exit(bQAResults ? EXIT_SUCCESS : EXIT_FAILURE);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Initialize the input data to the algorithm
-//! @param input  handles to the input data
-//! @param exec_path  path where executable is run (argv[0])
-//! @param mat_size  size of the matrix
-//! @param user_defined  1 if the matrix size has been requested by the user,
-//!                      0 if the default size
-////////////////////////////////////////////////////////////////////////////////
-void initInputData(InputData &input, char *exec_path,
-                   const unsigned int mat_size,
-                   const unsigned int user_defined) {
-  // allocate memory
-  input.a = (float *)malloc(sizeof(float) * mat_size);
-  input.b = (float *)malloc(sizeof(float) * mat_size);
-
-  if (1 == user_defined) {
-    // initialize diagonal and superdiagonal entries with random values
-    srand(278217421);
-
-    // srand( clock());
-    for (unsigned int i = 0; i < mat_size; ++i) {
-      input.a[i] = (float)(2.0 * (((double)rand() / (double)RAND_MAX) - 0.5));
-      input.b[i] = (float)(2.0 * (((double)rand() / (double)RAND_MAX) - 0.5));
-    }
-
-    // the first element of s is used as padding on the device (thus the
-    // whole vector is copied to the device but the kernels are launched
-    // with (s+1) as start address
-    input.b[0] = 0.0f;
-  } else {
-    // read default matrix
-    unsigned int input_data_size = mat_size;
-    char *diag_path = sdkFindFilePath("diagonal.dat", exec_path);
-    assert(NULL != diag_path);
-    sdkReadFile(diag_path, &(input.a), &input_data_size, false);
-
-    char *sdiag_path = sdkFindFilePath("superdiagonal.dat", exec_path);
-    assert(NULL != sdiag_path);
-    sdkReadFile(sdiag_path, &(input.b), &input_data_size, false);
-
-    free(diag_path);
-    free(sdiag_path);
-  }
-
-  // allocate device memory for input
-  HIPCHECK(hipMalloc((void **)&(input.g_a), sizeof(float) * mat_size));
-  HIPCHECK(
-      hipMalloc((void **)&(input.g_b_raw), sizeof(float) * mat_size));
-
-  // copy data to device
-  HIPCHECK(hipMemcpy(input.g_a, input.a, sizeof(float) * mat_size,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(input.g_b_raw, input.b, sizeof(float) * mat_size,
-                             hipMemcpyHostToDevice));
-
-  input.g_b = input.g_b_raw + 1;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Clean up input data, in particular allocated memory
-//! @param input  handles to the input data
-////////////////////////////////////////////////////////////////////////////////
-void cleanupInputData(InputData &input) {
-  freePtr(input.a);
-  freePtr(input.b);
-
-  HIPCHECK(hipFree(input.g_a));
-  input.g_a = NULL;
-  HIPCHECK(hipFree(input.g_b_raw));
-  input.g_b_raw = NULL;
-  input.g_b = NULL;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Check if a specific matrix size has to be used
-//! @param argc  number of command line arguments (from main(argc, argv)
-//! @param argv  pointers to command line arguments (from main(argc, argv)
-//! @param matrix_size  size of matrix, updated if specific size specified on
-//!                     command line
-////////////////////////////////////////////////////////////////////////////////
-void getMatrixSize(int argc, char **argv, unsigned int &mat_size,
-                   unsigned int &user_defined) {
-  int temp = -1;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "matrix-size")) {
-    temp = getCmdLineArgumentInt(argc, (const char **)argv, "matrix-size");
-  }
-
-  if (temp > 0) {
-    mat_size = (unsigned int)temp;
-    // data type short is used in the kernel
-    assert(mat_size < (1 << 16));
-
-    // mat_size should be large than 2
-    assert(mat_size >= 2);
-
-    user_defined = 1;
-  }
-
-  printf("Matrix size: %i x %i\n", mat_size, mat_size);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Check if a specific precision of the eigenvalue has to be obtained
-//! @param argc  number of command line arguments (from main(argc, argv)
-//! @param argv  pointers to command line arguments (from main(argc, argv)
-//! @param iters_timing  numbers of iterations for timing, updated if a
-//!                      specific number is specified on the command line
-//! @param user_defined  1 if the precision has been requested by the user,
-//!                      0 if the default size
-////////////////////////////////////////////////////////////////////////////////
-void getPrecision(int argc, char **argv, float &precision,
-                  unsigned int &user_defined) {
-  float temp = -1.0f;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "precision")) {
-    temp = getCmdLineArgumentFloat(argc, (const char **)argv, "precision");
-    printf("Precision is between [0.001, 0.000001]\n");
-  }
-
-  if (temp > 1e-6 && temp <= 0.001) {
-    precision = temp;
-    user_defined = 1;
-  }
-
-  printf("Precision: %f\n", precision);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Check if a particular number of iterations for timings has to be used
-//! @param argc  number of command line arguments (from main(argc, argv)
-//! @param argv  pointers to command line arguments (from main(argc, argv)
-//! @param  iters_timing  number of timing iterations, updated if user
-//!                       specific value
-////////////////////////////////////////////////////////////////////////////////
-void getItersTiming(int argc, char **argv, unsigned int &iters_timing) {
-  int temp = -1;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "iters-timing")) {
-    temp = getCmdLineArgumentInt(argc, (const char **)argv, "iters-timing");
-  }
-
-  if (temp > 0) {
-    iters_timing = temp;
-  }
-
-  printf("Iterations to be timed: %i\n", iters_timing);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Check if a particular filename has to be used for the file where the result
-//! is stored
-//! @param argc  number of command line arguments (from main(argc, argv)
-//! @param argv  pointers to command line arguments (from main(argc, argv)
-//! @param  filename  filename of result file, updated if user specified
-//!                   filename
-////////////////////////////////////////////////////////////////////////////////
-void getResultFilename(int argc, char **argv, char *&filename) {
-  char *temp = NULL;
-  getCmdLineArgumentString(argc, (const char **)argv, "filename-result", &temp);
-
-  if (NULL != temp) {
-    filename = (char *)malloc(sizeof(char) * strlen(temp));
-    strcpy(filename, temp);
-
-    free(temp);
-  }
-
-  printf("Result filename: '%s'\n", filename);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run a simple test for CUDA
-////////////////////////////////////////////////////////////////////////////////
-bool runTest(int argc, char **argv) {
-  bool bCompareResult = false;
-
-  findCudaDevice(argc, (const char **)argv);
-
-  StopWatchInterface *timer = NULL;
-  StopWatchInterface *timer_total = NULL;
-  sdkCreateTimer(&timer);
-  sdkCreateTimer(&timer_total);
-
-  // default
-  unsigned int mat_size = 2048;
-  // flag if the matrix size is due to explicit user request
-  unsigned int user_defined = 0;
-  // desired precision of eigenvalues
-  float precision = 0.00001f;
-  unsigned int iters_timing = 100;
-  char *result_file = (char *)"eigenvalues.dat";
-
-  // check if there is a command line request for the matrix size
-  getMatrixSize(argc, argv, mat_size, user_defined);
-
-  // check if user requested specific precision
-  getPrecision(argc, argv, precision, user_defined);
-
-  // check if user requested specific number of iterations for timing
-  getItersTiming(argc, argv, iters_timing);
-
-  // file name for result file
-  getResultFilename(argc, argv, result_file);
-
-  // set up input
-  InputData input;
-  initInputData(input, argv[0], mat_size, user_defined);
-
-  // compute Gerschgorin interval
-  float lg = FLT_MAX;
-  float ug = -FLT_MAX;
-  computeGerschgorin(input.a, input.b + 1, mat_size, lg, ug);
-  printf("Gerschgorin interval: %f / %f\n", lg, ug);
-
-  // two kernels, for small matrices a lot of overhead can be avoided
-  if (mat_size <= MAX_SMALL_MATRIX) {
-    // initialize memory for result
-    ResultDataSmall result;
-    initResultSmallMatrix(result, mat_size);
-
-    // run the kernel
-    computeEigenvaluesSmallMatrix(input, result, mat_size, lg, ug, precision,
-                                  iters_timing);
-
-    // get the result from the device and do some sanity checks,
-    // save the result
-    processResultSmallMatrix(input, result, mat_size, result_file);
-
-    // clean up
-    cleanupResultSmallMatrix(result);
-
-    printf("User requests non-default argument(s), skipping self-check!\n");
-    bCompareResult = true;
-  } else {
-    // initialize memory for result
-    ResultDataLarge result;
-    initResultDataLargeMatrix(result, mat_size);
-
-    // run the kernel
-    computeEigenvaluesLargeMatrix(input, result, mat_size, precision, lg, ug,
-                                  iters_timing);
-
-    // get the result from the device and do some sanity checks
-    // save the result if user specified matrix size
-    bCompareResult = processResultDataLargeMatrix(
-        input, result, mat_size, result_file, user_defined, argv[0]);
-
-    // cleanup
-    cleanupResultDataLargeMatrix(result);
-  }
-
-  cleanupInputData(input);
-
-  return bCompareResult;
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram256.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram256.cu.hip
index c3a4926..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram256.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram256.cu.hip
@@ -1,167 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <helper_cuda.h>
-#include "histogram_common.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// Shortcut shared memory atomic addition functions
-////////////////////////////////////////////////////////////////////////////////
-
-#define TAG_MASK 0xFFFFFFFFU
-inline __device__ void addByte(uint *s_WarpHist, uint data, uint threadTag) {
-  atomicAdd(s_WarpHist + data, 1);
-}
-
-inline __device__ void addWord(uint *s_WarpHist, uint data, uint tag) {
-  addByte(s_WarpHist, (data >> 0) & 0xFFU, tag);
-  addByte(s_WarpHist, (data >> 8) & 0xFFU, tag);
-  addByte(s_WarpHist, (data >> 16) & 0xFFU, tag);
-  addByte(s_WarpHist, (data >> 24) & 0xFFU, tag);
-}
-
-__global__ void histogram256Kernel(uint *d_PartialHistograms, uint *d_Data,
-                                   uint dataCount) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Per-warp subhistogram storage
-  __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];
-  uint *s_WarpHist =
-      s_Hist + (threadIdx.x >> LOG2_WARP_SIZE) * HISTOGRAM256_BIN_COUNT;
-
-// Clear shared memory storage for current threadblock before processing
-#pragma unroll
-
-  for (uint i = 0;
-       i < (HISTOGRAM256_THREADBLOCK_MEMORY / HISTOGRAM256_THREADBLOCK_SIZE);
-       i++) {
-    s_Hist[threadIdx.x + i * HISTOGRAM256_THREADBLOCK_SIZE] = 0;
-  }
-
-  // Cycle through the entire data set, update subhistograms for each warp
-  const uint tag = threadIdx.x << (UINT_BITS - LOG2_WARP_SIZE);
-
-  cg::sync(cta);
-
-  for (uint pos = UMAD(blockIdx.x, blockDim.x, threadIdx.x); pos < dataCount;
-       pos += UMUL(blockDim.x, gridDim.x)) {
-    uint data = d_Data[pos];
-    addWord(s_WarpHist, data, tag);
-  }
-
-  // Merge per-warp histograms into per-block and write to global memory
-  cg::sync(cta);
-
-  for (uint bin = threadIdx.x; bin < HISTOGRAM256_BIN_COUNT;
-       bin += HISTOGRAM256_THREADBLOCK_SIZE) {
-    uint sum = 0;
-
-    for (uint i = 0; i < WARP_COUNT; i++) {
-      sum += s_Hist[bin + i * HISTOGRAM256_BIN_COUNT] & TAG_MASK;
-    }
-
-    d_PartialHistograms[blockIdx.x * HISTOGRAM256_BIN_COUNT + bin] = sum;
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Merge histogram256() output
-// Run one threadblock per bin; each threadblock adds up the same bin counter
-// from every partial histogram. Reads are uncoalesced, but mergeHistogram256
-// takes only a fraction of total processing time
-////////////////////////////////////////////////////////////////////////////////
-#define MERGE_THREADBLOCK_SIZE 256
-
-__global__ void mergeHistogram256Kernel(uint *d_Histogram,
-                                        uint *d_PartialHistograms,
-                                        uint histogramCount) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-
-  uint sum = 0;
-
-  for (uint i = threadIdx.x; i < histogramCount; i += MERGE_THREADBLOCK_SIZE) {
-    sum += d_PartialHistograms[blockIdx.x + i * HISTOGRAM256_BIN_COUNT];
-  }
-
-  __shared__ uint data[MERGE_THREADBLOCK_SIZE];
-  data[threadIdx.x] = sum;
-
-  for (uint stride = MERGE_THREADBLOCK_SIZE / 2; stride > 0; stride >>= 1) {
-    cg::sync(cta);
-
-    if (threadIdx.x < stride) {
-      data[threadIdx.x] += data[threadIdx.x + stride];
-    }
-  }
-
-  if (threadIdx.x == 0) {
-    d_Histogram[blockIdx.x] = data[0];
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Host interface to GPU histogram
-////////////////////////////////////////////////////////////////////////////////
-// histogram256kernel() intermediate results buffer
-static const uint PARTIAL_HISTOGRAM256_COUNT = 240;
-static uint *d_PartialHistograms;
-
-// Internal memory allocation
-extern "C" void initHistogram256(void) {
-  HIPCHECK(hipMalloc(
-      (void **)&d_PartialHistograms,
-      PARTIAL_HISTOGRAM256_COUNT * HISTOGRAM256_BIN_COUNT * sizeof(uint)));
-}
-
-// Internal memory deallocation
-extern "C" void closeHistogram256(void) {
-  HIPCHECK(hipFree(d_PartialHistograms));
-}
-
-extern "C" void histogram256(uint *d_Histogram, void *d_Data, uint byteCount) {
-  assert(byteCount % sizeof(uint) == 0);
-  histogram256Kernel<<<PARTIAL_HISTOGRAM256_COUNT,
-                       HISTOGRAM256_THREADBLOCK_SIZE>>>(
-      d_PartialHistograms, (uint *)d_Data, byteCount / sizeof(uint));
-  getLastCudaError("histogram256Kernel() execution failed\n");
-
-  mergeHistogram256Kernel<<<HISTOGRAM256_BIN_COUNT, MERGE_THREADBLOCK_SIZE>>>(
-      d_Histogram, d_PartialHistograms, PARTIAL_HISTOGRAM256_COUNT);
-  getLastCudaError("mergeHistogram256Kernel() execution failed\n");
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram64.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram64.cu.hip
index a3db8b6..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram64.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/histogram/histogram64.cu.hip
@@ -1,209 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <helper_cuda.h>
-#include "histogram_common.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// GPU-specific common definitions
-////////////////////////////////////////////////////////////////////////////////
-// Data type used for input data fetches
-typedef uint4 data_t;
-
-// May change on future hardware, so better parametrize the code
-#define SHARED_MEMORY_BANKS 16
-
-////////////////////////////////////////////////////////////////////////////////
-// Main computation pass: compute gridDim.x partial histograms
-////////////////////////////////////////////////////////////////////////////////
-// Count a byte into shared-memory storage
-inline __device__ void addByte(uchar *s_ThreadBase, uint data) {
-  s_ThreadBase[UMUL(data, HISTOGRAM64_THREADBLOCK_SIZE)]++;
-}
-
-// Count four bytes of a word
-inline __device__ void addWord(uchar *s_ThreadBase, uint data) {
-  // Only higher 6 bits of each byte matter, as this is a 64-bin histogram
-  addByte(s_ThreadBase, (data >> 2) & 0x3FU);
-  addByte(s_ThreadBase, (data >> 10) & 0x3FU);
-  addByte(s_ThreadBase, (data >> 18) & 0x3FU);
-  addByte(s_ThreadBase, (data >> 26) & 0x3FU);
-}
-
-__global__ void histogram64Kernel(uint *d_PartialHistograms, data_t *d_Data,
-                                  uint dataCount) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Encode thread index in order to avoid bank conflicts in s_Hist[] access:
-  // each group of SHARED_MEMORY_BANKS threads accesses consecutive shared
-  // memory banks
-  // and the same bytes [0..3] within the banks
-  // Because of this permutation block size should be a multiple of 4 *
-  // SHARED_MEMORY_BANKS
-  const uint threadPos = ((threadIdx.x & ~(SHARED_MEMORY_BANKS * 4 - 1)) << 0) |
-                         ((threadIdx.x & (SHARED_MEMORY_BANKS - 1)) << 2) |
-                         ((threadIdx.x & (SHARED_MEMORY_BANKS * 3)) >> 4);
-
-  // Per-thread histogram storage
-  __shared__ uchar s_Hist[HISTOGRAM64_THREADBLOCK_SIZE * HISTOGRAM64_BIN_COUNT];
-  uchar *s_ThreadBase = s_Hist + threadPos;
-
-// Initialize shared memory (writing 32-bit words)
-#pragma unroll
-
-  for (uint i = 0; i < (HISTOGRAM64_BIN_COUNT / 4); i++) {
-    ((uint *)s_Hist)[threadIdx.x + i * HISTOGRAM64_THREADBLOCK_SIZE] = 0;
-  }
-
-  // Read data from global memory and submit to the shared-memory histogram
-  // Since histogram counters are byte-sized, every single thread can't do more
-  // than 255 submission
-  cg::sync(cta);
-
-  for (uint pos = UMAD(blockIdx.x, blockDim.x, threadIdx.x); pos < dataCount;
-       pos += UMUL(blockDim.x, gridDim.x)) {
-    data_t data = d_Data[pos];
-    addWord(s_ThreadBase, data.x);
-    addWord(s_ThreadBase, data.y);
-    addWord(s_ThreadBase, data.z);
-    addWord(s_ThreadBase, data.w);
-  }
-
-  // Accumulate per-thread histograms into per-block and write to global memory
-  cg::sync(cta);
-
-  if (threadIdx.x < HISTOGRAM64_BIN_COUNT) {
-    uchar *s_HistBase =
-        s_Hist + UMUL(threadIdx.x, HISTOGRAM64_THREADBLOCK_SIZE);
-
-    uint sum = 0;
-    uint pos = 4 * (threadIdx.x & (SHARED_MEMORY_BANKS - 1));
-
-#pragma unroll
-
-    for (uint i = 0; i < (HISTOGRAM64_THREADBLOCK_SIZE / 4); i++) {
-      sum += s_HistBase[pos + 0] + s_HistBase[pos + 1] + s_HistBase[pos + 2] +
-             s_HistBase[pos + 3];
-      pos = (pos + 4) & (HISTOGRAM64_THREADBLOCK_SIZE - 1);
-    }
-
-    d_PartialHistograms[blockIdx.x * HISTOGRAM64_BIN_COUNT + threadIdx.x] = sum;
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Merge histogram64() output
-// Run one threadblock per bin; each threadbock adds up the same bin counter
-// from every partial histogram. Reads are uncoalesced, but mergeHistogram64
-// takes only a fraction of total processing time
-////////////////////////////////////////////////////////////////////////////////
-#define MERGE_THREADBLOCK_SIZE 256
-
-__global__ void mergeHistogram64Kernel(uint *d_Histogram,
-                                       uint *d_PartialHistograms,
-                                       uint histogramCount) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ uint data[MERGE_THREADBLOCK_SIZE];
-
-  uint sum = 0;
-
-  for (uint i = threadIdx.x; i < histogramCount; i += MERGE_THREADBLOCK_SIZE) {
-    sum += d_PartialHistograms[blockIdx.x + i * HISTOGRAM64_BIN_COUNT];
-  }
-
-  data[threadIdx.x] = sum;
-
-  for (uint stride = MERGE_THREADBLOCK_SIZE / 2; stride > 0; stride >>= 1) {
-    cg::sync(cta);
-
-    if (threadIdx.x < stride) {
-      data[threadIdx.x] += data[threadIdx.x + stride];
-    }
-  }
-
-  if (threadIdx.x == 0) {
-    d_Histogram[blockIdx.x] = data[0];
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// CPU interface to GPU histogram calculator
-////////////////////////////////////////////////////////////////////////////////
-// histogram64kernel() intermediate results buffer
-// MAX_PARTIAL_HISTOGRAM64_COUNT == 32768 and HISTOGRAM64_THREADBLOCK_SIZE == 64
-// amounts to max. 480MB of input data
-static const uint MAX_PARTIAL_HISTOGRAM64_COUNT = 32768;
-static uint *d_PartialHistograms;
-
-// Internal memory allocation
-extern "C" void initHistogram64(void) {
-  assert(HISTOGRAM64_THREADBLOCK_SIZE % (4 * SHARED_MEMORY_BANKS) == 0);
-  HIPCHECK(hipMalloc(
-      (void **)&d_PartialHistograms,
-      MAX_PARTIAL_HISTOGRAM64_COUNT * HISTOGRAM64_BIN_COUNT * sizeof(uint)));
-}
-
-// Internal memory deallocation
-extern "C" void closeHistogram64(void) {
-  HIPCHECK(hipFree(d_PartialHistograms));
-}
-
-// Round a / b to nearest higher integer value
-inline uint iDivUp(uint a, uint b) {
-  return (a % b != 0) ? (a / b + 1) : (a / b);
-}
-
-// Snap a to nearest lower multiple of b
-inline uint iSnapDown(uint a, uint b) { return a - a % b; }
-
-extern "C" void histogram64(uint *d_Histogram, void *d_Data, uint byteCount) {
-  const uint histogramCount = iDivUp(
-      byteCount, HISTOGRAM64_THREADBLOCK_SIZE * iSnapDown(255, sizeof(data_t)));
-
-  assert(byteCount % sizeof(data_t) == 0);
-  assert(histogramCount <= MAX_PARTIAL_HISTOGRAM64_COUNT);
-
-  histogram64Kernel<<<histogramCount, HISTOGRAM64_THREADBLOCK_SIZE>>>(
-      d_PartialHistograms, (data_t *)d_Data, byteCount / sizeof(data_t));
-  getLastCudaError("histogram64Kernel() execution failed\n");
-
-  mergeHistogram64Kernel<<<HISTOGRAM64_BIN_COUNT, MERGE_THREADBLOCK_SIZE>>>(
-      d_Histogram, d_PartialHistograms, histogramCount);
-  getLastCudaError("mergeHistogram64() execution failed\n");
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip
index 2cf603b..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/imageDenoising/imageDenoising.cu.hip
@@ -1,116 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample demonstrates two adaptive image denoising techniques:
- * KNN and NLM, based on computation of both geometric and color distance
- * between texels. While both techniques are already implemented in the
- * DirectX SDK using shaders, massively speeded up variation
- * of the latter technique, taking advantage of shared memory, is implemented
- * in addition to DirectX counterparts.
- * See supplied whitepaper for more explanations.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-#include <helper_cuda.h>
-#include "imageDenoising.h"
-
-////////////////////////////////////////////////////////////////////////////////
-// Helper functions
-////////////////////////////////////////////////////////////////////////////////
-float Max(float x, float y) { return (x > y) ? x : y; }
-
-float Min(float x, float y) { return (x < y) ? x : y; }
-
-int iDivUp(int a, int b) { return ((a % b) != 0) ? (a / b + 1) : (a / b); }
-
-__device__ float lerpf(float a, float b, float c) { return a + (b - a) * c; }
-
-__device__ float vecLen(float4 a, float4 b) {
-  return ((b.x - a.x) * (b.x - a.x) + (b.y - a.y) * (b.y - a.y) +
-          (b.z - a.z) * (b.z - a.z));
-}
-
-__device__ TColor make_color(float r, float g, float b, float a) {
-  return ((int)(a * 255.0f) << 24) | ((int)(b * 255.0f) << 16) |
-         ((int)(g * 255.0f) << 8) | ((int)(r * 255.0f) << 0);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Global data handlers and parameters
-////////////////////////////////////////////////////////////////////////////////
-// Texture object and channel descriptor for image texture
-hipTextureObject_t texImage;
-hipChannelFormatDesc uchar4tex = hipCreateChannelDesc<uchar4>();
-
-// CUDA array descriptor
-hipArray *a_Src;
-
-////////////////////////////////////////////////////////////////////////////////
-// Filtering kernels
-////////////////////////////////////////////////////////////////////////////////
-#include "imageDenoising_copy_kernel.cuh"
-#include "imageDenoising_knn_kernel.cuh"
-#include "imageDenoising_nlm_kernel.cuh"
-#include "imageDenoising_nlm2_kernel.cuh"
-
-extern "C" hipError_t CUDA_MallocArray(uchar4 **h_Src, int imageW,
-                                        int imageH) {
-  hipError_t error;
-
-  error = hipMallocArray(&a_Src, &uchar4tex, imageW, imageH);
-  error = hipMemcpy2DToArray(a_Src, 0, 0, *h_Src, sizeof(uchar4) * imageW,
-                              sizeof(uchar4) * imageW, imageH,
-                              hipMemcpyHostToDevice);
-
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = a_Src;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(hipCreateTextureObject(&texImage, &texRes, &texDescr, NULL));
-
-  return error;
-}
-
-extern "C" hipError_t CUDA_FreeArray() { return hipFreeArray(a_Src); }
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip
index e3766a6..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/inlinePTX/inlinePTX.cu.hip
@@ -1,115 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * Demonstration of inline PTX (assembly language) usage in CUDA kernels
- */
-
-// System includes
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <assert.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-__global__ void sequence_gpu(int *d_ptr, int length)
-{
-    int elemID = blockIdx.x * blockDim.x + threadIdx.x;
-
-    if (elemID < length)
-    {
-        unsigned int laneid;
-        //This command gets the lane ID within the current warp
-        asm("mov.u32 %0, %%laneid;" : "=r"(laneid));
-        d_ptr[elemID] = laneid;
-    }
-}
-
-
-void sequence_cpu(int *h_ptr, int length)
-{
-    for (int elemID=0; elemID<length; elemID++)
-    {
-        h_ptr[elemID] = elemID % 32;
-    }
-}
-
-int main(int argc, char **argv)
-{
-    printf("CUDA inline PTX assembler sample\n");
-
-    const int N = 1000;
-
-    int dev = findCudaDevice(argc, (const char **) argv);
-
-    if (dev == -1)
-    {
-        return EXIT_FAILURE;
-    }
-
-    int *d_ptr;
-    HIPCHECK(hipMalloc(&d_ptr, N * sizeof(int)));
-
-    int *h_ptr;
-    HIPCHECK(hipHostMalloc(&h_ptr, N * sizeof(int)));
-
-    dim3 cudaBlockSize(256,1,1);
-    dim3 cudaGridSize((N + cudaBlockSize.x - 1) / cudaBlockSize.x, 1, 1);
-    sequence_gpu<<<cudaGridSize, cudaBlockSize>>>(d_ptr, N);
-    HIPCHECK(hipGetLastError());
-    HIPCHECK(hipDeviceSynchronize());
-
-    sequence_cpu(h_ptr, N);
-
-    int *h_d_ptr;
-    HIPCHECK(hipHostMalloc(&h_d_ptr, N *sizeof(int)));
-    HIPCHECK(hipMemcpy(h_d_ptr, d_ptr, N *sizeof(int), hipMemcpyDeviceToHost));
-
-    bool bValid = true;
-
-    for (int i=0; i<N && bValid; i++)
-    {
-        if (h_ptr[i] != h_d_ptr[i])
-        {
-            bValid = false;
-        }
-    }
-
-    printf("Test %s.\n", bValid ? "Successful" : "Failed");
-
-    HIPCHECK(hipFree(d_ptr));
-    HIPCHECK(hipHostFree(h_ptr));
-    HIPCHECK(hipHostFree(h_d_ptr));
-
-    return bValid ? EXIT_SUCCESS: EXIT_FAILURE;
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
index 4ca03d2..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/interval/interval.cu.hip
@@ -1,167 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Example of program using the interval_gpu<T> template class and operators:
- * Search for roots of a function using an interval Newton method.
-  *
- * Use the command-line argument "--n=<N>" to select which GPU implementation to
- * use,
- * otherwise the naive implementation will be used by default.
- * 0: the naive implementation
- * 1: the optimized implementation
- * 2: the recursive implementation
- *
- */
-
-const static char *sSDKsample = "Interval Computing";
-
-
-#include <hip/hip_runtime.h>
-#include <iostream>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include "helper_cuda.h"
-#include "interval.h"
-#include "cuda_interval.h"
-#include "cpu_interval.h"
-
-int main(int argc, char *argv[]) {
-  int implementation_choice = 0;
-
-  printf("[%s]  starting ...\n\n", sSDKsample);
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "n")) {
-    implementation_choice =
-        getCmdLineArgumentInt(argc, (const char **)argv, "n");
-  }
-
-  // Pick the best GPU available, or if the developer selects one at the command
-  // line
-  int devID = findCudaDevice(argc, (const char **)argv);
-  hipDeviceProp_t deviceProp;
-  hipGetDeviceProperties(&deviceProp, devID);
-  printf("> GPU Device has Compute Capabilities SM %d.%d\n\n", deviceProp.major,
-         deviceProp.minor);
-
-  switch (implementation_choice) {
-    case 0:
-      printf("GPU naive implementation\n");
-      break;
-
-    case 1:
-      printf("GPU optimized implementation\n");
-      break;
-
-    case 2:
-      printf("GPU recursive implementation (requires Compute SM 2.0+)\n");
-      break;
-
-    default:
-      printf("GPU naive implementation\n");
-  }
-
-  interval_gpu<T> *d_result;
-  int *d_nresults;
-  int *h_nresults = new int[THREADS];
-  hipEvent_t start, stop;
-
-  CHECKED_CALL(hipSetDevice(devID));
-  CHECKED_CALL(hipMalloc((void **)&d_result,
-                          THREADS * DEPTH_RESULT * sizeof(*d_result)));
-  CHECKED_CALL(hipMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));
-  CHECKED_CALL(hipEventCreate(&start));
-  CHECKED_CALL(hipEventCreate(&stop));
-
-  // We need L1 cache to store the stack (only applicable to sm_20 and higher)
-  CHECKED_CALL(
-      hipFuncSetCacheConfig(test_interval_newton<T>, hipFuncCachePreferL1));
-
-  // Increase the stack size large enough for the non-inlined and recursive
-  // function calls (only applicable to sm_20 and higher)
-  CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));
-
-  interval_gpu<T> i(0.01f, 4.0f);
-  std::cout << "Searching for roots in [" << i.lower() << ", " << i.upper()
-            << "]...\n";
-
-  CHECKED_CALL(hipEventRecord(start, 0));
-
-  for (int it = 0; it < NUM_RUNS; ++it) {
-    test_interval_newton<T><<<GRID_SIZE, BLOCK_SIZE>>>(d_result, d_nresults, i,
-                                                       implementation_choice);
-    CHECKED_CALL(hipGetLastError());
-  }
-
-  CHECKED_CALL(hipEventRecord(stop, 0));
-  CHECKED_CALL(hipDeviceSynchronize());
-
-  I_CPU *h_result = new I_CPU[THREADS * DEPTH_RESULT];
-  CHECKED_CALL(hipMemcpy(h_result, d_result,
-                          THREADS * DEPTH_RESULT * sizeof(*d_result),
-                          hipMemcpyDeviceToHost));
-  CHECKED_CALL(hipMemcpy(h_nresults, d_nresults, THREADS * sizeof(*d_nresults),
-                          hipMemcpyDeviceToHost));
-
-  std::cout << "Found " << h_nresults[0]
-            << " intervals that may contain the root(s)\n";
-  std::cout.precision(15);
-
-  for (int i = 0; i != h_nresults[0]; ++i) {
-    std::cout << " i[" << i << "] ="
-              << " [" << h_result[THREADS * i + 0].lower() << ", "
-              << h_result[THREADS * i + 0].upper() << "]\n";
-  }
-
-  float time;
-  CHECKED_CALL(hipEventElapsedTime(&time, start, stop));
-  std::cout << "Number of equations solved: " << THREADS << "\n";
-  std::cout << "Time per equation: "
-            << 1000000.0f * (time / (float)(THREADS)) / NUM_RUNS << " us\n";
-
-  CHECKED_CALL(hipEventDestroy(start));
-  CHECKED_CALL(hipEventDestroy(stop));
-  CHECKED_CALL(hipFree(d_result));
-  CHECKED_CALL(hipFree(d_nresults));
-
-  // Compute the results using a CPU implementation based on the Boost library
-  I_CPU i_cpu(0.01f, 4.0f);
-  I_CPU *h_result_cpu = new I_CPU[THREADS * DEPTH_RESULT];
-  int *h_nresults_cpu = new int[THREADS];
-  test_interval_newton_cpu<I_CPU>(h_result_cpu, h_nresults_cpu, i_cpu);
-
-  // Compare the CPU and GPU results
-  bool bTestResult =
-      checkAgainstHost(h_nresults, h_nresults_cpu, h_result, h_result_cpu);
-
-  delete[] h_result_cpu;
-  delete[] h_nresults_cpu;
-  delete[] h_result;
-  delete[] h_nresults;
-
-  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/scalarProd/scalarProd.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/scalarProd/scalarProd.cu.hip
index 5a04c6b..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/scalarProd/scalarProd.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/scalarProd/scalarProd.cu.hip
@@ -1,172 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample calculates scalar products of a
- * given set of input vector pairs
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <time.h>
-#include <string.h>
-
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-///////////////////////////////////////////////////////////////////////////////
-// Calculate scalar products of VectorN vectors of ElementN elements on CPU
-///////////////////////////////////////////////////////////////////////////////
-extern "C" void scalarProdCPU(float *h_C, float *h_A, float *h_B, int vectorN,
-                              int elementN);
-
-///////////////////////////////////////////////////////////////////////////////
-// Calculate scalar products of VectorN vectors of ElementN elements on GPU
-///////////////////////////////////////////////////////////////////////////////
-#include "scalarProd_kernel.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// Helper function, returning uniformly distributed
-// random float in [low, high] range
-////////////////////////////////////////////////////////////////////////////////
-float RandFloat(float low, float high) {
-  float t = (float)rand() / (float)RAND_MAX;
-  return (1.0f - t) * low + t * high;
-}
-
-///////////////////////////////////////////////////////////////////////////////
-// Data configuration
-///////////////////////////////////////////////////////////////////////////////
-
-// Total number of input vector pairs; arbitrary
-const int VECTOR_N = 256;
-// Number of elements per vector; arbitrary,
-// but strongly preferred to be a multiple of warp size
-// to meet memory coalescing constraints
-const int ELEMENT_N = 4096;
-// Total number of data elements
-const int DATA_N = VECTOR_N * ELEMENT_N;
-
-const int DATA_SZ = DATA_N * sizeof(float);
-const int RESULT_SZ = VECTOR_N * sizeof(float);
-
-///////////////////////////////////////////////////////////////////////////////
-// Main program
-///////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  float *h_A, *h_B, *h_C_CPU, *h_C_GPU;
-  float *d_A, *d_B, *d_C;
-  double delta, ref, sum_delta, sum_ref, L1norm;
-  StopWatchInterface *hTimer = NULL;
-  int i;
-
-  printf("%s Starting...\n\n", argv[0]);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  findCudaDevice(argc, (const char **)argv);
-
-  sdkCreateTimer(&hTimer);
-
-  printf("Initializing data...\n");
-  printf("...allocating CPU memory.\n");
-  h_A = (float *)malloc(DATA_SZ);
-  h_B = (float *)malloc(DATA_SZ);
-  h_C_CPU = (float *)malloc(RESULT_SZ);
-  h_C_GPU = (float *)malloc(RESULT_SZ);
-
-  printf("...allocating GPU memory.\n");
-  HIPCHECK(hipMalloc((void **)&d_A, DATA_SZ));
-  HIPCHECK(hipMalloc((void **)&d_B, DATA_SZ));
-  HIPCHECK(hipMalloc((void **)&d_C, RESULT_SZ));
-
-  printf("...generating input data in CPU mem.\n");
-  srand(123);
-
-  // Generating input data on CPU
-  for (i = 0; i < DATA_N; i++) {
-    h_A[i] = RandFloat(0.0f, 1.0f);
-    h_B[i] = RandFloat(0.0f, 1.0f);
-  }
-
-  printf("...copying input data to GPU mem.\n");
-  // Copy options data to GPU memory for further processing
-  HIPCHECK(hipMemcpy(d_A, h_A, DATA_SZ, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(d_B, h_B, DATA_SZ, hipMemcpyHostToDevice));
-  printf("Data init done.\n");
-
-  printf("Executing GPU kernel...\n");
-  HIPCHECK(hipDeviceSynchronize());
-  sdkResetTimer(&hTimer);
-  sdkStartTimer(&hTimer);
-  scalarProdGPU<<<128, 256>>>(d_C, d_A, d_B, VECTOR_N, ELEMENT_N);
-  getLastCudaError("scalarProdGPU() execution failed\n");
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&hTimer);
-  printf("GPU time: %f msecs.\n", sdkGetTimerValue(&hTimer));
-
-  printf("Reading back GPU result...\n");
-  // Read back GPU results to compare them to CPU results
-  HIPCHECK(hipMemcpy(h_C_GPU, d_C, RESULT_SZ, hipMemcpyDeviceToHost));
-
-  printf("Checking GPU results...\n");
-  printf("..running CPU scalar product calculation\n");
-  scalarProdCPU(h_C_CPU, h_A, h_B, VECTOR_N, ELEMENT_N);
-
-  printf("...comparing the results\n");
-  // Calculate max absolute difference and L1 distance
-  // between CPU and GPU results
-  sum_delta = 0;
-  sum_ref = 0;
-
-  for (i = 0; i < VECTOR_N; i++) {
-    delta = fabs(h_C_GPU[i] - h_C_CPU[i]);
-    ref = h_C_CPU[i];
-    sum_delta += delta;
-    sum_ref += ref;
-  }
-
-  L1norm = sum_delta / sum_ref;
-
-  printf("Shutting down...\n");
-  HIPCHECK(hipFree(d_C));
-  HIPCHECK(hipFree(d_B));
-  HIPCHECK(hipFree(d_A));
-  free(h_C_GPU);
-  free(h_C_CPU);
-  free(h_B);
-  free(h_A);
-  sdkDeleteTimer(&hTimer);
-
-  printf("L1 error: %E\n", L1norm);
-  printf((L1norm < 1e-6) ? "Test passed\n" : "Test failed!\n");
-  exit(L1norm < 1e-6 ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip
index c03bc5c..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/scan/scan.cu.hip
@@ -1,269 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <helper_cuda.h>
-#include "scan_common.h"
-
-// All three kernels run 512 threads per workgroup
-// Must be a power of two
-#define THREADBLOCK_SIZE 256
-
-////////////////////////////////////////////////////////////////////////////////
-// Basic scan codelets
-////////////////////////////////////////////////////////////////////////////////
-// Naive inclusive scan: O(N * log2(N)) operations
-// Allocate 2 * 'size' local memory, initialize the first half
-// with 'size' zeros avoiding if(pos >= offset) condition evaluation
-// and saving instructions
-inline __device__ uint scan1Inclusive(uint idata, volatile uint *s_Data,
-                                      uint size, cg::thread_block cta) {
-  uint pos = 2 * threadIdx.x - (threadIdx.x & (size - 1));
-  s_Data[pos] = 0;
-  pos += size;
-  s_Data[pos] = idata;
-
-  for (uint offset = 1; offset < size; offset <<= 1) {
-    cg::sync(cta);
-    uint t = s_Data[pos] + s_Data[pos - offset];
-    cg::sync(cta);
-    s_Data[pos] = t;
-  }
-
-  return s_Data[pos];
-}
-
-inline __device__ uint scan1Exclusive(uint idata, volatile uint *s_Data,
-                                      uint size, cg::thread_block cta) {
-  return scan1Inclusive(idata, s_Data, size, cta) - idata;
-}
-
-inline __device__ uint4 scan4Inclusive(uint4 idata4, volatile uint *s_Data,
-                                       uint size, cg::thread_block cta) {
-  // Level-0 inclusive scan
-  idata4.y += idata4.x;
-  idata4.z += idata4.y;
-  idata4.w += idata4.z;
-
-  // Level-1 exclusive scan
-  uint oval = scan1Exclusive(idata4.w, s_Data, size / 4, cta);
-
-  idata4.x += oval;
-  idata4.y += oval;
-  idata4.z += oval;
-  idata4.w += oval;
-
-  return idata4;
-}
-
-// Exclusive vector scan: the array to be scanned is stored
-// in local thread memory scope as uint4
-inline __device__ uint4 scan4Exclusive(uint4 idata4, volatile uint *s_Data,
-                                       uint size, cg::thread_block cta) {
-  uint4 odata4 = scan4Inclusive(idata4, s_Data, size, cta);
-  odata4.x -= idata4.x;
-  odata4.y -= idata4.y;
-  odata4.z -= idata4.z;
-  odata4.w -= idata4.w;
-  return odata4;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Scan kernels
-////////////////////////////////////////////////////////////////////////////////
-__global__ void scanExclusiveShared(uint4 *d_Dst, uint4 *d_Src, uint size) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ uint s_Data[2 * THREADBLOCK_SIZE];
-
-  uint pos = blockIdx.x * blockDim.x + threadIdx.x;
-
-  // Load data
-  uint4 idata4 = d_Src[pos];
-
-  // Calculate exclusive scan
-  uint4 odata4 = scan4Exclusive(idata4, s_Data, size, cta);
-
-  // Write back
-  d_Dst[pos] = odata4;
-}
-
-// Exclusive scan of top elements of bottom-level scans (4 * THREADBLOCK_SIZE)
-__global__ void scanExclusiveShared2(uint *d_Buf, uint *d_Dst, uint *d_Src,
-                                     uint N, uint arrayLength) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ uint s_Data[2 * THREADBLOCK_SIZE];
-
-  // Skip loads and stores for inactive threads of last threadblock (pos >= N)
-  uint pos = blockIdx.x * blockDim.x + threadIdx.x;
-
-  // Load top elements
-  // Convert results of bottom-level scan back to inclusive
-  uint idata = 0;
-
-  if (pos < N)
-    idata = d_Dst[(4 * THREADBLOCK_SIZE) - 1 + (4 * THREADBLOCK_SIZE) * pos] +
-            d_Src[(4 * THREADBLOCK_SIZE) - 1 + (4 * THREADBLOCK_SIZE) * pos];
-
-  // Compute
-  uint odata = scan1Exclusive(idata, s_Data, arrayLength, cta);
-
-  // Avoid out-of-bound access
-  if (pos < N) {
-    d_Buf[pos] = odata;
-  }
-}
-
-// Final step of large-array scan: combine basic inclusive scan with exclusive
-// scan of top elements of input arrays
-__global__ void uniformUpdate(uint4 *d_Data, uint *d_Buffer) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ uint buf;
-  uint pos = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (threadIdx.x == 0) {
-    buf = d_Buffer[blockIdx.x];
-  }
-
-  cg::sync(cta);
-
-  uint4 data4 = d_Data[pos];
-  data4.x += buf;
-  data4.y += buf;
-  data4.z += buf;
-  data4.w += buf;
-  d_Data[pos] = data4;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Interface function
-////////////////////////////////////////////////////////////////////////////////
-// Derived as 32768 (max power-of-two gridDim.x) * 4 * THREADBLOCK_SIZE
-// Due to scanExclusiveShared<<<>>>() 1D block addressing
-extern "C" const uint MAX_BATCH_ELEMENTS = 64 * 1048576;
-extern "C" const uint MIN_SHORT_ARRAY_SIZE = 4;
-extern "C" const uint MAX_SHORT_ARRAY_SIZE = 4 * THREADBLOCK_SIZE;
-extern "C" const uint MIN_LARGE_ARRAY_SIZE = 8 * THREADBLOCK_SIZE;
-extern "C" const uint MAX_LARGE_ARRAY_SIZE =
-    4 * THREADBLOCK_SIZE * THREADBLOCK_SIZE;
-
-// Internal exclusive scan buffer
-static uint *d_Buf;
-
-extern "C" void initScan(void) {
-  HIPCHECK(
-      hipMalloc((void **)&d_Buf,
-                 (MAX_BATCH_ELEMENTS / (4 * THREADBLOCK_SIZE)) * sizeof(uint)));
-}
-
-extern "C" void closeScan(void) { HIPCHECK(hipFree(d_Buf)); }
-
-static uint factorRadix2(uint &log2L, uint L) {
-  if (!L) {
-    log2L = 0;
-    return 0;
-  } else {
-    for (log2L = 0; (L & 1) == 0; L >>= 1, log2L++)
-      ;
-
-    return L;
-  }
-}
-
-static uint iDivUp(uint dividend, uint divisor) {
-  return ((dividend % divisor) == 0) ? (dividend / divisor)
-                                     : (dividend / divisor + 1);
-}
-
-extern "C" size_t scanExclusiveShort(uint *d_Dst, uint *d_Src, uint batchSize,
-                                     uint arrayLength) {
-  // Check power-of-two factorization
-  uint log2L;
-  uint factorizationRemainder = factorRadix2(log2L, arrayLength);
-  assert(factorizationRemainder == 1);
-
-  // Check supported size range
-  assert((arrayLength >= MIN_SHORT_ARRAY_SIZE) &&
-         (arrayLength <= MAX_SHORT_ARRAY_SIZE));
-
-  // Check total batch size limit
-  assert((batchSize * arrayLength) <= MAX_BATCH_ELEMENTS);
-
-  // Check all threadblocks to be fully packed with data
-  assert((batchSize * arrayLength) % (4 * THREADBLOCK_SIZE) == 0);
-
-  scanExclusiveShared<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE),
-                        THREADBLOCK_SIZE>>>((uint4 *)d_Dst, (uint4 *)d_Src,
-                                            arrayLength);
-  getLastCudaError("scanExclusiveShared() execution FAILED\n");
-
-  return THREADBLOCK_SIZE;
-}
-
-extern "C" size_t scanExclusiveLarge(uint *d_Dst, uint *d_Src, uint batchSize,
-                                     uint arrayLength) {
-  // Check power-of-two factorization
-  uint log2L;
-  uint factorizationRemainder = factorRadix2(log2L, arrayLength);
-  assert(factorizationRemainder == 1);
-
-  // Check supported size range
-  assert((arrayLength >= MIN_LARGE_ARRAY_SIZE) &&
-         (arrayLength <= MAX_LARGE_ARRAY_SIZE));
-
-  // Check total batch size limit
-  assert((batchSize * arrayLength) <= MAX_BATCH_ELEMENTS);
-
-  scanExclusiveShared<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE),
-                        THREADBLOCK_SIZE>>>((uint4 *)d_Dst, (uint4 *)d_Src,
-                                            4 * THREADBLOCK_SIZE);
-  getLastCudaError("scanExclusiveShared() execution FAILED\n");
-
-  // Not all threadblocks need to be packed with input data:
-  // inactive threads of highest threadblock just don't do global reads and
-  // writes
-  const uint blockCount2 = iDivUp(
-      (batchSize * arrayLength) / (4 * THREADBLOCK_SIZE), THREADBLOCK_SIZE);
-  scanExclusiveShared2<<<blockCount2, THREADBLOCK_SIZE>>>(
-      (uint *)d_Buf, (uint *)d_Dst, (uint *)d_Src,
-      (batchSize * arrayLength) / (4 * THREADBLOCK_SIZE),
-      arrayLength / (4 * THREADBLOCK_SIZE));
-  getLastCudaError("scanExclusiveShared2() execution FAILED\n");
-
-  uniformUpdate<<<(batchSize * arrayLength) / (4 * THREADBLOCK_SIZE),
-                  THREADBLOCK_SIZE>>>((uint4 *)d_Dst, (uint *)d_Buf);
-  getLastCudaError("uniformUpdate() execution FAILED\n");
-
-  return THREADBLOCK_SIZE;
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/bitonicSort.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/bitonicSort.cu.hip
index 9c9aa43..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/bitonicSort.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/bitonicSort.cu.hip
@@ -1,278 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-//Based on http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/bitonic/bitonicen.htm
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <helper_cuda.h>
-#include "sortingNetworks_common.h"
-#include "sortingNetworks_common.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// Monolithic bitonic sort kernel for short arrays fitting into shared memory
-////////////////////////////////////////////////////////////////////////////////
-__global__ void bitonicSortShared(uint *d_DstKey, uint *d_DstVal,
-                                  uint *d_SrcKey, uint *d_SrcVal,
-                                  uint arrayLength, uint dir) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Shared memory storage for one or more short vectors
-  __shared__ uint s_key[SHARED_SIZE_LIMIT];
-  __shared__ uint s_val[SHARED_SIZE_LIMIT];
-
-  // Offset to the beginning of subbatch and load data
-  d_SrcKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_SrcVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  s_key[threadIdx.x + 0] = d_SrcKey[0];
-  s_val[threadIdx.x + 0] = d_SrcVal[0];
-  s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcKey[(SHARED_SIZE_LIMIT / 2)];
-  s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcVal[(SHARED_SIZE_LIMIT / 2)];
-
-  for (uint size = 2; size < arrayLength; size <<= 1) {
-    // Bitonic merge
-    uint ddd = dir ^ ((threadIdx.x & (size / 2)) != 0);
-
-    for (uint stride = size / 2; stride > 0; stride >>= 1) {
-      cg::sync(cta);
-      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
-                 s_val[pos + stride], ddd);
-    }
-  }
-
-  // ddd == dir for the last bitonic merge step
-  {
-    for (uint stride = arrayLength / 2; stride > 0; stride >>= 1) {
-      cg::sync(cta);
-      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
-                 s_val[pos + stride], dir);
-    }
-  }
-
-  cg::sync(cta);
-  d_DstKey[0] = s_key[threadIdx.x + 0];
-  d_DstVal[0] = s_val[threadIdx.x + 0];
-  d_DstKey[(SHARED_SIZE_LIMIT / 2)] =
-      s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-  d_DstVal[(SHARED_SIZE_LIMIT / 2)] =
-      s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Bitonic sort kernel for large arrays (not fitting into shared memory)
-////////////////////////////////////////////////////////////////////////////////
-// Bottom-level bitonic sort
-// Almost the same as bitonicSortShared with the exception of
-// even / odd subarrays being sorted in opposite directions
-// Bitonic merge accepts both
-// Ascending | descending or descending | ascending sorted pairs
-__global__ void bitonicSortShared1(uint *d_DstKey, uint *d_DstVal,
-                                   uint *d_SrcKey, uint *d_SrcVal) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Shared memory storage for current subarray
-  __shared__ uint s_key[SHARED_SIZE_LIMIT];
-  __shared__ uint s_val[SHARED_SIZE_LIMIT];
-
-  // Offset to the beginning of subarray and load data
-  d_SrcKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_SrcVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  s_key[threadIdx.x + 0] = d_SrcKey[0];
-  s_val[threadIdx.x + 0] = d_SrcVal[0];
-  s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcKey[(SHARED_SIZE_LIMIT / 2)];
-  s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcVal[(SHARED_SIZE_LIMIT / 2)];
-
-  for (uint size = 2; size < SHARED_SIZE_LIMIT; size <<= 1) {
-    // Bitonic merge
-    uint ddd = (threadIdx.x & (size / 2)) != 0;
-
-    for (uint stride = size / 2; stride > 0; stride >>= 1) {
-      cg::sync(cta);
-      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
-                 s_val[pos + stride], ddd);
-    }
-  }
-
-  // Odd / even arrays of SHARED_SIZE_LIMIT elements
-  // sorted in opposite directions
-  uint ddd = blockIdx.x & 1;
-  {
-    for (uint stride = SHARED_SIZE_LIMIT / 2; stride > 0; stride >>= 1) {
-      cg::sync(cta);
-      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
-                 s_val[pos + stride], ddd);
-    }
-  }
-
-  cg::sync(cta);
-  d_DstKey[0] = s_key[threadIdx.x + 0];
-  d_DstVal[0] = s_val[threadIdx.x + 0];
-  d_DstKey[(SHARED_SIZE_LIMIT / 2)] =
-      s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-  d_DstVal[(SHARED_SIZE_LIMIT / 2)] =
-      s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-}
-
-// Bitonic merge iteration for stride >= SHARED_SIZE_LIMIT
-__global__ void bitonicMergeGlobal(uint *d_DstKey, uint *d_DstVal,
-                                   uint *d_SrcKey, uint *d_SrcVal,
-                                   uint arrayLength, uint size, uint stride,
-                                   uint dir) {
-  uint global_comparatorI = blockIdx.x * blockDim.x + threadIdx.x;
-  uint comparatorI = global_comparatorI & (arrayLength / 2 - 1);
-
-  // Bitonic merge
-  uint ddd = dir ^ ((comparatorI & (size / 2)) != 0);
-  uint pos = 2 * global_comparatorI - (global_comparatorI & (stride - 1));
-
-  uint keyA = d_SrcKey[pos + 0];
-  uint valA = d_SrcVal[pos + 0];
-  uint keyB = d_SrcKey[pos + stride];
-  uint valB = d_SrcVal[pos + stride];
-
-  Comparator(keyA, valA, keyB, valB, ddd);
-
-  d_DstKey[pos + 0] = keyA;
-  d_DstVal[pos + 0] = valA;
-  d_DstKey[pos + stride] = keyB;
-  d_DstVal[pos + stride] = valB;
-}
-
-// Combined bitonic merge steps for
-// size > SHARED_SIZE_LIMIT and stride = [1 .. SHARED_SIZE_LIMIT / 2]
-__global__ void bitonicMergeShared(uint *d_DstKey, uint *d_DstVal,
-                                   uint *d_SrcKey, uint *d_SrcVal,
-                                   uint arrayLength, uint size, uint dir) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Shared memory storage for current subarray
-  __shared__ uint s_key[SHARED_SIZE_LIMIT];
-  __shared__ uint s_val[SHARED_SIZE_LIMIT];
-
-  d_SrcKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_SrcVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  s_key[threadIdx.x + 0] = d_SrcKey[0];
-  s_val[threadIdx.x + 0] = d_SrcVal[0];
-  s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcKey[(SHARED_SIZE_LIMIT / 2)];
-  s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcVal[(SHARED_SIZE_LIMIT / 2)];
-
-  // Bitonic merge
-  uint comparatorI =
-      UMAD(blockIdx.x, blockDim.x, threadIdx.x) & ((arrayLength / 2) - 1);
-  uint ddd = dir ^ ((comparatorI & (size / 2)) != 0);
-
-  for (uint stride = SHARED_SIZE_LIMIT / 2; stride > 0; stride >>= 1) {
-    cg::sync(cta);
-    uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-    Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
-               s_val[pos + stride], ddd);
-  }
-
-  cg::sync(cta);
-  d_DstKey[0] = s_key[threadIdx.x + 0];
-  d_DstVal[0] = s_val[threadIdx.x + 0];
-  d_DstKey[(SHARED_SIZE_LIMIT / 2)] =
-      s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-  d_DstVal[(SHARED_SIZE_LIMIT / 2)] =
-      s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Interface function
-////////////////////////////////////////////////////////////////////////////////
-// Helper function (also used by odd-even merge sort)
-extern "C" uint factorRadix2(uint *log2L, uint L) {
-  if (!L) {
-    *log2L = 0;
-    return 0;
-  } else {
-    for (*log2L = 0; (L & 1) == 0; L >>= 1, *log2L++)
-      ;
-
-    return L;
-  }
-}
-
-extern "C" uint bitonicSort(uint *d_DstKey, uint *d_DstVal, uint *d_SrcKey,
-                            uint *d_SrcVal, uint batchSize, uint arrayLength,
-                            uint dir) {
-  // Nothing to sort
-  if (arrayLength < 2) return 0;
-
-  // Only power-of-two array lengths are supported by this implementation
-  uint log2L;
-  uint factorizationRemainder = factorRadix2(&log2L, arrayLength);
-  assert(factorizationRemainder == 1);
-
-  dir = (dir != 0);
-
-  uint blockCount = batchSize * arrayLength / SHARED_SIZE_LIMIT;
-  uint threadCount = SHARED_SIZE_LIMIT / 2;
-
-  if (arrayLength <= SHARED_SIZE_LIMIT) {
-    assert((batchSize * arrayLength) % SHARED_SIZE_LIMIT == 0);
-    bitonicSortShared<<<blockCount, threadCount>>>(d_DstKey, d_DstVal, d_SrcKey,
-                                                   d_SrcVal, arrayLength, dir);
-  } else {
-    bitonicSortShared1<<<blockCount, threadCount>>>(d_DstKey, d_DstVal,
-                                                    d_SrcKey, d_SrcVal);
-
-    for (uint size = 2 * SHARED_SIZE_LIMIT; size <= arrayLength; size <<= 1)
-      for (unsigned stride = size / 2; stride > 0; stride >>= 1)
-        if (stride >= SHARED_SIZE_LIMIT) {
-          bitonicMergeGlobal<<<(batchSize * arrayLength) / 512, 256>>>(
-              d_DstKey, d_DstVal, d_DstKey, d_DstVal, arrayLength, size, stride,
-              dir);
-        } else {
-          bitonicMergeShared<<<blockCount, threadCount>>>(
-              d_DstKey, d_DstVal, d_DstKey, d_DstVal, arrayLength, size, dir);
-          break;
-        }
-  }
-
-  return threadCount;
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/oddEvenMergeSort.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/oddEvenMergeSort.cu.hip
index f7c5e44..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/oddEvenMergeSort.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/sortingNetworks/oddEvenMergeSort.cu.hip
@@ -1,182 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-//Based on http://www.iti.fh-flensburg.de/lang/algorithmen/sortieren/networks/oemen.htm
-
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-
-#include <helper_cuda.h>
-#include "sortingNetworks_common.h"
-#include "sortingNetworks_common.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// Monolithic Bacther's sort kernel for short arrays fitting into shared memory
-////////////////////////////////////////////////////////////////////////////////
-__global__ void oddEvenMergeSortShared(uint *d_DstKey, uint *d_DstVal,
-                                       uint *d_SrcKey, uint *d_SrcVal,
-                                       uint arrayLength, uint dir) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  // Shared memory storage for one or more small vectors
-  __shared__ uint s_key[SHARED_SIZE_LIMIT];
-  __shared__ uint s_val[SHARED_SIZE_LIMIT];
-
-  // Offset to the beginning of subbatch and load data
-  d_SrcKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_SrcVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstKey += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  d_DstVal += blockIdx.x * SHARED_SIZE_LIMIT + threadIdx.x;
-  s_key[threadIdx.x + 0] = d_SrcKey[0];
-  s_val[threadIdx.x + 0] = d_SrcVal[0];
-  s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcKey[(SHARED_SIZE_LIMIT / 2)];
-  s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)] =
-      d_SrcVal[(SHARED_SIZE_LIMIT / 2)];
-
-  for (uint size = 2; size <= arrayLength; size <<= 1) {
-    uint stride = size / 2;
-    uint offset = threadIdx.x & (stride - 1);
-
-    {
-      cg::sync(cta);
-      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-      Comparator(s_key[pos + 0], s_val[pos + 0], s_key[pos + stride],
-                 s_val[pos + stride], dir);
-      stride >>= 1;
-    }
-
-    for (; stride > 0; stride >>= 1) {
-      cg::sync(cta);
-      uint pos = 2 * threadIdx.x - (threadIdx.x & (stride - 1));
-
-      if (offset >= stride)
-        Comparator(s_key[pos - stride], s_val[pos - stride], s_key[pos + 0],
-                   s_val[pos + 0], dir);
-    }
-  }
-
-  cg::sync(cta);
-  d_DstKey[0] = s_key[threadIdx.x + 0];
-  d_DstVal[0] = s_val[threadIdx.x + 0];
-  d_DstKey[(SHARED_SIZE_LIMIT / 2)] =
-      s_key[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-  d_DstVal[(SHARED_SIZE_LIMIT / 2)] =
-      s_val[threadIdx.x + (SHARED_SIZE_LIMIT / 2)];
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Odd-even merge sort iteration kernel
-// for large arrays (not fitting into shared memory)
-////////////////////////////////////////////////////////////////////////////////
-__global__ void oddEvenMergeGlobal(uint *d_DstKey, uint *d_DstVal,
-                                   uint *d_SrcKey, uint *d_SrcVal,
-                                   uint arrayLength, uint size, uint stride,
-                                   uint dir) {
-  uint global_comparatorI = blockIdx.x * blockDim.x + threadIdx.x;
-
-  // Odd-even merge
-  uint pos = 2 * global_comparatorI - (global_comparatorI & (stride - 1));
-
-  if (stride < size / 2) {
-    uint offset = global_comparatorI & ((size / 2) - 1);
-
-    if (offset >= stride) {
-      uint keyA = d_SrcKey[pos - stride];
-      uint valA = d_SrcVal[pos - stride];
-      uint keyB = d_SrcKey[pos + 0];
-      uint valB = d_SrcVal[pos + 0];
-
-      Comparator(keyA, valA, keyB, valB, dir);
-
-      d_DstKey[pos - stride] = keyA;
-      d_DstVal[pos - stride] = valA;
-      d_DstKey[pos + 0] = keyB;
-      d_DstVal[pos + 0] = valB;
-    }
-  } else {
-    uint keyA = d_SrcKey[pos + 0];
-    uint valA = d_SrcVal[pos + 0];
-    uint keyB = d_SrcKey[pos + stride];
-    uint valB = d_SrcVal[pos + stride];
-
-    Comparator(keyA, valA, keyB, valB, dir);
-
-    d_DstKey[pos + 0] = keyA;
-    d_DstVal[pos + 0] = valA;
-    d_DstKey[pos + stride] = keyB;
-    d_DstVal[pos + stride] = valB;
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Interface function
-////////////////////////////////////////////////////////////////////////////////
-// Helper function
-extern "C" uint factorRadix2(uint *log2L, uint L);
-
-extern "C" void oddEvenMergeSort(uint *d_DstKey, uint *d_DstVal, uint *d_SrcKey,
-                                 uint *d_SrcVal, uint batchSize,
-                                 uint arrayLength, uint dir) {
-  // Nothing to sort
-  if (arrayLength < 2) return;
-
-  // Only power-of-two array lengths are supported by this implementation
-  uint log2L;
-  uint factorizationRemainder = factorRadix2(&log2L, arrayLength);
-  assert(factorizationRemainder == 1);
-
-  dir = (dir != 0);
-
-  uint blockCount = (batchSize * arrayLength) / SHARED_SIZE_LIMIT;
-  uint threadCount = SHARED_SIZE_LIMIT / 2;
-
-  if (arrayLength <= SHARED_SIZE_LIMIT) {
-    assert(SHARED_SIZE_LIMIT % arrayLength == 0);
-    oddEvenMergeSortShared<<<blockCount, threadCount>>>(
-        d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, arrayLength, dir);
-  } else {
-    oddEvenMergeSortShared<<<blockCount, threadCount>>>(
-        d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, SHARED_SIZE_LIMIT, dir);
-
-    for (uint size = 2 * SHARED_SIZE_LIMIT; size <= arrayLength; size <<= 1)
-      for (unsigned stride = size / 2; stride > 0; stride >>= 1) {
-        // Unlike with bitonic sort, combining bitonic merge steps with
-        // stride = [SHARED_SIZE_LIMIT / 2 .. 1] seems to be impossible as there
-        // are dependencies between data elements crossing the SHARED_SIZE_LIMIT
-        // borders
-        oddEvenMergeGlobal<<<(batchSize * arrayLength) / 512, 256>>>(
-            d_DstKey, d_DstVal, d_DstKey, d_DstVal, arrayLength, size, stride,
-            dir);
-      }
-  }
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocation/streamOrderedAllocation.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocation/streamOrderedAllocation.cu.hip
index 8ec9302..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocation/streamOrderedAllocation.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocation/streamOrderedAllocation.cu.hip
@@ -1,245 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample demonstrates stream ordered memory allocation on a GPU using
- * cudaMallocAsync and cudaMemPool family of APIs.
- *
- * basicStreamOrderedAllocation(): demonstrates stream ordered allocation using
- * cudaMallocAsync/cudaFreeAsync APIs with default settings.
- *
- * streamOrderedAllocationPostSync(): demonstrates if there's a synchronization
- * in between allocations, then setting the release threshold on the pool will
- * make sure the synchronize will not free memory back to the OS.
- */
-
-// System includes
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <climits>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-#define MAX_ITER 20
-
-/* Add two vectors on the GPU */
-__global__ void vectorAddGPU(const float *a, const float *b, float *c, int N) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < N) {
-    c[idx] = a[idx] + b[idx];
-  }
-}
-
-int basicStreamOrderedAllocation(const int dev, const int nelem, const float *a,
-                                 const float *b, float *c) {
-  float *d_a, *d_b, *d_c;  // Device buffers
-  float errorNorm, refNorm, ref, diff;
-  size_t bytes = nelem * sizeof(float);
-
-  hipStream_t stream;
-  printf("Starting basicStreamOrderedAllocation()\n");
-  HIPCHECK(hipSetDevice(dev));
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  HIPCHECK(hipMallocAsync(&d_a, bytes, stream));
-  HIPCHECK(hipMallocAsync(&d_b, bytes, stream));
-  HIPCHECK(hipMallocAsync(&d_c, bytes, stream));
-  HIPCHECK(
-      hipMemcpyAsync(d_a, a, bytes, hipMemcpyHostToDevice, stream));
-  HIPCHECK(
-      hipMemcpyAsync(d_b, b, bytes, hipMemcpyHostToDevice, stream));
-
-  dim3 block(256);
-  dim3 grid((unsigned int)ceil(nelem / (float)block.x));
-  vectorAddGPU<<<grid, block, 0, stream>>>(d_a, d_b, d_c, nelem);
-
-  HIPCHECK(hipFreeAsync(d_a, stream));
-  HIPCHECK(hipFreeAsync(d_b, stream));
-  HIPCHECK(
-      hipMemcpyAsync(c, d_c, bytes, hipMemcpyDeviceToHost, stream));
-  HIPCHECK(hipFreeAsync(d_c, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-
-  /* Compare the results */
-  printf("> Checking the results from vectorAddGPU() ...\n");
-  errorNorm = 0.f;
-  refNorm = 0.f;
-
-  for (int n = 0; n < nelem; n++) {
-    ref = a[n] + b[n];
-    diff = c[n] - ref;
-    errorNorm += diff * diff;
-    refNorm += ref * ref;
-  }
-
-  errorNorm = (float)sqrt((double)errorNorm);
-  refNorm = (float)sqrt((double)refNorm);
-  if (errorNorm / refNorm < 1.e-6f)
-    printf("basicStreamOrderedAllocation PASSED\n");
-
-  HIPCHECK(hipStreamDestroy(stream));
-
-  return errorNorm / refNorm < 1.e-6f ? EXIT_SUCCESS : EXIT_FAILURE;
-}
-
-// streamOrderedAllocationPostSync(): demonstrates If the application wants the
-// memory to persist in the pool beyond synchronization, then it sets the
-// release threshold on the pool. This way, when the application reaches the
-// "steady state", it is no longer allocating/freeing memory from the OS.
-int streamOrderedAllocationPostSync(const int dev, const int nelem,
-                                    const float *a, const float *b, float *c) {
-  float *d_a, *d_b, *d_c;  // Device buffers
-  float errorNorm, refNorm, ref, diff;
-  size_t bytes = nelem * sizeof(float);
-
-  hipStream_t stream;
-  hipMemPool_t memPool;
-  hipEvent_t start, end;
-  printf("Starting streamOrderedAllocationPostSync()\n");
-  HIPCHECK(hipSetDevice(dev));
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&end));
-
-  HIPCHECK(hipDeviceGetDefaultMemPool(&memPool, dev));
-  uint64_t thresholdVal = ULONG_MAX;
-  // set high release threshold on the default pool so that cudaFreeAsync will
-  // not actually release memory to the system. By default, the release
-  // threshold for a memory pool is set to zero. This implies that the CUDA
-  // driver is allowed to release a memory chunk back to the system as long as
-  // it does not contain any active suballocations.
-  HIPCHECK(hipMemPoolSetAttribute(
-      memPool, hipMemPoolAttrReleaseThreshold, (void *)&thresholdVal));
-
-  // Record the start event
-  HIPCHECK(hipEventRecord(start, stream));
-  for (int i = 0; i < MAX_ITER; i++) {
-    HIPCHECK(hipMallocAsync(&d_a, bytes, stream));
-    HIPCHECK(hipMallocAsync(&d_b, bytes, stream));
-    HIPCHECK(hipMallocAsync(&d_c, bytes, stream));
-    HIPCHECK(
-        hipMemcpyAsync(d_a, a, bytes, hipMemcpyHostToDevice, stream));
-    HIPCHECK(
-        hipMemcpyAsync(d_b, b, bytes, hipMemcpyHostToDevice, stream));
-
-    dim3 block(256);
-    dim3 grid((unsigned int)ceil(nelem / (float)block.x));
-    vectorAddGPU<<<grid, block, 0, stream>>>(d_a, d_b, d_c, nelem);
-
-    HIPCHECK(hipFreeAsync(d_a, stream));
-    HIPCHECK(hipFreeAsync(d_b, stream));
-    HIPCHECK(
-        hipMemcpyAsync(c, d_c, bytes, hipMemcpyDeviceToHost, stream));
-    HIPCHECK(hipFreeAsync(d_c, stream));
-    HIPCHECK(hipStreamSynchronize(stream));
-  }
-  HIPCHECK(hipEventRecord(end, stream));
-  // Wait for the end event to complete
-  HIPCHECK(hipEventSynchronize(end));
-
-  float msecTotal = 0.0f;
-  HIPCHECK(hipEventElapsedTime(&msecTotal, start, end));
-  printf("Total elapsed time = %f ms over %d iterations\n", msecTotal,
-         MAX_ITER);
-
-  /* Compare the results */
-  printf("> Checking the results from vectorAddGPU() ...\n");
-  errorNorm = 0.f;
-  refNorm = 0.f;
-
-  for (int n = 0; n < nelem; n++) {
-    ref = a[n] + b[n];
-    diff = c[n] - ref;
-    errorNorm += diff * diff;
-    refNorm += ref * ref;
-  }
-
-  errorNorm = (float)sqrt((double)errorNorm);
-  refNorm = (float)sqrt((double)refNorm);
-  if (errorNorm / refNorm < 1.e-6f)
-    printf("streamOrderedAllocationPostSync PASSED\n");
-
-  HIPCHECK(hipStreamDestroy(stream));
-
-  return errorNorm / refNorm < 1.e-6f ? EXIT_SUCCESS : EXIT_FAILURE;
-}
-
-int main(int argc, char **argv) {
-  int nelem;
-  int dev = 0;  // use default device 0
-  size_t bytes;
-  float *a, *b, *c;  // Host
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "help")) {
-    printf("Usage:  streamOrderedAllocation [OPTION]\n\n");
-    printf("Options:\n");
-    printf("  --device=[device #]  Specify the device to be used\n");
-    return EXIT_SUCCESS;
-  }
-
-  dev = findCudaDevice(argc, (const char **)argv);
-
-  int isMemPoolSupported = 0;
-  HIPCHECK(hipDeviceGetAttribute(&isMemPoolSupported,
-                                         hipDeviceAttributeMemoryPoolsSupported, dev));
-  if (!isMemPoolSupported) {
-    printf("Waiving execution as device does not support Memory Pools\n");
-    exit(EXIT_WAIVED);
-  }
-
-  // Allocate CPU memory.
-  nelem = 1048576;
-  bytes = nelem * sizeof(float);
-
-  a = (float *)malloc(bytes);
-  b = (float *)malloc(bytes);
-  c = (float *)malloc(bytes);
-  /* Initialize the vectors. */
-  for (int n = 0; n < nelem; n++) {
-    a[n] = rand() / (float)RAND_MAX;
-    b[n] = rand() / (float)RAND_MAX;
-  }
-
-  int ret1 = basicStreamOrderedAllocation(dev, nelem, a, b, c);
-  int ret2 = streamOrderedAllocationPostSync(dev, nelem, a, b, c);
-
-  /* Memory clean up */
-  free(a);
-  free(b);
-  free(c);
-
-  return ((ret1 == EXIT_SUCCESS && ret2 == EXIT_SUCCESS) ? EXIT_SUCCESS
-                                                         : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
index 5a34cb1..e69de29 100644
--- a/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
+++ b/src/samples/Samples/2_Concepts_and_Techniques/streamOrderedAllocationP2P/streamOrderedAllocationP2P.cu.hip
@@ -1,259 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample demonstrates peer-to-peer access of stream ordered memory
- * allocated with cudaMallocAsync and cudaMemPool family of APIs through simple
- * kernel which does peer-to-peer to access & scales vector elements.
- */
-
-// System includes
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <iostream>
-#include <map>
-#include <set>
-#include <utility>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-// Simple kernel to demonstrate copying cudaMallocAsync memory via P2P to peer
-// device
-__global__ void copyP2PAndScale(const int *src, int *dst, int N) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < N) {
-    // scale & store src vector.
-    dst[idx] = 2 * src[idx];
-  }
-}
-
-// Map of device version to device number
-std::multimap<std::pair<int, int>, int> getIdenticalGPUs() {
-  int numGpus = 0;
-  HIPCHECK(hipGetDeviceCount(&numGpus));
-
-  std::multimap<std::pair<int, int>, int> identicalGpus;
-
-  for (int i = 0; i < numGpus; i++) {
-    int isMemPoolSupported = 0;
-    HIPCHECK(hipDeviceGetAttribute(&isMemPoolSupported,
-                                           hipDeviceAttributeMemoryPoolsSupported, i));
-
-    // Filter unsupported devices
-    if (isMemPoolSupported) {
-      int major = 0, minor = 0;
-      HIPCHECK(
-          hipDeviceGetAttribute(&major, hipDeviceAttributeComputeCapabilityMajor, i));
-      HIPCHECK(
-          hipDeviceGetAttribute(&minor, hipDeviceAttributeComputeCapabilityMinor, i));
-      identicalGpus.emplace(std::make_pair(major, minor), i);
-    }
-  }
-
-  return identicalGpus;
-}
-
-std::pair<int, int> getP2PCapableGpuPair() {
-  constexpr size_t kNumGpusRequired = 2;
-
-  auto gpusByArch = getIdenticalGPUs();
-
-  auto it = gpusByArch.begin();
-  auto end = gpusByArch.end();
-
-  auto bestFit = std::make_pair(it, it);
-  // use std::distance to find the largest number of GPUs amongst architectures
-  auto distance = [](decltype(bestFit) p) {
-    return std::distance(p.first, p.second);
-  };
-
-  // Read each unique key/pair element in order
-  for (; it != end; it = gpusByArch.upper_bound(it->first)) {
-    // first and second are iterators bounded within the architecture group
-    auto testFit = gpusByArch.equal_range(it->first);
-    // Always use devices with highest architecture version or whichever has the
-    // most devices available
-    if (distance(bestFit) <= distance(testFit)) bestFit = testFit;
-  }
-
-  if (distance(bestFit) < kNumGpusRequired) {
-    printf(
-        "No Two or more GPUs with same architecture capable of cuda Memory "
-        "Pools found."
-        "\nWaiving the sample\n");
-    exit(EXIT_WAIVED);
-  }
-
-  std::set<int> bestFitDeviceIds;
-
-  // check & select peer-to-peer access capable GPU devices.
-  int devIds[2];
-  for (auto itr = bestFit.first; itr != bestFit.second; itr++) {
-    int deviceId = itr->second;
-    HIPCHECK(hipSetDevice(deviceId));
-
-    std::for_each(itr, bestFit.second, [&deviceId, &bestFitDeviceIds,
-                                        &kNumGpusRequired](
-                                           decltype(*itr) mapPair) {
-      if (deviceId != mapPair.second) {
-        int access = 0;
-        HIPCHECK(
-            hipDeviceCanAccessPeer(&access, deviceId, mapPair.second));
-        printf("Device=%d %s Access Peer Device=%d\n", deviceId,
-               access ? "CAN" : "CANNOT", mapPair.second);
-        if (access && bestFitDeviceIds.size() < kNumGpusRequired) {
-          bestFitDeviceIds.emplace(deviceId);
-          bestFitDeviceIds.emplace(mapPair.second);
-        } else {
-          printf("Ignoring device %i (max devices exceeded)\n", mapPair.second);
-        }
-      }
-    });
-
-    if (bestFitDeviceIds.size() >= kNumGpusRequired) {
-      printf("Selected p2p capable devices - ");
-      int i = 0;
-      for (auto devicesItr = bestFitDeviceIds.begin();
-           devicesItr != bestFitDeviceIds.end(); devicesItr++) {
-        devIds[i++] = *devicesItr;
-        printf("deviceId = %d  ", *devicesItr);
-      }
-      printf("\n");
-      break;
-    }
-  }
-
-  // if bestFitDeviceIds.size() == 0 it means the GPUs in system are not p2p
-  // capable, hence we add it without p2p capability check.
-  if (!bestFitDeviceIds.size()) {
-    printf("No Two or more Devices p2p capable found.. exiting..\n");
-    exit(EXIT_WAIVED);
-  }
-
-  auto p2pGpuPair = std::make_pair(devIds[0], devIds[1]);
-
-  return p2pGpuPair;
-}
-
-int memPoolP2PCopy() {
-  int *dev0_srcVec, *dev1_dstVec;  // Device buffers
-  hipStream_t stream1, stream2;
-  hipMemPool_t memPool;
-  hipEvent_t waitOnStream1;
-
-  // Allocate CPU memory.
-  size_t nelem = 1048576;
-  size_t bytes = nelem * sizeof(int);
-
-  int *a = (int *)malloc(bytes);
-  int *output = (int *)malloc(bytes);
-
-  /* Initialize the vectors. */
-  for (int n = 0; n < nelem; n++) {
-    a[n] = rand() / (int)RAND_MAX;
-  }
-
-  auto p2pDevices = getP2PCapableGpuPair();
-  printf("selected devices = %d & %d\n", p2pDevices.first, p2pDevices.second);
-  HIPCHECK(hipSetDevice(p2pDevices.first));
-  HIPCHECK(hipEventCreate(&waitOnStream1));
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream1, hipStreamNonBlocking));
-
-  // Get the default mempool for device p2pDevices.first from the pair
-  HIPCHECK(hipDeviceGetDefaultMemPool(&memPool, p2pDevices.first));
-
-  // Allocate memory in a stream from the pool set above.
-  HIPCHECK(hipMallocAsync(&dev0_srcVec, bytes, stream1));
-
-  HIPCHECK(
-      hipMemcpyAsync(dev0_srcVec, a, bytes, hipMemcpyHostToDevice, stream1));
-  HIPCHECK(hipEventRecord(waitOnStream1, stream1));
-
-  HIPCHECK(hipSetDevice(p2pDevices.second));
-  HIPCHECK(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
-
-  // Allocate memory in p2pDevices.second device
-  HIPCHECK(hipMallocAsync(&dev1_dstVec, bytes, stream2));
-
-  // Setup peer mappings for p2pDevices.second device
-  hipMemAccessDesc desc;
-  memset(&desc, 0, sizeof(hipMemAccessDesc));
-  desc.location.type = hipMemLocationTypeDevice;
-  desc.location.id = p2pDevices.second;
-  desc.flags = hipMemAccessFlagsProtReadWrite;
-  HIPCHECK(hipMemPoolSetAccess(memPool, &desc, 1));
-
-  printf("> copyP2PAndScale kernel running ...\n");
-  dim3 block(256);
-  dim3 grid((unsigned int)ceil(nelem / (int)block.x));
-  HIPCHECK(hipStreamWaitEvent(stream2, waitOnStream1));
-  copyP2PAndScale<<<grid, block, 0, stream2>>>(dev0_srcVec, dev1_dstVec, nelem);
-
-  HIPCHECK(hipMemcpyAsync(output, dev1_dstVec, bytes,
-                                  hipMemcpyDeviceToHost, stream2));
-  HIPCHECK(hipFreeAsync(dev0_srcVec, stream2));
-  HIPCHECK(hipFreeAsync(dev1_dstVec, stream2));
-  HIPCHECK(hipStreamSynchronize(stream2));
-
-  /* Compare the results */
-  printf("> Checking the results from copyP2PAndScale() ...\n");
-
-  for (int n = 0; n < nelem; n++) {
-    if ((2 * a[n]) != output[n]) {
-      printf("mismatch i = %d expected = %d val = %d\n", n, 2 * a[n],
-             output[n]);
-      return EXIT_FAILURE;
-    }
-  }
-
-  free(a);
-  free(output);
-  HIPCHECK(hipStreamDestroy(stream1));
-  HIPCHECK(hipStreamDestroy(stream2));
-  printf("PASSED\n");
-
-  return EXIT_SUCCESS;
-}
-
-int main(int argc, char **argv) {
-  int ret = memPoolP2PCopy();
-  return ret;
-}
-CudaErrors(hipStreamDestroy(stream2));
-  printf("PASSED\n");
-
-  return EXIT_SUCCESS;
-}
diff --git a/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip b/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip
index 6ba6066..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/StreamPriorities/StreamPriorities.cu.hip
@@ -1,201 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// std::system includes
-#include <cstdio>
-
-// CUDA-C includes
-#include <hip/hip_runtime.h>
-
-#include <helper_cuda.h>
-
-#define TOTAL_SIZE 256 * 1024 * 1024
-#define EACH_SIZE 128 * 1024 * 1024
-
-// # threadblocks
-#define TBLOCKS 1024
-#define THREADS 512
-
-// throw error on equality
-#define ERR_EQ(X, Y)                                                           \
-  do {                                                                         \
-    if ((X) == (Y)) {                                                          \
-      fprintf(stderr, "Error in %s at %s:%d\n", __func__, __FILE__, __LINE__); \
-      exit(-1);                                                                \
-    }                                                                          \
-  } while (0)
-
-// throw error on difference
-#define ERR_NE(X, Y)                                                           \
-  do {                                                                         \
-    if ((X) != (Y)) {                                                          \
-      fprintf(stderr, "Error in %s at %s:%d\n", __func__, __FILE__, __LINE__); \
-      exit(-1);                                                                \
-    }                                                                          \
-  } while (0)
-
-// copy from source -> destination arrays
-__global__ void memcpy_kernel(int *dst, int *src, size_t n) {
-  int num = gridDim.x * blockDim.x;
-  int id = blockDim.x * blockIdx.x + threadIdx.x;
-
-  for (int i = id; i < n / sizeof(int); i += num) {
-    dst[i] = src[i];
-  }
-}
-
-// initialise memory
-void mem_init(int *buf, size_t n) {
-  for (int i = 0; i < n / sizeof(int); i++) {
-    buf[i] = i;
-  }
-}
-
-int main(int argc, char **argv) {
-  hipDeviceProp_t device_prop;
-  int dev_id;
-
-  printf("Starting [%s]...\n", argv[0]);
-
-  // set device
-  dev_id = findCudaDevice(argc, (const char **)argv);
-  HIPCHECK(hipGetDeviceProperties(&device_prop, dev_id));
-
-  if ((device_prop.major << 4) + device_prop.minor < 0x35) {
-    fprintf(stderr,
-            "%s requires Compute Capability of SM 3.5 or higher to "
-            "run.\nexiting...\n",
-            argv[0]);
-    exit(EXIT_WAIVED);
-  }
-
-  // get the range of priorities available
-  // [ greatest_priority, lowest_priority ]
-  int priority_low;
-  int priority_hi;
-  HIPCHECK(
-      hipDeviceGetStreamPriorityRange(&priority_low, &priority_hi));
-
-  printf("CUDA stream priority range: LOW: %d to HIGH: %d\n", priority_low,
-         priority_hi);
-
-  // create streams with highest and lowest available priorities
-  hipStream_t st_low;
-  hipStream_t st_hi;
-  HIPCHECK(hipStreamCreateWithPriority(&st_low, hipStreamNonBlocking,
-                                               priority_low));
-  HIPCHECK(
-      hipStreamCreateWithPriority(&st_hi, hipStreamNonBlocking, priority_hi));
-
-  size_t size;
-  size = TOTAL_SIZE;
-
-  // initialise host data
-  int *h_src_low;
-  int *h_src_hi;
-  ERR_EQ(h_src_low = (int *)malloc(size), NULL);
-  ERR_EQ(h_src_hi = (int *)malloc(size), NULL);
-  mem_init(h_src_low, size);
-  mem_init(h_src_hi, size);
-
-  // initialise device data
-  int *h_dst_low;
-  int *h_dst_hi;
-  ERR_EQ(h_dst_low = (int *)malloc(size), NULL);
-  ERR_EQ(h_dst_hi = (int *)malloc(size), NULL);
-  memset(h_dst_low, 0, size);
-  memset(h_dst_hi, 0, size);
-
-  // copy source data -> device
-  int *d_src_low;
-  int *d_src_hi;
-  HIPCHECK(hipMalloc(&d_src_low, size));
-  HIPCHECK(hipMalloc(&d_src_hi, size));
-  HIPCHECK(
-      hipMemcpy(d_src_low, h_src_low, size, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(d_src_hi, h_src_hi, size, hipMemcpyHostToDevice));
-
-  // allocate memory for memcopy destination
-  int *d_dst_low;
-  int *d_dst_hi;
-  HIPCHECK(hipMalloc(&d_dst_low, size));
-  HIPCHECK(hipMalloc(&d_dst_hi, size));
-
-  // create some events
-  hipEvent_t ev_start_low;
-  hipEvent_t ev_start_hi;
-  hipEvent_t ev_end_low;
-  hipEvent_t ev_end_hi;
-  HIPCHECK(hipEventCreate(&ev_start_low));
-  HIPCHECK(hipEventCreate(&ev_start_hi));
-  HIPCHECK(hipEventCreate(&ev_end_low));
-  HIPCHECK(hipEventCreate(&ev_end_hi));
-
-  /* */
-
-  // call pair of kernels repeatedly (with different priority streams)
-  HIPCHECK(hipEventRecord(ev_start_low, st_low));
-  HIPCHECK(hipEventRecord(ev_start_hi, st_hi));
-
-  for (int i = 0; i < TOTAL_SIZE; i += EACH_SIZE) {
-    int j = i / sizeof(int);
-    memcpy_kernel<<<TBLOCKS, THREADS, 0, st_low>>>(d_dst_low + j, d_src_low + j,
-                                                   EACH_SIZE);
-    memcpy_kernel<<<TBLOCKS, THREADS, 0, st_hi>>>(d_dst_hi + j, d_src_hi + j,
-                                                  EACH_SIZE);
-  }
-
-  HIPCHECK(hipEventRecord(ev_end_low, st_low));
-  HIPCHECK(hipEventRecord(ev_end_hi, st_hi));
-
-  HIPCHECK(hipEventSynchronize(ev_end_low));
-  HIPCHECK(hipEventSynchronize(ev_end_hi));
-
-  /* */
-
-  size = TOTAL_SIZE;
-  HIPCHECK(
-      hipMemcpy(h_dst_low, d_dst_low, size, hipMemcpyDeviceToHost));
-  HIPCHECK(hipMemcpy(h_dst_hi, d_dst_hi, size, hipMemcpyDeviceToHost));
-
-  // check results of kernels
-  ERR_NE(memcmp(h_dst_low, h_src_low, size), 0);
-  ERR_NE(memcmp(h_dst_hi, h_src_hi, size), 0);
-
-  // check timings
-  float ms_low;
-  float ms_hi;
-  HIPCHECK(hipEventElapsedTime(&ms_low, ev_start_low, ev_end_low));
-  HIPCHECK(hipEventElapsedTime(&ms_hi, ev_start_hi, ev_end_hi));
-
-  printf("elapsed time of kernels launched to LOW priority stream: %.3lf ms\n",
-         ms_low);
-  printf("elapsed time of kernels launched to HI  priority stream: %.3lf ms\n",
-         ms_hi);
-
-  exit(EXIT_SUCCESS);
-}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpBezierTessellation/BezierLineCDP.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpBezierTessellation/BezierLineCDP.cu.hip
index e315270..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/cdpBezierTessellation/BezierLineCDP.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpBezierTessellation/BezierLineCDP.cu.hip
@@ -1,211 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <hip/hip_runtime_api.h>
-#include <helper_cuda.h>
-#include <string.h>
-
-__forceinline__ __device__ float2 operator+(float2 a, float2 b) {
-  float2 c;
-  c.x = a.x + b.x;
-  c.y = a.y + b.y;
-  return c;
-}
-
-__forceinline__ __device__ float2 operator-(float2 a, float2 b) {
-  float2 c;
-  c.x = a.x - b.x;
-  c.y = a.y - b.y;
-  return c;
-}
-
-__forceinline__ __device__ float2 operator*(float a, float2 b) {
-  float2 c;
-  c.x = a * b.x;
-  c.y = a * b.y;
-  return c;
-}
-
-__forceinline__ __device__ float length(float2 a) {
-  return sqrtf(a.x * a.x + a.y * a.y);
-}
-
-#define MAX_TESSELLATION 32
-struct BezierLine {
-  float2 CP[3];
-  float2 *vertexPos;
-  int nVertices;
-};
-
-__global__ void computeBezierLinePositions(int lidx, BezierLine *bLines,
-                                           int nTessPoints) {
-  int idx = threadIdx.x + blockDim.x * blockIdx.x;
-
-  if (idx < nTessPoints) {
-    float u = (float)idx / (float)(nTessPoints - 1);
-    float omu = 1.0f - u;
-
-    float B3u[3];
-
-    B3u[0] = omu * omu;
-    B3u[1] = 2.0f * u * omu;
-    B3u[2] = u * u;
-
-    float2 position = {0, 0};
-
-    for (int i = 0; i < 3; i++) {
-      position = position + B3u[i] * bLines[lidx].CP[i];
-    }
-
-    bLines[lidx].vertexPos[idx] = position;
-  }
-}
-
-__global__ void computeBezierLinesCDP(BezierLine *bLines, int nLines) {
-  int lidx = threadIdx.x + blockDim.x * blockIdx.x;
-
-  if (lidx < nLines) {
-    float curvature = length(bLines[lidx].CP[1] -
-                             0.5f * (bLines[lidx].CP[0] + bLines[lidx].CP[2])) /
-                      length(bLines[lidx].CP[2] - bLines[lidx].CP[0]);
-    int nTessPoints = min(max((int)(curvature * 16.0f), 4), MAX_TESSELLATION);
-
-    if (bLines[lidx].vertexPos == NULL) {
-      bLines[lidx].nVertices = nTessPoints;
-      hipMalloc((void **)&bLines[lidx].vertexPos,
-                 nTessPoints * sizeof(float2));
-    }
-
-    computeBezierLinePositions<<<ceilf((float)bLines[lidx].nVertices / 32.0f),
-                                 32>>>(lidx, bLines, bLines[lidx].nVertices);
-  }
-}
-
-__global__ void freeVertexMem(BezierLine *bLines, int nLines) {
-  int lidx = threadIdx.x + blockDim.x * blockIdx.x;
-
-  if (lidx < nLines) hipFree(bLines[lidx].vertexPos);
-}
-
-unsigned int checkCapableSM35Device(int argc, char **argv) {
-  // Get device properties
-  hipDeviceProp_t properties;
-  int device_count = 0, device = -1;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "device")) {
-    device = getCmdLineArgumentInt(argc, (const char **)argv, "device");
-
-    hipDeviceProp_t properties;
-    HIPCHECK(hipGetDeviceProperties(&properties, device));
-
-    if (properties.major > 3 ||
-        (properties.major == 3 && properties.minor >= 5)) {
-      printf("Running on GPU  %d (%s)\n", device, properties.name);
-    } else {
-      printf(
-          "cdpBezierTessellation requires GPU devices with compute SM 3.5 or "
-          "higher.");
-      printf("Current GPU device has compute SM %d.%d. Exiting...\n",
-             properties.major, properties.minor);
-      return EXIT_FAILURE;
-    }
-
-  } else {
-    HIPCHECK(hipGetDeviceCount(&device_count));
-
-    for (int i = 0; i < device_count; ++i) {
-      HIPCHECK(hipGetDeviceProperties(&properties, i));
-
-      if (properties.major > 3 ||
-          (properties.major == 3 && properties.minor >= 5)) {
-        device = i;
-        printf("Running on GPU %d (%s)\n", i, properties.name);
-        break;
-      }
-
-      printf("GPU %d %s does not support CUDA Dynamic Parallelism\n", i,
-             properties.name);
-    }
-  }
-  if (device == -1) {
-    fprintf(stderr,
-            "cdpBezierTessellation requires GPU devices with compute SM 3.5 or "
-            "higher.  Exiting...\n");
-    return EXIT_WAIVED;
-  }
-
-  return EXIT_SUCCESS;
-}
-
-#define N_LINES 256
-#define BLOCK_DIM 64
-int main(int argc, char **argv) {
-  BezierLine *bLines_h = new BezierLine[N_LINES];
-
-  float2 last = {0, 0};
-
-  for (int i = 0; i < N_LINES; i++) {
-    bLines_h[i].CP[0] = last;
-
-    for (int j = 1; j < 3; j++) {
-      bLines_h[i].CP[j].x = (float)rand() / (float)RAND_MAX;
-      bLines_h[i].CP[j].y = (float)rand() / (float)RAND_MAX;
-    }
-
-    last = bLines_h[i].CP[2];
-    bLines_h[i].vertexPos = NULL;
-    bLines_h[i].nVertices = 0;
-  }
-
-  unsigned int sm35Ret = checkCapableSM35Device(argc, argv);
-  if (sm35Ret != EXIT_SUCCESS) {
-    exit(sm35Ret);
-  }
-
-  BezierLine *bLines_d;
-  HIPCHECK(hipMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));
-  HIPCHECK(hipMemcpy(bLines_d, bLines_h, N_LINES * sizeof(BezierLine),
-                             hipMemcpyHostToDevice));
-  printf("Computing Bezier Lines (CUDA Dynamic Parallelism Version) ... ");
-  computeBezierLinesCDP<<<(unsigned int)ceil((float)N_LINES / (float)BLOCK_DIM),
-                          BLOCK_DIM>>>(bLines_d, N_LINES);
-  printf("Done!\n");
-
-  // Do something to draw the lines here
-
-  freeVertexMem<<<(unsigned int)ceil((float)N_LINES / (float)BLOCK_DIM),
-                  BLOCK_DIM>>>(bLines_d, N_LINES);
-  HIPCHECK(hipFree(bLines_d));
-  delete[] bLines_h;
-
-  exit(EXIT_SUCCESS);
-}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
index 0cbf36f..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimplePrint/cdpSimplePrint.cu.hip
@@ -1,174 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-#include <helper_string.h>
-
-#include <cstdio>
-#include <cstdlib>
-#include <iostream>
-
-////////////////////////////////////////////////////////////////////////////////
-// Variable on the GPU used to generate unique identifiers of blocks.
-////////////////////////////////////////////////////////////////////////////////
-__device__ int g_uids = 0;
-
-////////////////////////////////////////////////////////////////////////////////
-// Print a simple message to signal the block which is currently executing.
-////////////////////////////////////////////////////////////////////////////////
-__device__ void print_info(int depth, int thread, int uid, int parent_uid) {
-  if (threadIdx.x == 0) {
-    if (depth == 0)
-      printf("BLOCK %d launched by the host\n", uid);
-    else {
-      char buffer[32];
-
-      for (int i = 0; i < depth; ++i) {
-        buffer[3 * i + 0] = '|';
-        buffer[3 * i + 1] = ' ';
-        buffer[3 * i + 2] = ' ';
-      }
-
-      buffer[3 * depth] = '\0';
-      printf("%sBLOCK %d launched by thread %d of block %d\n", buffer, uid,
-             thread, parent_uid);
-    }
-  }
-
-  __syncthreads();
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// The kernel using CUDA dynamic parallelism.
-//
-// It generates a unique identifier for each block. Prints the information
-// about that block. Finally, if the 'max_depth' has not been reached, the
-// block launches new blocks directly from the GPU.
-////////////////////////////////////////////////////////////////////////////////
-__global__ void cdp_kernel(int max_depth, int depth, int thread,
-                           int parent_uid) {
-  // We create a unique ID per block. Thread 0 does that and shares the value
-  // with the other threads.
-  __shared__ int s_uid;
-
-  if (threadIdx.x == 0) {
-    s_uid = atomicAdd(&g_uids, 1);
-  }
-
-  __syncthreads();
-
-  // We print the ID of the block and information about its parent.
-  print_info(depth, thread, s_uid, parent_uid);
-
-  // We launch new blocks if we haven't reached the max_depth yet.
-  if (++depth >= max_depth) {
-    return;
-  }
-
-  cdp_kernel<<<gridDim.x, blockDim.x>>>(max_depth, depth, threadIdx.x, s_uid);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Main entry point.
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  printf("starting Simple Print (CUDA Dynamic Parallelism)\n");
-
-  // Parse a few command-line arguments.
-  int max_depth = 2;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
-      checkCmdLineFlag(argc, (const char **)argv, "h")) {
-    printf(
-        "Usage: %s depth=<max_depth>\t(where max_depth is a value between 1 "
-        "and 8).\n",
-        argv[0]);
-    exit(EXIT_SUCCESS);
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "depth")) {
-    max_depth = getCmdLineArgumentInt(argc, (const char **)argv, "depth");
-
-    if (max_depth < 1 || max_depth > 8) {
-      printf("depth parameter has to be between 1 and 8\n");
-      exit(EXIT_FAILURE);
-    }
-  }
-
-  // Find/set the device.
-  int device = -1;
-  hipDeviceProp_t deviceProp;
-  device = findCudaDevice(argc, (const char **)argv);
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
-
-  if (!(deviceProp.major > 3 ||
-        (deviceProp.major == 3 && deviceProp.minor >= 5))) {
-    printf("GPU %d - %s  does not support CUDA Dynamic Parallelism\n Exiting.",
-           device, deviceProp.name);
-    exit(EXIT_WAIVED);
-  }
-
-  // Print a message describing what the sample does.
-  printf(
-      "*********************************************************************"
-      "******\n");
-  printf(
-      "The CPU launches 2 blocks of 2 threads each. On the device each thread "
-      "will\n");
-  printf(
-      "launch 2 blocks of 2 threads each. The GPU we will do that "
-      "recursively\n");
-  printf("until it reaches max_depth=%d\n\n", max_depth);
-  printf("In total 2");
-  int num_blocks = 2, sum = 2;
-
-  for (int i = 1; i < max_depth; ++i) {
-    num_blocks *= 4;
-    printf("+%d", num_blocks);
-    sum += num_blocks;
-  }
-
-  printf("=%d blocks are launched!!! (%d from the GPU)\n", sum, sum - 2);
-  printf(
-      "************************************************************************"
-      "***\n\n");
-
-  // We set the recursion limit for CDP to max_depth.
-  cudaDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, max_depth);
-
-  // Launch the kernel from the CPU.
-  printf("Launching cdp_kernel() with CUDA Dynamic Parallelism:\n\n");
-  cdp_kernel<<<2, 2>>>(max_depth, 0, 0, -1);
-  HIPCHECK(hipGetLastError());
-
-  // Finalize.
-  HIPCHECK(hipDeviceSynchronize());
-
-  exit(EXIT_SUCCESS);
-}
diff --git a/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip b/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
index a722796..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cdpSimpleQuicksort/cdpSimpleQuicksort.cu.hip
@@ -1,248 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <iostream>
-#include <cstdio>
-#include <helper_cuda.h>
-#include <helper_string.h>
-
-#define MAX_DEPTH 16
-#define INSERTION_SORT 32
-
-////////////////////////////////////////////////////////////////////////////////
-// Selection sort used when depth gets too big or the number of elements drops
-// below a threshold.
-////////////////////////////////////////////////////////////////////////////////
-__device__ void selection_sort(unsigned int *data, int left, int right) {
-  for (int i = left; i <= right; ++i) {
-    unsigned min_val = data[i];
-    int min_idx = i;
-
-    // Find the smallest value in the range [left, right].
-    for (int j = i + 1; j <= right; ++j) {
-      unsigned val_j = data[j];
-
-      if (val_j < min_val) {
-        min_idx = j;
-        min_val = val_j;
-      }
-    }
-
-    // Swap the values.
-    if (i != min_idx) {
-      data[min_idx] = data[i];
-      data[i] = min_val;
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Very basic quicksort algorithm, recursively launching the next level.
-////////////////////////////////////////////////////////////////////////////////
-__global__ void cdp_simple_quicksort(unsigned int *data, int left, int right,
-                                     int depth) {
-  // If we're too deep or there are few elements left, we use an insertion
-  // sort...
-  if (depth >= MAX_DEPTH || right - left <= INSERTION_SORT) {
-    selection_sort(data, left, right);
-    return;
-  }
-
-  unsigned int *lptr = data + left;
-  unsigned int *rptr = data + right;
-  unsigned int pivot = data[(left + right) / 2];
-
-  // Do the partitioning.
-  while (lptr <= rptr) {
-    // Find the next left- and right-hand values to swap
-    unsigned int lval = *lptr;
-    unsigned int rval = *rptr;
-
-    // Move the left pointer as long as the pointed element is smaller than the
-    // pivot.
-    while (lval < pivot) {
-      lptr++;
-      lval = *lptr;
-    }
-
-    // Move the right pointer as long as the pointed element is larger than the
-    // pivot.
-    while (rval > pivot) {
-      rptr--;
-      rval = *rptr;
-    }
-
-    // If the swap points are valid, do the swap!
-    if (lptr <= rptr) {
-      *lptr++ = rval;
-      *rptr-- = lval;
-    }
-  }
-
-  // Now the recursive part
-  int nright = rptr - data;
-  int nleft = lptr - data;
-
-  // Launch a new block to sort the left part.
-  if (left < (rptr - data)) {
-    hipStream_t s;
-    hipStreamCreateWithFlags(&s, hipStreamNonBlocking);
-    cdp_simple_quicksort<<<1, 1, 0, s>>>(data, left, nright, depth + 1);
-    hipStreamDestroy(s);
-  }
-
-  // Launch a new block to sort the right part.
-  if ((lptr - data) < right) {
-    hipStream_t s1;
-    hipStreamCreateWithFlags(&s1, hipStreamNonBlocking);
-    cdp_simple_quicksort<<<1, 1, 0, s1>>>(data, nleft, right, depth + 1);
-    hipStreamDestroy(s1);
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Call the quicksort kernel from the host.
-////////////////////////////////////////////////////////////////////////////////
-void run_qsort(unsigned int *data, unsigned int nitems) {
-  // Prepare CDP for the max depth 'MAX_DEPTH'.
-  HIPCHECK(cudaDeviceSetLimit(cudaLimitDevRuntimeSyncDepth, MAX_DEPTH));
-
-  // Launch on device
-  int left = 0;
-  int right = nitems - 1;
-  std::cout << "Launching kernel on the GPU" << std::endl;
-  cdp_simple_quicksort<<<1, 1>>>(data, left, right, 0);
-  HIPCHECK(hipDeviceSynchronize());
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Initialize data on the host.
-////////////////////////////////////////////////////////////////////////////////
-void initialize_data(unsigned int *dst, unsigned int nitems) {
-  // Fixed seed for illustration
-  srand(2047);
-
-  // Fill dst with random values
-  for (unsigned i = 0; i < nitems; i++) dst[i] = rand() % nitems;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Verify the results.
-////////////////////////////////////////////////////////////////////////////////
-void check_results(int n, unsigned int *results_d) {
-  unsigned int *results_h = new unsigned[n];
-  HIPCHECK(hipMemcpy(results_h, results_d, n * sizeof(unsigned),
-                             hipMemcpyDeviceToHost));
-
-  for (int i = 1; i < n; ++i)
-    if (results_h[i - 1] > results_h[i]) {
-      std::cout << "Invalid item[" << i - 1 << "]: " << results_h[i - 1]
-                << " greater than " << results_h[i] << std::endl;
-      exit(EXIT_FAILURE);
-    }
-
-  std::cout << "OK" << std::endl;
-  delete[] results_h;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Main entry point.
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  int num_items = 128;
-  bool verbose = false;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
-      checkCmdLineFlag(argc, (const char **)argv, "h")) {
-    std::cerr << "Usage: " << argv[0]
-              << " num_items=<num_items>\twhere num_items is the number of "
-                 "items to sort"
-              << std::endl;
-    exit(EXIT_SUCCESS);
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "v")) {
-    verbose = true;
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "num_items")) {
-    num_items = getCmdLineArgumentInt(argc, (const char **)argv, "num_items");
-
-    if (num_items < 1) {
-      std::cerr << "ERROR: num_items has to be greater than 1" << std::endl;
-      exit(EXIT_FAILURE);
-    }
-  }
-
-  // Find/set device and get device properties
-  int device = -1;
-  hipDeviceProp_t deviceProp;
-  device = findCudaDevice(argc, (const char **)argv);
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
-
-  if (!(deviceProp.major > 3 ||
-        (deviceProp.major == 3 && deviceProp.minor >= 5))) {
-    printf("GPU %d - %s  does not support CUDA Dynamic Parallelism\n Exiting.",
-           device, deviceProp.name);
-    exit(EXIT_WAIVED);
-  }
-
-  // Create input data
-  unsigned int *h_data = 0;
-  unsigned int *d_data = 0;
-
-  // Allocate CPU memory and initialize data.
-  std::cout << "Initializing data:" << std::endl;
-  h_data = (unsigned int *)malloc(num_items * sizeof(unsigned int));
-  initialize_data(h_data, num_items);
-
-  if (verbose) {
-    for (int i = 0; i < num_items; i++)
-      std::cout << "Data [" << i << "]: " << h_data[i] << std::endl;
-  }
-
-  // Allocate GPU memory.
-  HIPCHECK(
-      hipMalloc((void **)&d_data, num_items * sizeof(unsigned int)));
-  HIPCHECK(hipMemcpy(d_data, h_data, num_items * sizeof(unsigned int),
-                             hipMemcpyHostToDevice));
-
-  // Execute
-  std::cout << "Running quicksort on " << num_items << " elements" << std::endl;
-  run_qsort(d_data, num_items);
-
-  // Check result
-  std::cout << "Validating results: ";
-  check_results(num_items, d_data);
-
-  free(h_data);
-  HIPCHECK(hipFree(d_data));
-
-  exit(EXIT_SUCCESS);
-}
diff --git a/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip b/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
index f3b3399..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cudaCompressibleMemory/saxpy.cu.hip
@@ -1,173 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-//
-// This sample uses the compressible memory allocation if device supports it
-// and performs saxpy on it. 
-// Compressible memory may give better performance if the data is amenable to 
-// compression.
-
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <hip/hip_runtime.h>
-#define CUDA_DRIVER_API
-#include "helper_cuda.h"
-#include "compMalloc.h"
-
-__global__ void saxpy(const float a, const float4 *x, const float4 *y, float4 *z, const size_t n)
-{
-    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < n; i += gridDim.x * blockDim.x)
-    {
-        const float4 x4 = x[i];
-        const float4 y4 = y[i];
-        z[i] = make_float4(a * x4.x + y4.x, a * x4.y + y4.y,
-                            a * x4.z + y4.z, a * x4.w + y4.w);
-    }
-}
-
-__global__ void init(float4 *x, float4 *y, const float val, const size_t n)
-{
-    const float4 val4 = make_float4(val, val, val, val);
-    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < n; i += gridDim.x * blockDim.x)
-    {
-        x[i] = y[i] = val4;
-    }
-}
-
-void launchSaxpy(const float a, float4 *x, float4 *y, float4 *z, const size_t n, const float init_val, const bool compressibleZbuf)
-{
-    hipEvent_t start, stop;
-    float ms;
-    int blockSize;
-    int minGridSize;
-    dim3 threads, blocks; 
-
-    if (!compressibleZbuf)
-    {
-        // We are on config where compressible buffer can only be initialized through cudaMemcpy
-        // hence, x & y buffers are allocated as compressible and initialized via cudaMemcpy
-        // whereas z buffer is allocated as non-compressible.
-        float4 *h_x = (float4 *) malloc(sizeof(float4) * n);
-        float4 *h_y = (float4 *) malloc(sizeof(float4) * n);
-        for (int i = 0; i < n; i++)
-        {
-            h_x[i].x = h_x[i].y = h_x[i].z = h_x[i].w = init_val;
-            h_y[i].x = h_y[i].y = h_y[i].z = h_y[i].w = init_val;
-        }
-        HIPCHECK(hipMemcpy(x, h_x, sizeof(float4) * n, hipMemcpyHostToDevice));
-        HIPCHECK(hipMemcpy(y, h_y, sizeof(float4) * n, hipMemcpyHostToDevice));
-        free(h_x);
-        free(h_y);
-    }
-    else
-    {
-        HIPCHECK(hipOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)init));
-        threads = dim3(blockSize, 1, 1);
-        blocks  = dim3(minGridSize, 1, 1);
-        init<<<blocks, threads>>>(x, y, init_val, n);
-    }
-
-    HIPCHECK(hipOccupancyMaxPotentialBlockSize(&minGridSize, &blockSize, (void*)saxpy));
-    threads = dim3(blockSize, 1, 1);
-    blocks  = dim3(minGridSize, 1, 1);
-
-    HIPCHECK(hipEventCreate(&start));
-    HIPCHECK(hipEventCreate(&stop));
-    HIPCHECK(hipEventRecord(start));
-    saxpy<<<blocks, threads>>>(a, x, y, z, n);
-    HIPCHECK(hipEventRecord(stop));
-    HIPCHECK(hipEventSynchronize(stop));
-    HIPCHECK(hipEventElapsedTime(&ms, start, stop));
-
-    const size_t size = n * sizeof(float4);
-    printf("Running saxpy with %d blocks x %d threads = %.3f ms %.3f TB/s\n", blocks.x, threads.x, ms, (size*3)/ms/1e9);
-}
-
-int main(int argc, char **argv)
-{
-    const size_t n = 10485760;
-
-    if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
-            checkCmdLineFlag(argc, (const char **)argv, "?")) {
-        printf("Usage -device=n (n >= 0 for deviceID)\n");
-        exit(EXIT_SUCCESS);
-    }
-
-    findCudaDevice(argc, (const char**)argv);
-    hipDevice_t currentDevice;
-    HIPCHECK(hipCtxGetDevice(&currentDevice));
-
-    // Check that the selected device supports virtual memory management
-    int vmm_supported = -1;
-    HIPCHECK(hipDeviceGetAttribute(&vmm_supported,
-                          CU_DEVICE_ATTRIBUTE_VIRTUAL_ADDRESS_MANAGEMENT_SUPPORTED,
-                          currentDevice));
-    if (vmm_supported == 0) {
-        printf("Device %d doesn't support Virtual Memory Management, waiving the execution.\n", currentDevice);
-        exit(EXIT_WAIVED);
-    }
-
-    int isCompressionAvailable;
-    HIPCHECK(hipDeviceGetAttribute(&isCompressionAvailable,
-                             CU_DEVICE_ATTRIBUTE_GENERIC_COMPRESSION_SUPPORTED,
-                             currentDevice));
-    if (isCompressionAvailable == 0)
-    {
-        printf("Device %d doesn't support Generic memory compression, waiving the execution.\n", currentDevice);
-        exit(EXIT_WAIVED);
-    }
-
-    printf("Generic memory compression support is available\n");
-
-    int major, minor;
-    HIPCHECK(hipDeviceGetAttribute(&major,
-                          hipDeviceAttributeComputeCapabilityMajor,
-                          currentDevice));
-    HIPCHECK(hipDeviceGetAttribute(&minor,
-                          hipDeviceAttributeComputeCapabilityMinor,
-                          currentDevice));
-    float4 *x, *y, *z;
-    const size_t size = n * sizeof(float4);
-
-    // Allocating compressible memory
-    HIPCHECK(allocateCompressible((void **)&x, size, true));
-    HIPCHECK(allocateCompressible((void **)&y, size, true));
-    bool compressibleZbuf = 0;
-    if ((major == 8 && minor == 0) || (major == 8 && minor == 6))
-    {
-        // On SM 8.0 and 8.6 GPUs compressible buffer can only be initialized 
-        // through cudaMemcpy.
-        printf("allocating non-compressible Z buffer\n");
-        HIPCHECK(allocateCompressible((void **)&z, size, false));
-        compressibleZbuf = 0;
-    }
-    else
-    {
-        HIPCHECK(allocateCompressible((void **)&z, size, true));
-        compressibleZbuf = 1;
-    }
diff --git a/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
index 005352e..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/cudaTensorCoreGemm/cudaTensorCoreGemm.cu.hip
@@ -1,650 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// CUDA sample demonstrating a GEMM computation using the Warp Matrix Multiply
-// and Accumulate API introduced in CUDA 9.
-
-// In this program, the compute_gemm kernel computes the result of a matrix
-// multiplication and addition: D = alpha * A * B + beta * C. The dimensions of
-// both C and D matrices are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x
-// K_GLOBAL (row-major), the B matrix is K_GLOBAL x N_GLOBAL (column-major). In
-// that kernel, each CTA computes one 128 x 128 tile of the resulting matrix per
-// iteration. When the tile is computed, the CTA stores it to the global memory
-// and begins a new iteration, selecting a new 128 x 128 tile to compute.
-// Each CTA consists of eight warps. For the 128 x 128 tile, each warp computes
-// eight 16 x 16 subtiles, organized in a 2 x 4 two-dimensional array. Warps
-// compute the 16 x 16 subtiles using nvcuda::wmma::mma_sync operations by
-// moving through the K_GLOBAL dimension of the A and B matrices and
-// accumulating the intermediate result in the local thread state.
-
-// There are a number of simple optimizations used in the algorithm:
-// - The CTA copies the 128 x 128 tile of the C matrix from the global memory to
-//   shared memory. After that is done, each warp loads the C matrix fragments
-//   from shared memory, thus avoiding a random global memory access.
-// - On each internal iteration, the CTA copies a portion of the A and B
-//   matrices from global memory to shared memory. After that, all warps in the
-//   CTA reuse the A and B data from shared memory, thus reducing the number of
-//   data copies from global memory.
-// - The portions of the A and B matrices are stored in shared memory with an
-//   additional padding (skew) to reduce the number of shared memory access bank
-//   conflicts.
-//   (See a detailed explanation near the SKEW_HALF macro definition.)
-// - When the CTA finishes computing the tiles of the resulting matrix, each
-//   warp stores its subtiles to shared memory. The CTA then copies the shared
-//   memory contents to global memory, again avoiding redundant random global
-//   memory  accesses.
-// - Note that the CTA tile size is chosen to maximize the GPU register
-//   utilization, but carefully enough to avoid local memory use.
-
-#include <assert.h>
-#include <hip/hip_runtime.h>
-#include <mma.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-// helper functions and utilities to work with CUDA
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-// Externally configurable parameters.
-
-#ifndef CPU_DEBUG
-// Set this to 1 to verify the correctness of the GPU-computed matrix.
-#define CPU_DEBUG 0
-#endif
-
-#ifndef SHARED_MEMORY_LIMIT_64K
-// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
-// improve the performance of the computations on GPU.
-// Note that you need a GPU that can have more than 64 Kb of shared memory
-// per multiprocessor.
-#define SHARED_MEMORY_LIMIT_64K 1
-#endif
-
-// GPU configuration.
-
-#define WARP_SIZE 32
-
-// MMA matrix tile dimensions.
-
-#define M 16
-#define N 16
-#define K 16
-
-#define WMMA_M 16
-#define WMMA_N 16
-#define WMMA_K 16
-
-// GEMM configuration.
-
-#define M_TILES 256
-#define N_TILES 256
-#define K_TILES 256
-
-#define M_GLOBAL (M * M_TILES)
-#define N_GLOBAL (N * N_TILES)
-#define K_GLOBAL (K * K_TILES)
-
-#define C_LAYOUT wmma::mem_row_major
-
-// Implementation constants.
-
-#define WARPS_PER_BLOCK 8
-#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
-
-#if SHARED_MEMORY_LIMIT_64K
-// With only 64 Kb shared memory available, we can fit two 8-tile chunks of
-// the A and B matrix data, that are 16 * 16 * 8 * 8 * 2 = 32 Kb each
-// (i.e. two 8x8 arrays of tiles of 16x16 half-typed elements per CTA).
-// But we cannot account the 8 Kb total skew overhead, without which the
-// performance would be severely impacted. So we choose to reduce the chunk size
-// in half, i.e. the amount of A and B matrix data we cache in shared memory.
-// Accordingly, this doubles the number of outer iterations across the global K
-// dimension, which only slightly impacts the performance.
-#define CHUNK_K 4
-#else
-#define CHUNK_K 8
-#endif
-
-#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(half))
-#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
-#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
-#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
-
-#define BLOCK_ROW_WARPS 2
-#define BLOCK_COL_WARPS 4
-
-#define WARP_ROW_TILES 4
-#define WARP_COL_TILES 2
-
-#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
-#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
-
-#define GLOBAL_MEM_STRIDE N_GLOBAL
-
-#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
-#define SHMEM_OFFSET (N * WARP_ROW_TILES)
-
-// The macro below is used to shift rows of the A matrix and columns of the B matrix
-// in shared memory to minimize possible bank conflicts.
-// Before performing the nvcuda::wmma::mma_sync operation, the warp must load the matrix
-// data using the nvcuda::wmma::load_matrix_sync operation. Although the memory access pattern
-// is not specified for that function, each lane in the warp can read one or multiple matrix
-// elements from different matrix rows or columns.
-// For shared memory, such access can result in bank conflicts if different rows / columns
-// of the matrix map to the same bank. By shifting each row and column by a few bytes, we
-// make sure that they map to different banks, thus reducing the number of possible bank
-// conflicts.
-// The number of 16 two-byte "half" elements is chosen as the minimum possible shift because
-// we must keep each row and column 256-bit aligned, as required by nvcuda::wmma::load_matrix_sync.
-#define SKEW_HALF 16
-
-#define checkKernelErrors(expr)                             \
-  do {                                                      \
-    expr;                                                   \
-                                                            \
-    hipError_t __err = hipGetLastError();                 \
-    if (__err != hipSuccess) {                             \
-      printf("Line %d: '%s' failed: %s\n", __LINE__, #expr, \
-             hipGetErrorString(__err));                    \
-      abort();                                              \
-    }                                                       \
-  } while (0)
-
-using namespace nvcuda;
-
-__host__ void init_host_matrices(half *a, half *b, float *c) {
-  for (int i = 0; i < M_GLOBAL; i++) {
-    for (int j = 0; j < K_GLOBAL; j++) {
-      a[i * K_GLOBAL + j] = (half)(rand() % 3);
-    }
-  }
-
-  for (int i = 0; i < N_GLOBAL; i++) {
-    for (int j = 0; j < K_GLOBAL; j++) {
-      b[i * K_GLOBAL + j] = (half)(rand() % 3);
-    }
-  }
-
-  for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
-    c[t] = static_cast<float>(rand() % 3);
-  }
-}
-
-__global__ void compute_gemm(const half *A, const half *B, const float *C,
-                             float *D, float alpha, float beta) {
-  extern __shared__ half shmem[][CHUNK_K * K + SKEW_HALF];
-
-  // Warp and lane identification.
-  const unsigned int warpId = threadIdx.x / WARP_SIZE;
-  const unsigned int laneId = threadIdx.x % WARP_SIZE;
-
-  // Offset in shared memory from which the B matrix is stored.
-  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
-
-  // This pointer is used to access the C and D matrix tiles this warp computes.
-  float *shmem_warp_tile_ptr = (float *)&shmem[0][0] +
-                               (warpId / 2) * SHMEM_STRIDE * K * 2 +
-                               (warpId % 2) * SHMEM_OFFSET;
-
-  // This pointer is used to stream the C and D matrices block-wide tile to and
-  // from shared memory.
-  float *shmem_warp_stream_ptr =
-      (float *)&shmem[0][0] + warpId * SHMEM_STRIDE * K;
-
-  // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
-  // each tile computation. Technically this is not generally correct (may
-  // result in a loss of precision). Zero still needs to be specially handled
-  // though.
-  beta /= alpha;
-
-  // Each CTA slides along the 128 x 128 tiles from the top left corner of the
-  // matrix to the right and down, and selects the next tile to compute. Once
-  // there's no such tile, all warps in this CTA exit.
-  for (unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
-    const unsigned int block_tile_i =
-        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
-    const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
-
-    // Stop when there are no more D matrix tiles to compute in this CTA.
-    if (block_tile_i >= M_TILES) {
-      break;
-    }
-
-    // This warp's pointer to the C matrix data to copy memory from to shared
-    // memory.
-    const size_t gmem_idx =
-        (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
-    const float *src_gmem_warp_stream_ptr = &C[gmem_idx];
-
-    // Stream multiple C tiles to shared memory.
-#pragma unroll
-    for (int i = 0; i < K; i++) {
-      typedef int4 copy_t;
-
-      *((copy_t *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) =
-          *((copy_t *)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) +
-            laneId);
-    }
-
-    __syncthreads();
-
-    // These fragments will accumulate the result of A and B matrix fragment
-    // multiplications along the K_GLOBAL dimension.
-    wmma::fragment<wmma::accumulator, M, N, K, float> c[WARP_COL_TILES]
-                                                       [WARP_ROW_TILES];
-
-    // Load the C matrix tiles into fragments from shared memory.
-#pragma unroll
-    for (int i = 0; i < WARP_COL_TILES; i++) {
-#pragma unroll
-      for (int j = 0; j < WARP_ROW_TILES; j++) {
-        const float *tile_ptr =
-            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
-
-        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
-      }
-    }
-
-    __syncthreads();
-
-    // Scale the C matrix.
-#pragma unroll
-    for (int i = 0; i < WARP_COL_TILES; i++) {
-#pragma unroll
-      for (int j = 0; j < WARP_ROW_TILES; j++) {
-#pragma unroll
-        for (int t = 0; t < c[i][j].num_elements; t++) {
-          c[i][j].x[t] *= beta;
-        }
-      }
-    }
-
-    // Select what warp copies what matrix to shared memory.
-    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
-    const half *warp_ptr = (warpId < 4) ? (&A[block_tile_i * M * K_GLOBAL] +
-                                           M * K_GLOBAL * (warpId % 4) * 2)
-                                        : (&B[block_tile_j * N * K_GLOBAL] +
-                                           N * K_GLOBAL * (warpId % 4) * 2);
-
-    // Go through the global K dimension by a fixed step at a time.
-#pragma unroll
-    for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
-      // Copy slices of the A and B matrices to shared memory.
-      // The first half of the warps in the CTA copy the A matrix, the rest copy
-      // the B matrix.
-      size_t shmem_idx =
-          warpId < (WARPS_PER_BLOCK / 2)
-              ? (M * (warpId % (WARPS_PER_BLOCK / 2)) * 2)
-              : (N * (warpId % (WARPS_PER_BLOCK / 2)) * 2 + shmem_idx_b_off);
-
-      // First half of the warp copies the first row / column of the matrix,
-      // the second half of the warp copies the next.
-      int4 *lane_ptr = (int4 *)(warp_ptr + tile_k * K +
-                                (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +
-                       (laneId % CHUNK_COPY_LINE_LANES);
-
-      // Shift the second half of the warp to the next row / column in the
-      // shared memory.
-      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
-
-#pragma unroll
-      for (int i = 0; i < ((WARP_SIZE / 2) / CHUNK_COPY_LINES_PER_WARP) * 2;
-           i++) {
-        // Copy 16 bytes at once in each lane.
-        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =
-            *lane_ptr;
-
-        // Advance the global memory pointer and the shared memory index.
-        lane_ptr =
-            (int4 *)((half *)lane_ptr + K_GLOBAL * CHUNK_COPY_LINES_PER_WARP);
-        shmem_idx += CHUNK_COPY_LINES_PER_WARP;
-      }
-
-      __syncthreads();
-
-      // Compute a grid of C matrix tiles in each warp.
-#pragma unroll
-      for (int k_step = 0; k_step < CHUNK_K; k_step++) {
-        wmma::fragment<wmma::matrix_a, M, N, K, half, wmma::row_major>
-            a[WARP_COL_TILES];
-        wmma::fragment<wmma::matrix_b, M, N, K, half, wmma::col_major>
-            b[WARP_ROW_TILES];
-
-#pragma unroll
-        for (int i = 0; i < WARP_COL_TILES; i++) {
-          size_t shmem_idx_a = (warpId / 2) * M * 2 + (i * M);
-          const half *tile_ptr = &shmem[shmem_idx_a][k_step * K];
-
-          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_HALF);
-
-#pragma unroll
-          for (int j = 0; j < WARP_ROW_TILES; j++) {
-            if (i == 0) {
-              // Load the B matrix fragment once, because it is going to be
-              // reused against the other A matrix fragments.
-              size_t shmem_idx_b = shmem_idx_b_off +
-                                   (WARP_ROW_TILES * N) * (warpId % 2) +
-                                   (j * N);
-              const half *tile_ptr = &shmem[shmem_idx_b][k_step * K];
-
-              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_HALF);
-            }
-
-            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
-          }
-        }
-      }
-
-      __syncthreads();
-    }
-
-      // Store the D fragments to shared memory.
-#pragma unroll
-    for (int i = 0; i < WARP_COL_TILES; i++) {
-#pragma unroll
-      for (int j = 0; j < WARP_ROW_TILES; j++) {
-#pragma unroll
-        // Uniform, point-wise transformations of ALL fragment elements by ALL
-        // threads in the warp are well-defined even though element indices
-        // within fragment storage are not defined.
-        for (int t = 0; t < c[i][j].num_elements; t++) c[i][j].x[t] *= alpha;
-
-        float *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
-
-        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
-      }
-    }
-
-    __syncthreads();
-
-    // Now that shared memory contains all the D tiles, stream them to global
-    // memory.
-    float *dst_gmem_warp_stream_ptr = &D[gmem_idx];
-
-#pragma unroll
-    for (int i = 0; i < K; i++) {
-      *((int4 *)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
-          *((int4 *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
-    }
-
-    __syncthreads();
-  }
-}
-
-// Performs an MxNxK GEMM (C=alpha*A*B + beta*C) assuming:
-//  1) Matrices are packed in memory.
-//  2) M, N and K are multiples of 16.
-//  3) Neither A nor B are transposed.
-// Note: This is a less performant version of the compute_gemm kernel. It is
-// designed for
-//       demonstration purposes only to show the CUDA WMMA API use without
-//       relying on availability of the shared memory.
-__global__ void simple_wmma_gemm(half *a, half *b, float *c, float *d, int m_ld,
-                                 int n_ld, int k_ld, float alpha, float beta) {
-  // Leading dimensions. Packed with no transpositions.
-  int lda = k_ld;
-  int ldb = k_ld;
-  int ldc = n_ld;
-
-  // Tile using a 2D grid
-  int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
-  int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
-
-  // Declare the fragments
-  wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major>
-      a_frag;
-  wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major>
-      b_frag;
-  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;
-  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;
-
-  wmma::fill_fragment(acc_frag, 0.0f);
-
-  // Loop over k
-  for (int i = 0; i < k_ld; i += WMMA_K) {
-    int aCol = i;
-    int aRow = warpM * WMMA_M;
-    int bCol = warpN * N;
-    int bRow = i;
-
-    // Bounds checking
-    if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
-      // Load the inputs
-      wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
-      wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);
-
-      // Perform the matrix multiplication
-      wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
-    }
-  }
-
-  // Load in the current value of c, scale it by beta, and add this our result
-  // scaled by alpha
-  int cCol = warpN * WMMA_N;
-  int cRow = warpM * WMMA_M;
-
-  if (cRow < m_ld && cCol < n_ld) {
-    wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc,
-                           wmma::mem_row_major);
-
-    for (int i = 0; i < c_frag.num_elements; i++) {
-      c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
-    }
-
-    // Store the output
-    wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc,
-                            wmma::mem_row_major);
-  }
-}
-
-__host__ void matMultiplyOnHost(half *A, half *B, float *C, float alpha,
-                                float beta, int numARows, int numAColumns,
-                                int numBRows, int numBColumns, int numCRows,
-                                int numCColumns) {
-  for (int i = 0; i < numCRows; i++) {
-    for (int j = 0; j < numCColumns; j++) {
-      float temp = 0.0;
-
-      for (int k = 0; k < numAColumns; k++) {
-        temp += (float)A[i * numAColumns + k] * (float)B[j * numBRows + k];
-      }
-
-      C[i * numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
-    }
-  }
-}
-
-int main(int argc, char **argv) {
-  printf("Initializing...\n");
-
-  int dev = findCudaDevice(argc, (const char **)argv);
-
-  hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
-
-  // Tensor cores require a GPU of Volta (SM7X) architecture or higher.
-  if (deviceProp.major < 7) {
-    printf(
-        "cudaTensorCoreGemm requires SM 7.0 or higher to use Tensor "
-        "Cores.  Exiting...\n");
-    exit(EXIT_WAIVED);
-  }
-
-  printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
-  printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
-  printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
-
-  half *A_h = NULL;
-  half *B_h = NULL;
-  float *C_h = NULL;
-#if CPU_DEBUG
-  float *result_hD = NULL;
-  float *result_host = NULL;
-#endif
-
-  A_h = (half *)malloc(sizeof(half) * M_GLOBAL * K_GLOBAL);
-  B_h = (half *)malloc(sizeof(half) * K_GLOBAL * N_GLOBAL);
-  C_h = (float *)malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
-#if CPU_DEBUG
-  result_hD = (float *)malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
-  result_host = (float *)malloc(sizeof(float) * M_GLOBAL * N_GLOBAL);
-#endif
-
-  half *A = NULL;
-  half *B = NULL;
-  float *C = NULL;
-  float *D = NULL;
-
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&A),
-                             sizeof(half) * M_GLOBAL * K_GLOBAL));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&B),
-                             sizeof(half) * N_GLOBAL * K_GLOBAL));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&C),
-                             sizeof(float) * M_GLOBAL * N_GLOBAL));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&D),
-                             sizeof(float) * M_GLOBAL * N_GLOBAL));
-
-  assert(((unsigned long long)A) % 128 == 0);
-  assert(((unsigned long long)B) % 128 == 0);
-  assert(((unsigned long long)C) % 128 == 0);
-  assert(((unsigned long long)D) % 128 == 0);
-
-  init_host_matrices(A_h, B_h, C_h);
-
-  printf("Preparing data for GPU...\n");
-
-  HIPCHECK(hipMemcpy(A, A_h, sizeof(half) * M_GLOBAL * K_GLOBAL,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(B, B_h, sizeof(half) * N_GLOBAL * K_GLOBAL,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(C, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemset(D, 0, sizeof(float) * M_GLOBAL * N_GLOBAL));
-
-  enum {
-    // Compute the right amount of shared memory to request.
-    // We need shared memory to hold per-CTA C and D matrix tiles, and to cache
-    // per-CTA chunks
-    // of the A and B matrices. Therefore, the right amount to request is the
-    // maximum of those
-    // two numbers.
-    SHMEM_SZ = MAX(
-        sizeof(half) * (BLOCK_COL_TILES * M) * (CHUNK_K * K + SKEW_HALF) * 2,
-        M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *
-            (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(float))
-  };
-
-  printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
-
-  const float alpha = 1.1f;
-  const float beta = 1.2f;
-
-  hipEvent_t start, stop;
-
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-  HIPCHECK(hipEventRecord(start));
-
-  // If enough shared memory available on the GPU use high performant kernel
-  if (deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) {
-    printf("Computing... using high performance kernel compute_gemm \n");
-
-    HIPCHECK(hipFuncSetAttribute(
-        compute_gemm, hipFuncAttributeMaxDynamicSharedMemorySize, SHMEM_SZ));
-    checkKernelErrors(
-        (compute_gemm<<<deviceProp.multiProcessorCount, THREADS_PER_BLOCK,
-                        SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
-#if CPU_DEBUG
-    HIPCHECK(hipMemcpy(result_hD, D,
-                               sizeof(float) * M_GLOBAL * N_GLOBAL,
-                               hipMemcpyDeviceToHost));
-#endif
-  } else {
-    dim3 gridDim;
-    dim3 blockDim;
-
-    // blockDim.x must be a multple of warpSize
-    // 128x4 means we have 16 warps and a block computes a 64x64 output tile
-    blockDim.x = 128;
-    blockDim.y = 4;
-
-    gridDim.x = (M_GLOBAL + (WMMA_M * blockDim.x / 32 - 1)) /
-                (WMMA_M * blockDim.x / 32);
-    gridDim.y = (N_GLOBAL + WMMA_N * blockDim.y - 1) / (WMMA_N * blockDim.y);
-
-    printf("Computing... using simple_wmma_gemm kernel\n");
-    simple_wmma_gemm<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL,
-                                            K_GLOBAL, alpha, beta);
-#if CPU_DEBUG
-    HIPCHECK(hipMemcpy(result_hD, D,
-                               sizeof(float) * M_GLOBAL * N_GLOBAL,
-                               hipMemcpyDeviceToHost));
-#endif
-  }
-
-  HIPCHECK(hipEventRecord(stop));
-  HIPCHECK(hipEventSynchronize(stop));
-
-#if CPU_DEBUG
-  printf("Verifying correctness of the computations...\n");
-
-  memcpy(result_host, C_h, sizeof(float) * M_GLOBAL * N_GLOBAL);
-
-  matMultiplyOnHost(A_h, B_h, result_host, alpha, beta, M_GLOBAL, K_GLOBAL,
-                    K_GLOBAL, N_GLOBAL, M_GLOBAL, N_GLOBAL);
-
-  for (int i = 0; i < N_GLOBAL * M_GLOBAL; i++) {
-    if (fabs(result_hD[i] - result_host[i]) > 0.1f)
-      printf("mismatch i=%d result_hD=%f result_host=%f\n", i, result_hD[i],
-             result_host[i]);
-  }
-  free(result_hD);
-  free(result_host);
-#endif
-
-  float milliseconds = 0;
-
-  HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
-
-  printf("Time: %f ms\n", milliseconds);
-  printf("TFLOPS: %.2f\n", static_cast<double>((static_cast<double>(M_GLOBAL) *
-                                                N_GLOBAL * K_GLOBAL * 2) /
-                                               (milliseconds / 1000.)) /
-                               1e12);
-
-  free(A_h);
-  free(B_h);
-  free(C_h);
-  HIPCHECK(hipFree(reinterpret_cast<void *>(A)));
-  HIPCHECK(hipFree(reinterpret_cast<void *>(B)));
-  HIPCHECK(hipFree(reinterpret_cast<void *>(C)));
-  HIPCHECK(hipFree(reinterpret_cast<void *>(D)));
-
-  return 0;
-}
diff --git a/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip b/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip
index b5bebaf..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/graphMemoryFootprint/graphMemoryFootprint.cu.hip
@@ -1,411 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// System includes
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-// helper functions and utilities to work with CUDA
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-#define NUM_GRAPHS 8
-#define THREADS_PER_BLOCK 512
-
-void printMemoryFootprint(int device) {
-  size_t footprint;
-  HIPCHECK(cudaDeviceGetGraphMemAttribute(
-      device, (cudaGraphMemAttributeType)0, &footprint));
-  printf("    FOOTPRINT: %lu bytes\n", footprint);
-}
-
-void prepareAllocParams(cudaMemAllocNodeParams *allocParams, size_t bytes,
-                        int device) {
-  memset(allocParams, 0, sizeof(*allocParams));
-
-  allocParams->bytesize = bytes;
-  allocParams->poolProps.allocType = hipMemAllocationTypePinned;
-  allocParams->poolProps.location.id = device;
-  allocParams->poolProps.location.type = hipMemLocationTypeDevice;
-}
-
-void createVirtAddrReuseGraph(hipGraphExec_t *graphExec, size_t bytes,
-                              int device) {
-  hipGraph_t graph;
-  hipGraphNode_t allocNodeA, allocNodeB, freeNodeA, freeNodeB;
-  cudaMemAllocNodeParams allocParams;
-  float *d_a, *d_b;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-  prepareAllocParams(&allocParams, bytes, device);
-
-  HIPCHECK(
-      cudaGraphAddMemAllocNode(&allocNodeA, graph, NULL, 0, &allocParams));
-  d_a = (float *)allocParams.dptr;
-  HIPCHECK(
-      cudaGraphAddMemFreeNode(&freeNodeA, graph, &allocNodeA, 1, (void *)d_a));
-
-  // The dependency between the allocation of d_b and the free of d_a allows d_b
-  // to reuse the same VA.
-  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeB, graph, &freeNodeA, 1,
-                                           &allocParams));
-  d_b = (float *)allocParams.dptr;
-
-  if (d_a == d_b) {
-    printf("Check confirms that d_a and d_b share a virtual address.\n");
-  } else {
-    printf("Check shows that d_a and d_b DO NOT share a virtual address.\n");
-  }
-
-  HIPCHECK(
-      cudaGraphAddMemFreeNode(&freeNodeB, graph, &allocNodeB, 1, (void *)d_b));
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-void virtualAddressReuseSingleGraph(size_t bytes, int device) {
-  hipStream_t stream;
-  hipGraphExec_t graphExec;
-
-  printf("================================\n");
-  printf("Running virtual address reuse example.\n");
-  printf(
-      "Sequential allocations & frees within a single graph enable CUDA to "
-      "reuse virtual addresses.\n\n");
-
-  createVirtAddrReuseGraph(&graphExec, bytes, device);
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  HIPCHECK(hipGraphLaunch(graphExec, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-  printMemoryFootprint(device);
-
-  HIPCHECK(hipGraphExecDestroy(graphExec));
-  HIPCHECK(hipStreamDestroy(stream));
-}
-
-// This is a kernel that does no real work but runs at least for a specified
-// number of clocks
-__global__ void clockBlock(clock_t clock_count) {
-  unsigned int start_clock = (unsigned int)clock();
-
-  clock_t clock_offset = 0;
-
-  while (clock_offset < clock_count) {
-    unsigned int end_clock = (unsigned int)clock();
-
-    // The code below should work like
-    // this (thanks to modular arithmetics):
-    //
-    // clock_offset = (clock_t) (end_clock > start_clock ?
-    //                           end_clock - start_clock :
-    //                           end_clock + (0xffffffffu - start_clock));
-    //
-    // Indeed, let m = 2^32 then
-    // end - start = end + m - start (mod m).
-
-    clock_offset = (clock_t)(end_clock - start_clock);
-  }
-}
-
-// A pointer to the allocated device buffer is returned in dPtr so the caller
-// can compare virtual addresses. The kernel node is added to increase the
-// graph's runtime.
-void createSimpleAllocFreeGraph(hipGraphExec_t *graphExec, float **dPtr,
-                                size_t bytes, int device) {
-  hipGraph_t graph;
-  hipGraphNode_t allocNodeA, freeNodeA, blockDeviceNode;
-  cudaMemAllocNodeParams allocParams;
-  hipKernelNodeParams blockDeviceNodeParams = {0};
-  int numElements = bytes / sizeof(float);
-  float kernelTime = 5;  // time for each thread to run in microseconds
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-  prepareAllocParams(&allocParams, bytes, device);
-
-  HIPCHECK(
-      cudaGraphAddMemAllocNode(&allocNodeA, graph, NULL, 0, &allocParams));
-  *dPtr = (float *)allocParams.dptr;
-
-  hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, device));
-  clock_t time_clocks = (clock_t)((kernelTime / 1000.0) * deviceProp.clockRate);
-
-  void *blockDeviceArgs[1] = {(void *)&time_clocks};
-
-  size_t numBlocks = numElements / (size_t)THREADS_PER_BLOCK;
-  blockDeviceNodeParams.gridDim = dim3(numBlocks, 1, 1);
-  blockDeviceNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
-  blockDeviceNodeParams.sharedMemBytes = 0;
-  blockDeviceNodeParams.extra = NULL;
-  blockDeviceNodeParams.func = (void *)clockBlock;
-  blockDeviceNodeParams.kernelParams = (void **)blockDeviceArgs;
-  HIPCHECK(hipGraphAddKernelNode(&blockDeviceNode, graph, &allocNodeA,
-                                         1, &blockDeviceNodeParams));
-
-  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeA, graph, &blockDeviceNode,
-                                          1, (void *)*dPtr));
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-void physicalMemoryReuseSingleStream(size_t bytes, int device) {
-  hipStream_t stream;
-  hipGraphExec_t graphExecs[NUM_GRAPHS];
-  float *dPtrs[NUM_GRAPHS];
-  bool virtualAddrDiffer = true;
-
-  printf("================================\n");
-  printf("Running physical memory reuse example.\n");
-  printf(
-      "CUDA reuses the same physical memory for allocations from separate "
-      "graphs when the allocation lifetimes don't overlap.\n\n");
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    createSimpleAllocFreeGraph(&graphExecs[i], &dPtrs[i], bytes, device);
-  }
-
-  printf("Creating the graph execs does not reserve any physical memory.\n");
-  printMemoryFootprint(device);
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  HIPCHECK(hipGraphLaunch(graphExecs[0], stream));
-  printf("\nThe first graph launched reserves the memory it needs.\n");
-  printMemoryFootprint(device);
-
-  HIPCHECK(hipGraphLaunch(graphExecs[0], stream));
-  printf(
-      "A subsequent launch of the same graph in the same stream reuses the "
-      "same physical memory. ");
-  printf("Thus the memory footprint does not grow here.\n");
-  printMemoryFootprint(device);
-
-  printf(
-      "\nSubsequent launches of other graphs in the same stream also reuse the "
-      "physical memory. ");
-  printf("Thus the memory footprint does not grow here.\n");
-  for (int i = 1; i < NUM_GRAPHS; i++) {
-    HIPCHECK(hipGraphLaunch(graphExecs[i], stream));
-    printf("%02d: ", i);
-    printMemoryFootprint(device);
-  }
-
-  HIPCHECK(hipStreamSynchronize(stream));
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    for (int j = i + 1; j < NUM_GRAPHS; j++) {
-      if (dPtrs[i] == dPtrs[j]) {
-        virtualAddrDiffer = false;
-        printf("Error: Graph exec %d and %d have the same virtual address!\n",
-               i - 1, i);
-      }
-    }
-    HIPCHECK(hipGraphExecDestroy(graphExecs[i]));
-  }
-  if (virtualAddrDiffer) {
-    printf("\nCheck confirms all graphs use a different virtual address.\n");
-  } else {
-    printf(
-        "\nAll graphs do NOT use different virtual addresses. Exiting test.\n");
-    exit(EXIT_FAILURE);
-  }
-
-  HIPCHECK(hipStreamDestroy(stream));
-}
-
-void simultaneousStreams(size_t bytes, int device) {
-  hipStream_t streams[NUM_GRAPHS];
-  hipGraphExec_t graphExecs[NUM_GRAPHS];
-  float *dPtrs[NUM_GRAPHS];
-
-  printf("================================\n");
-  printf("Running simultaneous streams example.\n");
-  printf("Graphs that can run concurrently need separate physical memory. ");
-  printf(
-      "In this example, each graph launched in a separate stream increases the "
-      "total memory footprint.\n\n");
-
-  printf(
-      "When launching a new graph, CUDA may reuse physical memory from a graph "
-      "whose execution has already ");
-  printf(
-      "finished -- even if the new graph is being launched in a different "
-      "stream from the completed graph. ");
-  printf(
-      "Therefore, a kernel node is added to the graphs to increase "
-      "runtime.\n\n");
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    createSimpleAllocFreeGraph(&graphExecs[i], &dPtrs[i], bytes, device);
-    HIPCHECK(
-        hipStreamCreateWithFlags(&streams[i], hipStreamNonBlocking));
-  }
-
-  printf("Initial footprint:\n");
-  printMemoryFootprint(device);
-
-  printf(
-      "\nEach graph launch in a seperate stream grows the memory footprint:\n");
-  for (int i = 1; i < NUM_GRAPHS; i++) {
-    HIPCHECK(hipGraphLaunch(graphExecs[i], streams[i]));
-    printf("%02d: ", i);
-    printMemoryFootprint(device);
-  }
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    HIPCHECK(hipStreamSynchronize(streams[i]));
-    HIPCHECK(hipGraphExecDestroy(graphExecs[i]));
-    HIPCHECK(hipStreamDestroy(streams[i]));
-  }
-}
-
-void createSimpleAllocNoFreeGraph(hipGraphExec_t *graphExec, float **dPtr,
-                                  size_t bytes, int device) {
-  hipGraph_t graph;
-  hipGraphNode_t allocNodeA;
-  cudaMemAllocNodeParams allocParams;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-  prepareAllocParams(&allocParams, bytes, device);
-
-  HIPCHECK(
-      cudaGraphAddMemAllocNode(&allocNodeA, graph, NULL, 0, &allocParams));
-  *dPtr = (float *)allocParams.dptr;
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-void unfreedAllocations(size_t bytes, int device) {
-  hipStream_t stream;
-  hipGraphExec_t graphExecs[NUM_GRAPHS];
-  float *dPtrs[NUM_GRAPHS];
-
-  printf("================================\n");
-  printf("Running unfreed streams example.\n");
-  printf(
-      "CUDA cannot reuse phyiscal memory from graphs which do not free their "
-      "allocations.\n\n");
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    createSimpleAllocNoFreeGraph(&graphExecs[i], &dPtrs[i], bytes, device);
-  }
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  printf(
-      "Despite being launched in the same stream, each graph launch grows the "
-      "memory footprint. ");
-  printf(
-      "Since the allocation is not freed, CUDA keeps the memory valid for "
-      "use.\n");
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    HIPCHECK(hipGraphLaunch(graphExecs[i], stream));
-    printf("%02d: ", i);
-    printMemoryFootprint(device);
-  }
-
-  HIPCHECK(hipStreamSynchronize(stream));
-
-  HIPCHECK(cudaDeviceGraphMemTrim(device));
-  printf(
-      "\nTrimming does not impact the memory footprint since the un-freed "
-      "allocations are still holding onto the memory.\n");
-  printMemoryFootprint(device);
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    HIPCHECK(hipFree(dPtrs[i]));
-  }
-  printf("\nFreeing the allocations does not shrink the footprint.\n");
-  printMemoryFootprint(device);
-
-  HIPCHECK(cudaDeviceGraphMemTrim(device));
-  printf(
-      "\nSince the allocations are now freed, trimming does reduce the "
-      "footprint even when the graph execs are not yet destroyed.\n");
-  printMemoryFootprint(device);
-
-  for (int i = 0; i < NUM_GRAPHS; i++) {
-    HIPCHECK(hipGraphExecDestroy(graphExecs[i]));
-  }
-  HIPCHECK(hipStreamDestroy(stream));
-}
-
-void cleanupMemory(int device) {
-  HIPCHECK(cudaDeviceGraphMemTrim(device));
-  printf("\nCleaning up example by trimming device memory.\n");
-  printMemoryFootprint(device);
-  printf("\n");
-}
-
-int main(int argc, char **argv) {
-  size_t bytes = 64 * 1024 * 1024;
-  int device = findCudaDevice(argc, (const char **)argv);
-
-  int driverVersion = 0;
-  int deviceSupportsMemoryPools = 0;
-
-  hipDriverGetVersion(&driverVersion);
-  printf("Driver version is: %d.%d\n", driverVersion / 1000,
-         (driverVersion % 100) / 10);
-
-  if (driverVersion < 11040) {
-    printf("Waiving execution as driver does not support Graph Memory Nodes\n");
-    exit(EXIT_WAIVED);
-  }
-
-  hipDeviceGetAttribute(&deviceSupportsMemoryPools,
-                         hipDeviceAttributeMemoryPoolsSupported, device);
-  if (!deviceSupportsMemoryPools) {
-    printf("Waiving execution as device does not support Memory Pools\n");
-    exit(EXIT_WAIVED);
-  } else {
-    printf("Running sample.\n");
-  }
-
-  virtualAddressReuseSingleGraph(bytes, device);
-  cleanupMemory(device);
-
-  physicalMemoryReuseSingleStream(bytes, device);
-  cleanupMemory(device);
-
-  simultaneousStreams(bytes, device);
-  cleanupMemory(device);
-
-  unfreedAllocations(bytes, device);
-  cleanupMemory(device);
-
-  printf("================================\n");
-  printf("Sample complete.\n");
-}
diff --git a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
index 3ac88ed..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/graphMemoryNodes/graphMemoryNodes.cu.hip
@@ -1,468 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// System includes
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-#include <climits>
-#include <vector>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-#define THREADS_PER_BLOCK 512
-#define ALLOWABLE_VARIANCE 1.e-6f
-#define NUM_ELEMENTS 8000000
-
-// Stores the square of each input element in output array
-__global__ void squareArray(const float *input, float *output,
-                            int numElements) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < numElements) {
-    output[idx] = input[idx] * input[idx];
-  }
-}
-
-// Stores the negative of each input element in output array
-__global__ void negateArray(const float *input, float *output,
-                            int numElements) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (idx < numElements) {
-    output[idx] = input[idx] * -1;
-  }
-}
-
-struct negSquareArrays {
-  float *input;
-  float *square;
-  float *negSquare;
-  int numElements;
-  size_t bytes;
-  size_t numBlocks;
-};
-
-void fillRandomly(float *array, int numElements) {
-  for (int n = 0; n < numElements; n++) {
-    array[n] = rand() / (float)RAND_MAX;
-  }
-}
-
-void resetOutputArrays(negSquareArrays *hostArrays) {
-  fillRandomly(hostArrays->square, hostArrays->numElements);
-  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
-}
-
-void prepareHostArrays(negSquareArrays *hostArrays) {
-  hostArrays->numElements = NUM_ELEMENTS;
-  size_t bytes = hostArrays->numElements * sizeof(float);
-
-  size_t numBlocks = hostArrays->numElements / (size_t)THREADS_PER_BLOCK;
-  if ((numBlocks % (size_t)THREADS_PER_BLOCK) != 0) {
-    numBlocks++;
-  }
-
-  hostArrays->input = (float *)malloc(bytes);
-  hostArrays->square = (float *)malloc(bytes);
-  hostArrays->negSquare = (float *)malloc(bytes);
-  hostArrays->bytes = bytes;
-  hostArrays->numBlocks = numBlocks;
-
-  fillRandomly(hostArrays->input, hostArrays->numElements);
-  fillRandomly(hostArrays->square, hostArrays->numElements);
-  fillRandomly(hostArrays->negSquare, hostArrays->numElements);
-}
-
-void createFreeGraph(hipGraphExec_t *graphExec, float *dPtr) {
-  hipGraph_t graph;
-  hipGraphNode_t freeNode;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-
-  HIPCHECK(
-      cudaGraphAddMemFreeNode(&freeNode, graph, NULL, 0, (void *)dPtr));
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-/**
- * Demonstrates explicitly creating a CUDA graph including memory nodes.
- * createNegateSquaresGraphWithStreamCapture constructs an equivalent graph
- * using stream capture.
- *
- * If d_negSquare_out is non null, then:
- * 1) d_negSquare will not be freed;
- * 2) the value of d_negSquare_out will be set to d_negSquare.
- *
- * Diagram of the graph constructed by createNegateSquaresGraphExplicitly:
- *
- * alloc d_input
- *       |
- * alloc d_square
- *       |
- * Memcpy a to device
- *       |
- * launch kernel squareArray ------->---- Memcpy d_square to host
- *       |                                      |
- * free d_input                                 |
- *       |                                      |
- * allocate d_negSquare                         |
- *       |                                      |
- * launch kernel negateArray -------->--- free d_square
- *       |
- * Memcpy d_negSquare to host
- *       |
- * free d_negSquare
- */
-void createNegateSquaresGraphExplicitly(hipGraphExec_t *graphExec, int device,
-                                        negSquareArrays *hostArrays,
-                                        float **d_negSquare_out = NULL) {
-  // Array buffers on device
-  float *d_input, *d_square, *d_negSquare;
-
-  // Memory allocation parameters
-  cudaMemAllocNodeParams allocParams;
-  memset(&allocParams, 0, sizeof(allocParams));
-  allocParams.bytesize = hostArrays->bytes;
-  allocParams.poolProps.allocType = hipMemAllocationTypePinned;
-  allocParams.poolProps.location.id = device;
-  allocParams.poolProps.location.type = hipMemLocationTypeDevice;
-
-  // Kernel launch parameters
-  hipKernelNodeParams kernelNodeParams = {0};
-  kernelNodeParams.gridDim = dim3(hostArrays->numBlocks, 1, 1);
-  kernelNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
-  kernelNodeParams.sharedMemBytes = 0;
-  kernelNodeParams.extra = NULL;
-
-  hipGraph_t graph;
-  hipGraphNode_t allocNodeInput, allocNodeSquare, allocNodeNegSquare;
-  hipGraphNode_t copyNodeInput, copyNodeSquare, copyNodeNegSquare;
-  hipGraphNode_t squareKernelNode, negateKernelNode;
-  hipGraphNode_t freeNodeInput, freeNodeSquare;
-
-  // Buffer for storing graph node dependencies
-  std::vector<hipGraphNode_t> nodeDependencies;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-
-  HIPCHECK(
-      cudaGraphAddMemAllocNode(&allocNodeInput, graph, NULL, 0, &allocParams));
-  d_input = (float *)allocParams.dptr;
-
-  // To keep the graph structure simple (fewer branching dependencies),
-  // allocNodeSquare should depend on allocNodeInput
-  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeSquare, graph,
-                                           &allocNodeInput, 1, &allocParams));
-  d_square = (float *)allocParams.dptr;
-
-  // copyNodeInput needs to depend on allocNodeInput because copyNodeInput
-  // writes to d_input. It does so here indirectly through allocNodeSquare.
-  HIPCHECK(hipGraphAddMemcpyNode1D(
-      &copyNodeInput, graph, &allocNodeSquare, 1, d_input, hostArrays->input,
-      hostArrays->bytes, hipMemcpyHostToDevice));
-
-  void *squareKernelArgs[3] = {(void *)&d_input, (void *)&d_square,
-                               (void *)&(hostArrays->numElements)};
-  kernelNodeParams.func = (void *)squareArray;
-  kernelNodeParams.kernelParams = (void **)squareKernelArgs;
-
-  // Square kernel depends on copyNodeInput to ensure all data is on the device
-  // before kernel launch.
-  HIPCHECK(hipGraphAddKernelNode(&squareKernelNode, graph,
-                                         &copyNodeInput, 1, &kernelNodeParams));
-
-  HIPCHECK(hipGraphAddMemcpyNode1D(
-      &copyNodeSquare, graph, &squareKernelNode, 1, hostArrays->square,
-      d_square, hostArrays->bytes, hipMemcpyDeviceToHost));
-
-  // Free of d_input depends on the square kernel to ensure that d_input is not
-  // freed while being read by the kernel. It also depends on the alloc of
-  // d_input via squareKernelNode > copyNodeInput > allocNodeSquare >
-  // allocNodeInput.
-  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeInput, graph,
-                                          &squareKernelNode, 1, d_input));
-
-  // Allocation of C depends on free of A so CUDA can reuse the virtual address.
-  HIPCHECK(cudaGraphAddMemAllocNode(&allocNodeNegSquare, graph,
-                                           &freeNodeInput, 1, &allocParams));
-  d_negSquare = (float *)allocParams.dptr;
-
-  if (d_negSquare == d_input) {
-    printf(
-        "Check verified that d_negSquare and d_input share a virtual "
-        "address.\n");
-  }
-
-  void *negateKernelArgs[3] = {(void *)&d_square, (void *)&d_negSquare,
-                               (void *)&(hostArrays->numElements)};
-  kernelNodeParams.func = (void *)negateArray;
-  kernelNodeParams.kernelParams = (void **)negateKernelArgs;
-
-  HIPCHECK(hipGraphAddKernelNode(
-      &negateKernelNode, graph, &allocNodeNegSquare, 1, &kernelNodeParams));
-
-  nodeDependencies.push_back(copyNodeSquare);
-  nodeDependencies.push_back(negateKernelNode);
-  HIPCHECK(cudaGraphAddMemFreeNode(&freeNodeSquare, graph,
-                                          nodeDependencies.data(),
-                                          nodeDependencies.size(), d_square));
-  nodeDependencies.clear();
-
-  HIPCHECK(hipGraphAddMemcpyNode1D(
-      &copyNodeNegSquare, graph, &negateKernelNode, 1, hostArrays->negSquare,
-      d_negSquare, hostArrays->bytes, hipMemcpyDeviceToHost));
-
-  if (d_negSquare_out == NULL) {
-    hipGraphNode_t freeNodeNegSquare;
-    HIPCHECK(cudaGraphAddMemFreeNode(
-        &freeNodeNegSquare, graph, &copyNodeNegSquare, 1, d_negSquare));
-  } else {
-    *d_negSquare_out = d_negSquare;
-  }
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-/**
- * Adds work to a CUDA stream which negates the square of values in the input
- * array.
- *
- * If d_negSquare_out is non null, then:
- * 1) d_negSquare will not be freed;
- * 2) the value of d_negSquare_out will be set to d_negSquare.
- *
- * Diagram of the stream operations in doNegateSquaresInStream
- * ---------------------------------------------------------------------
- * | STREAM                             | STREAM2                      |
- * ---------------------------------------------------------------------
- *
- * alloc d_input
- *       |
- * alloc d_square
- *       |
- * Memcpy a to device
- *       |
- * launch kernel squareArray
- *       |
- * record squareKernelCompleteEvent -->-- wait squareKernelCompleteEvent
- *       |                                      |
- * free d_input                                 |
- *       |                                      |
- * allocate d_negSquare                   Memcpy d_square to host
- *       |                                      |
- * launch kernel negateArray                    |
- *       |                                      |
- * record negateKernelCompleteEvent -->-- wait negateKernelCompleteEvent
- *       |                                      |
- * Memcpy d_negSquare to host                   |
- *       |                                free d_square
- * free d_negSquare                             |
- *       |                                      |
- * wait squareFreeEvent --------------<---- record squareFreeEvent
- */
-void doNegateSquaresInStream(hipStream_t stream1, negSquareArrays *hostArrays,
-                             float **d_negSquare_out = NULL) {
-  float *d_input, *d_square, *d_negSquare;
-  hipStream_t stream2;
-  hipEvent_t squareKernelCompleteEvent, negateKernelCompleteEvent,
-      squareFreeEvent;
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream2, hipStreamNonBlocking));
-
-  HIPCHECK(hipEventCreate(&squareKernelCompleteEvent));
-  HIPCHECK(hipEventCreate(&negateKernelCompleteEvent));
-  HIPCHECK(hipEventCreate(&squareFreeEvent));
-
-  // Virtual addresses are assigned synchronously when cudaMallocAsync is
-  // called, thus there is no performace benefit gained by separating the
-  // allocations into two streams.
-  HIPCHECK(hipMallocAsync(&d_input, hostArrays->bytes, stream1));
-  HIPCHECK(hipMallocAsync(&d_square, hostArrays->bytes, stream1));
-
-  HIPCHECK(hipMemcpyAsync(d_input, hostArrays->input, hostArrays->bytes,
-                                  hipMemcpyHostToDevice, stream1));
-  squareArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
-      d_input, d_square, hostArrays->numElements);
-  HIPCHECK(hipEventRecord(squareKernelCompleteEvent, stream1));
-
-  HIPCHECK(hipStreamWaitEvent(stream2, squareKernelCompleteEvent, 0));
-  HIPCHECK(hipMemcpyAsync(hostArrays->square, d_square,
-                                  hostArrays->bytes, hipMemcpyDeviceToHost,
-                                  stream2));
-
-  HIPCHECK(hipFreeAsync(d_input, stream1));
-  HIPCHECK(hipMallocAsync(&d_negSquare, hostArrays->bytes, stream1));
-  negateArray<<<hostArrays->numBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
-      d_square, d_negSquare, hostArrays->numElements);
-  HIPCHECK(hipEventRecord(negateKernelCompleteEvent, stream1));
-  HIPCHECK(hipMemcpyAsync(hostArrays->negSquare, d_negSquare,
-                                  hostArrays->bytes, hipMemcpyDeviceToHost,
-                                  stream1));
-  if (d_negSquare_out == NULL) {
-    HIPCHECK(hipFreeAsync(d_negSquare, stream1));
-  } else {
-    *d_negSquare_out = d_negSquare;
-  }
-
-  HIPCHECK(hipStreamWaitEvent(stream2, negateKernelCompleteEvent, 0));
-  HIPCHECK(hipFreeAsync(d_square, stream2));
-  HIPCHECK(hipEventRecord(squareFreeEvent, stream2));
-
-  HIPCHECK(hipStreamWaitEvent(stream1, squareFreeEvent, 0));
-
-  HIPCHECK(hipStreamDestroy(stream2));
-  HIPCHECK(hipEventDestroy(squareKernelCompleteEvent));
-  HIPCHECK(hipEventDestroy(negateKernelCompleteEvent));
-  HIPCHECK(hipEventDestroy(squareFreeEvent));
-}
-
-/**
- * Demonstrates creating a CUDA graph including memory nodes using stream
- * capture. createNegateSquaresGraphExplicitly constructs an equivalent graph
- * without stream capture.
- */
-void createNegateSquaresGraphWithStreamCapture(hipGraphExec_t *graphExec,
-                                               negSquareArrays *hostArrays,
-                                               float **d_negSquare_out = NULL) {
-  hipGraph_t graph;
-  hipStream_t stream;
-
-  HIPCHECK(hipStreamCreateWithFlags(&stream, hipStreamNonBlocking));
-
-  HIPCHECK(hipStreamBeginCapture(stream, hipStreamCaptureModeGlobal));
-  doNegateSquaresInStream(stream, hostArrays, d_negSquare_out);
-  HIPCHECK(hipStreamEndCapture(stream, &graph));
-
-  HIPCHECK(hipGraphInstantiate(graphExec, graph, NULL, NULL, 0));
-  HIPCHECK(hipStreamDestroy(stream));
-  HIPCHECK(hipGraphDestroy(graph));
-}
-
-void prepareRefArrays(negSquareArrays *hostArrays,
-                      negSquareArrays *deviceRefArrays,
-                      bool **foundValidationFailure) {
-  deviceRefArrays->bytes = hostArrays->bytes;
-  deviceRefArrays->numElements = hostArrays->numElements;
-
-  for (int i = 0; i < hostArrays->numElements; i++) {
-    hostArrays->square[i] = hostArrays->input[i] * hostArrays->input[i];
-    hostArrays->negSquare[i] = hostArrays->square[i] * -1;
-  }
-
-  HIPCHECK(
-      hipMalloc((void **)&deviceRefArrays->negSquare, deviceRefArrays->bytes));
-  HIPCHECK(hipMemcpy(deviceRefArrays->negSquare, hostArrays->negSquare,
-                             hostArrays->bytes, hipMemcpyHostToDevice));
-
-  HIPCHECK(
-      hipMallocManaged((void **)foundValidationFailure, sizeof(bool)));
-}
-
-int checkValidationFailure(bool *foundValidationFailure) {
-  if (*foundValidationFailure) {
-    printf("Validation FAILURE!\n\n");
-    *foundValidationFailure = false;
-    return EXIT_FAILURE;
-  } else {
-    printf("Validation PASSED!\n\n");
-    return EXIT_SUCCESS;
-  }
-}
-
-__global__ void validateGPU(float *d_negSquare, negSquareArrays devRefArrays,
-                            bool *foundValidationFailure) {
-  int idx = blockIdx.x * blockDim.x + threadIdx.x;
-  float ref, diff;
-
-  if (idx < devRefArrays.numElements) {
-    ref = devRefArrays.negSquare[idx];
-    diff = d_negSquare[idx] - ref;
-    diff *= diff;
-    ref *= ref;
-    if (diff / ref > ALLOWABLE_VARIANCE) {
-      *foundValidationFailure = true;
-    }
-  }
-}
-
-void validateHost(negSquareArrays *hostArrays, bool *foundValidationFailure) {
-  float ref, diff;
-
-  for (int i = 0; i < hostArrays->numElements; i++) {
-    ref = hostArrays->input[i] * hostArrays->input[i] * -1;
-    diff = hostArrays->negSquare[i] - ref;
-    diff *= diff;
-    ref *= ref;
-    if (diff / ref > ALLOWABLE_VARIANCE) {
-      *foundValidationFailure = true;
-    }
-  }
-}
-
-int main(int argc, char **argv) {
-  negSquareArrays hostArrays, deviceRefArrays;
-  hipStream_t stream;
-  hipGraphExec_t graphExec, graphExecFreeC;
-
-  // Declare pointers for GPU buffers
-  float *d_negSquare = NULL;
-  bool *foundValidationFailure = NULL;
-
-  srand(time(0));
-  int device = findCudaDevice(argc, (const char **)argv);
-
-  int driverVersion = 0;
-  int deviceSupportsMemoryPools = 0;
-
-  hipDriverGetVersion(&driverVersion);
-  printf("Driver version is: %d.%d\n", driverVersion / 1000,
-         (driverVersion % 100) / 10);
-
-  if (driverVersion < 11040) {
-    printf("Waiving execution as driver does not support Graph Memory Nodes\n");
-    exit(EXIT_WAIVED);
-  }
-
-  hipDeviceGetAttribute(&deviceSupportsMemoryPools,
-                         hipDeviceAttributeMemoryPoolsSupported, device);
-  if (!deviceSupportsMemoryPools) {
-    printf("Waiving execution as device does not support Memory Pools\n");
-    exit(EXIT_WAIVED);
-  } else {
-    printf("Setting up sample.\n");
-  }
diff --git a/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip b/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip
index a9b9f99..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/immaTensorCoreGemm/immaTensorCoreGemm.cu.hip
@@ -1,657 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// CUDA sample demonstrating a integer GEMM computation using the Warp Matrix
-// Multiply and Accumulate API.
-
-// In this program, the compute_gemm kernel computes the result of a matrix
-// multiplication and addition: D = alpha * A * B + beta * C. The dimensions of
-// both C and D matrices are M_GLOBAL x N_GLOBAL. The A matrix is M_GLOBAL x
-// K_GLOBAL (row-major), the B matrix is K_GLOBAL x N_GLOBAL (column-major). In
-// that kernel, each CTA computes one 128 x 128 tile of the resulting matrix per
-// iteration. When the tile is computed, the CTA stores it to the global memory
-// and begins a new iteration, selecting a new 128 x 128 tile to compute.
-// Each CTA consists of eight warps. For the 128 x 128 tile, each warp computes
-// eight 16 x 16 subtiles, organized in a 2 x 4 two-dimensional array. Warps
-// compute the 16 x 16 subtiles using nvcuda::wmma::mma_sync operations by
-// moving through the K_GLOBAL dimension of the A and B matrices and
-// accumulating the intermediate result in the local thread state.
-
-// There are a number of simple optimizations used in the algorithm:
-// - The CTA copies the 128 x 128 tile of the C matrix from the global memory to
-//   shared memory. After that is done, each warp loads the C matrix fragments
-//   from shared memory, thus avoiding a random global memory access.
-// - On each internal iteration, the CTA copies a portion of the A and B
-// matrices from
-//   global memory to shared memory. After that, all warps in the CTA reuse the
-//   A and B data from shared memory, thus reducing the number of data copies
-//   from global memory.
-// - The portions of the A and B matrices are stored in shared memory with an
-// additional
-//   padding (skew) to reduce the number of shared memory access bank conflicts.
-//   (See a detailed explanation near the SKEW_HALF macro definition.)
-// - When the CTA finishes computing the tiles of the resulting matrix, each
-// warp stores
-//   its subtiles to shared memory. The CTA then copies the shared memory
-//   contents to global memory, again avoiding redundant random global memory
-//   accesses.
-// - Note that the CTA tile size is chosen to maximize the GPU register
-// utilization,
-//   but carefully enough to avoid local memory use.
-
-#include <assert.h>
-#include <hip/hip_runtime.h>
-#include <mma.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-// helper functions and utilities to work with CUDA
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-// Externally configurable parameters.
-
-#ifndef CPU_DEBUG
-// Set this to 1 to verify the correctness of the GPU-computed matrix.
-#define CPU_DEBUG 0
-#endif
-
-#ifndef SHARED_MEMORY_LIMIT_64K
-// Set this to 0 to use more than 64 Kb of shared memory to cache data, to
-// improve the performance of the computations on GPU.
-// Note that you need a GPU that can have more than 64 Kb of shared memory
-// per multiprocessor.
-#define SHARED_MEMORY_LIMIT_64K 1
-#endif
-
-// GPU configuration.
-
-#define WARP_SIZE 32
-
-// MMA matrix tile dimensions.
-
-#define M 16
-#define N 16
-#define K 16
-
-#define WMMA_M 16
-#define WMMA_N 16
-#define WMMA_K 16
-
-// GEMM configuration.
-
-#define M_TILES 256
-#define N_TILES 256
-#define K_TILES 256
-
-#define M_GLOBAL (M * M_TILES)
-#define N_GLOBAL (N * N_TILES)
-#define K_GLOBAL (K * K_TILES)
-
-#define C_LAYOUT wmma::mem_row_major
-
-// Implementation constants.
-
-#define WARPS_PER_BLOCK 8
-#define THREADS_PER_BLOCK (WARP_SIZE * WARPS_PER_BLOCK)
-
-#if SHARED_MEMORY_LIMIT_64K
-// With only 64 Kb shared memory available, we can fit two 8-tile chunks of
-// the A and B matrix data, that are 16 * 16 * 8 * 8 * 2 = 32 Kb each
-// (i.e. two 8x8 arrays of tiles of 16x16 uint8_t-typed elements per CTA).
-// But we cannot account the 8 Kb total skew overhead, without which the
-// performance would be severely impacted. So we choose to reduce the chunk size
-// in half, i.e. the amount of A and B matrix data we cache in shared memory.
-// Accordingly, this doubles the number of outer iterations across the global K
-// dimension, which only slightly impacts the performance.
-#define CHUNK_K 8
-#else
-#define CHUNK_K 16
-#endif
-
-#define CHUNK_LINE_BYTES (CHUNK_K * K * sizeof(uint8_t))
-#define WARP_COPY_BYTES (WARP_SIZE * sizeof(int4))
-#define CHUNK_COPY_LINES_PER_WARP (WARP_COPY_BYTES / CHUNK_LINE_BYTES)
-#define CHUNK_COPY_LINE_LANES (WARP_SIZE / CHUNK_COPY_LINES_PER_WARP)
-
-#define BLOCK_ROW_WARPS 2
-#define BLOCK_COL_WARPS 4
-
-#define WARP_ROW_TILES 4
-#define WARP_COL_TILES 2
-
-#define BLOCK_ROW_TILES (WARP_ROW_TILES * BLOCK_ROW_WARPS)
-#define BLOCK_COL_TILES (WARP_COL_TILES * BLOCK_COL_WARPS)
-
-#define GLOBAL_MEM_STRIDE N_GLOBAL
-
-#define SHMEM_STRIDE (N * BLOCK_ROW_TILES)
-#define SHMEM_OFFSET (N * WARP_ROW_TILES)
-
-// The macro below is used to shift rows of the A matrix and columns of the B
-// matrix in shared memory to minimize possible bank conflicts. Before
-// performing the nvcuda::wmma::mma_sync operation, the warp must load the
-// matrix data using the nvcuda::wmma::load_matrix_sync operation. Although the
-// memory access pattern is not specified for that function, each lane in the
-// warp can read one or multiple matrix elements from different matrix rows or
-// columns. For shared memory, such access can result in bank conflicts if
-// different rows / columns of the matrix map to the same bank. By shifting each
-// row and column by a few bytes, we make sure that they map to different banks,
-// thus reducing the number of possible bank conflicts. The number of 32
-// one-byte "uint8_t" elements is chosen as the minimum possible shift because
-// we must keep each row and column 256-bit aligned, as required by
-// nvcuda::wmma::load_matrix_sync.
-#define SKEW_UINT8 32
-
-#define checkKernelErrors(expr)                             \
-  do {                                                      \
-    expr;                                                   \
-                                                            \
-    hipError_t __err = hipGetLastError();                 \
-    if (__err != hipSuccess) {                             \
-      printf("Line %d: '%s' failed: %s\n", __LINE__, #expr, \
-             hipGetErrorString(__err));                    \
-      abort();                                              \
-    }                                                       \
-  } while (0)
-
-using namespace nvcuda;
-
-__host__ void init_host_matrices(uint8_t *a, uint8_t *b, int *c) {
-  for (int i = 0; i < M_GLOBAL; i++) {
-    for (int j = 0; j < K_GLOBAL; j++) {
-      a[i * K_GLOBAL + j] = (uint8_t)(rand() % 3);
-    }
-  }
-
-  for (int i = 0; i < N_GLOBAL; i++) {
-    for (int j = 0; j < K_GLOBAL; j++) {
-      b[i * K_GLOBAL + j] = (uint8_t)(rand() % 3);
-    }
-  }
-
-  for (int t = 0; t < M_GLOBAL * N_GLOBAL; t++) {
-    c[t] = (rand() % 3);
-  }
-}
-
-__global__ void compute_gemm_imma(const uint8_t *A, const uint8_t *B,
-                                  const int *C, int *D, int alpha, int beta) {
-  extern __shared__ uint8_t shmem[][CHUNK_K * K + SKEW_UINT8];
-
-  // Warp and lane identification.
-  const unsigned int warpId = threadIdx.x / WARP_SIZE;
-  const unsigned int laneId = threadIdx.x % WARP_SIZE;
-
-  // Offset in shared memory from which the B matrix is stored.
-  const size_t shmem_idx_b_off = BLOCK_COL_TILES * M;
-
-  // This pointer is used to access the C and D matrix tiles this warp computes.
-  int *shmem_warp_tile_ptr = (int *)&shmem[0][0] +
-                             (warpId / 2) * SHMEM_STRIDE * K * 2 +
-                             (warpId % 2) * SHMEM_OFFSET;
-
-  // This pointer is used to stream the C and D matrices block-wide tile to and
-  // from shared memory.
-  int *shmem_warp_stream_ptr = (int *)&shmem[0][0] + warpId * SHMEM_STRIDE * K;
-
-  // Adjust the beta scaler, as it'll be multiplied by alpha at the end of
-  // each tile computation. Technically this is not generally correct (may
-  // result in a loss of precision). Zero still needs to be specially handled
-  // though.
-  beta /= alpha;
-
-  // Each CTA slides along the 128 x 128 tiles from the top left corner of the
-  // matrix to the right and down, and selects the next tile to compute. Once
-  // there's no such tile, all warps in this CTA exit.
-  for (unsigned int block_pos = blockIdx.x;; block_pos += gridDim.x) {
-    const unsigned int block_tile_i =
-        ((block_pos * BLOCK_ROW_TILES) / N_TILES) * (BLOCK_COL_TILES);
-    const unsigned int block_tile_j = (block_pos * BLOCK_COL_TILES) % N_TILES;
-
-    // Stop when there are no more D matrix tiles to compute in this CTA.
-    if (block_tile_i >= M_TILES) {
-      break;
-    }
-
-    // This warp's pointer to the C matrix data to copy memory from to shared
-    // memory.
-    const size_t gmem_idx =
-        (block_tile_i + warpId) * M * GLOBAL_MEM_STRIDE + block_tile_j * N;
-    const int *src_gmem_warp_stream_ptr = &C[gmem_idx];
-
-    // Stream multiple C tiles to shared memory.
-#pragma unroll
-    for (int i = 0; i < K; i++) {
-      typedef int4 copy_t;
-
-      *((copy_t *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId) =
-          *((copy_t *)(src_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) +
-            laneId);
-    }
-
-    __syncthreads();
-
-    // These fragments will accumulate the result of A and B matrix fragment
-    // multiplications along the K_GLOBAL dimension.
-    wmma::fragment<wmma::accumulator, M, N, K, int> c[WARP_COL_TILES]
-                                                     [WARP_ROW_TILES];
-
-    // Load the C matrix tiles into fragments from shared memory.
-#pragma unroll
-    for (int i = 0; i < WARP_COL_TILES; i++) {
-#pragma unroll
-      for (int j = 0; j < WARP_ROW_TILES; j++) {
-        const int *tile_ptr =
-            shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
-
-        wmma::load_matrix_sync(c[i][j], tile_ptr, SHMEM_STRIDE, C_LAYOUT);
-      }
-    }
-
-    __syncthreads();
-
-    // Scale the C matrix.
-#pragma unroll
-    for (int i = 0; i < WARP_COL_TILES; i++) {
-#pragma unroll
-      for (int j = 0; j < WARP_ROW_TILES; j++) {
-#pragma unroll
-        for (int t = 0; t < c[i][j].num_elements; t++) {
-          c[i][j].x[t] *= beta;
-        }
-      }
-    }
-
-    // Select what warp copies what matrix to shared memory.
-    // Warps 0-3 copy the A matrix, warps 4-7 copy the B matrix.
-    const uint8_t *warp_ptr = (warpId < 4) ? (&A[block_tile_i * M * K_GLOBAL] +
-                                              M * K_GLOBAL * (warpId % 4) * 2)
-                                           : (&B[block_tile_j * N * K_GLOBAL] +
-                                              N * K_GLOBAL * (warpId % 4) * 2);
-
-    // Go through the global K dimension by a fixed step at a time.
-#pragma unroll
-    for (int tile_k = 0; tile_k < K_TILES; tile_k += CHUNK_K) {
-      // Copy slices of the A and B matrices to shared memory.
-      // The first half of the warps in the CTA copy the A matrix, the rest copy
-      // the B matrix.
-      size_t shmem_idx =
-          warpId < (WARPS_PER_BLOCK / 2)
-              ? (M * (warpId % (WARPS_PER_BLOCK / 2)) * 2)
-              : (N * (warpId % (WARPS_PER_BLOCK / 2)) * 2 + shmem_idx_b_off);
-
-      // First half of the warp copies the first row / column of the matrix,
-      // the second half of the warp copies the next.
-      int4 *lane_ptr = (int4 *)(warp_ptr + tile_k * K +
-                                (laneId / CHUNK_COPY_LINE_LANES) * K_GLOBAL) +
-                       (laneId % CHUNK_COPY_LINE_LANES);
-
-      // Shift the second half of the warp to the next row / column in the
-      // shared memory.
-      shmem_idx += laneId / CHUNK_COPY_LINE_LANES;
-
-#pragma unroll
-      for (int i = 0; i < ((WARP_SIZE / 2) / CHUNK_COPY_LINES_PER_WARP) * 2;
-           i++) {
-        // Copy 16 bytes at once in each lane.
-        *((int4 *)&shmem[shmem_idx][0] + (laneId % CHUNK_COPY_LINE_LANES)) =
-            *lane_ptr;
-
-        // Advance the global memory pointer and the shared memory index.
-        lane_ptr = (int4 *)((uint8_t *)lane_ptr +
-                            K_GLOBAL * CHUNK_COPY_LINES_PER_WARP);
-        shmem_idx += CHUNK_COPY_LINES_PER_WARP;
-      }
-
-      __syncthreads();
-
-      // Compute a grid of C matrix tiles in each warp.
-#pragma unroll
-      for (int k_step = 0; k_step < CHUNK_K; k_step++) {
-        wmma::fragment<wmma::matrix_a, M, N, K, uint8_t, wmma::row_major>
-            a[WARP_COL_TILES];
-        wmma::fragment<wmma::matrix_b, M, N, K, uint8_t, wmma::col_major>
-            b[WARP_ROW_TILES];
-
-#pragma unroll
-        for (int i = 0; i < WARP_COL_TILES; i++) {
-          size_t shmem_idx_a = (warpId / 2) * M * 2 + (i * M);
-          const uint8_t *tile_ptr = &shmem[shmem_idx_a][k_step * K];
-
-          wmma::load_matrix_sync(a[i], tile_ptr, K * CHUNK_K + SKEW_UINT8);
-
-#pragma unroll
-          for (int j = 0; j < WARP_ROW_TILES; j++) {
-            if (i == 0) {
-              // Load the B matrix fragment once, because it is going to be
-              // reused against the other A matrix fragments.
-              size_t shmem_idx_b = shmem_idx_b_off +
-                                   (WARP_ROW_TILES * N) * (warpId % 2) +
-                                   (j * N);
-              const uint8_t *tile_ptr = &shmem[shmem_idx_b][k_step * K];
-
-              wmma::load_matrix_sync(b[j], tile_ptr, K * CHUNK_K + SKEW_UINT8);
-            }
-
-            wmma::mma_sync(c[i][j], a[i], b[j], c[i][j]);
-          }
-        }
-      }
-
-      __syncthreads();
-    }
-
-      // Store the D fragments to shared memory.
-#pragma unroll
-    for (int i = 0; i < WARP_COL_TILES; i++) {
-#pragma unroll
-      for (int j = 0; j < WARP_ROW_TILES; j++) {
-#pragma unroll
-        // Uniform, point-wise transformations of ALL fragment elements by ALL
-        // threads in the warp are well-defined even though element indices
-        // within fragment storage are not defined.
-        for (int t = 0; t < c[i][j].num_elements; t++) c[i][j].x[t] *= alpha;
-
-        int *tile_ptr = shmem_warp_tile_ptr + i * SHMEM_STRIDE * K + j * N;
-
-        wmma::store_matrix_sync(tile_ptr, c[i][j], SHMEM_STRIDE, C_LAYOUT);
-      }
-    }
-
-    __syncthreads();
-
-    // Now that shared memory contains all the D tiles, stream them to global
-    // memory.
-    int *dst_gmem_warp_stream_ptr = &D[gmem_idx];
-
-#pragma unroll
-    for (int i = 0; i < K; i++) {
-      *((int4 *)(dst_gmem_warp_stream_ptr + GLOBAL_MEM_STRIDE * i) + laneId) =
-          *((int4 *)(shmem_warp_stream_ptr + SHMEM_STRIDE * i) + laneId);
-    }
-
-    __syncthreads();
-  }
-}
-
-// Performs an MxNxK GEMM (C=alpha*A*B + beta*C) assuming:
-//  1) Matrices are packed in memory.
-//  2) M, N and K are multiples of 16.
-//  3) Neither A nor B are transposed.
-// Note: This is a less performant version of the compute_gemm_imma kernel. It
-// is designed for
-//       demonstration purposes only to show the CUDA WMMA API use without
-//       relying on availability of the shared memory.
-__global__ void simple_wmma_gemm_imma(const uint8_t *a, const uint8_t *b,
-                                      const int *c, int *d, int m_ld, int n_ld,
-                                      int k_ld, int alpha, int beta) {
-  // Leading dimensions. Packed with no transpositions.
-  int lda = m_ld;
-  int ldb = k_ld;
-  int ldc = n_ld;
-
-  // Tile using a 2D grid
-  int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;
-  int warpN = (blockIdx.y * blockDim.y + threadIdx.y);
-
-  // Declare the fragments
-  wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, uint8_t,
-                 wmma::row_major>
-      a_frag;
-  wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, uint8_t,
-                 wmma::col_major>
-      b_frag;
-  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, int> acc_frag;
-  wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, int> c_frag;
-
-  wmma::fill_fragment(acc_frag, 0.0f);
-
-  // Loop over k
-  for (int i = 0; i < k_ld; i += WMMA_K) {
-    int aCol = i;
-    int aRow = warpM * WMMA_M;
-
-    int bCol = i;
-    int bRow = warpN * WMMA_N;
-
-    // Bounds checking
-    if (aRow < m_ld && aCol < k_ld && bRow < k_ld && bCol < n_ld) {
-      // Load the inputs
-      wmma::load_matrix_sync(a_frag, a + aCol + aRow * lda, lda);
-      wmma::load_matrix_sync(b_frag, b + bCol + bRow * ldb, ldb);
-
-      // Perform the matrix multiplication
-      wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);
-    }
-  }
-
-  // Load in the current value of c, scale it by beta, and add this our result
-  // scaled by alpha
-  int cCol = warpN * WMMA_N;
-  int cRow = warpM * WMMA_M;
-
-  if (cRow < m_ld && cCol < n_ld) {
-    wmma::load_matrix_sync(c_frag, c + cCol + cRow * ldc, ldc,
-                           wmma::mem_row_major);
-
-    for (int i = 0; i < c_frag.num_elements; i++) {
-      c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];
-    }
-
-    // Store the output
-    wmma::store_matrix_sync(d + cCol + cRow * ldc, c_frag, ldc,
-                            wmma::mem_row_major);
-  }
-}
-
-__host__ void matMultiplyOnHost(uint8_t *A, uint8_t *B, int *C, int alpha,
-                                int beta, int numARows, int numAColumns,
-                                int numBRows, int numBColumns, int numCRows,
-                                int numCColumns) {
-  for (int i = 0; i < numCRows; i++) {
-    for (int j = 0; j < numCColumns; j++) {
-      int temp = 0;
-
-      for (int k = 0; k < numAColumns; k++) {
-        temp += A[i * numAColumns + k] * B[j * numBRows + k];
-      }
-
-      C[i * numCColumns + j] = temp * alpha + beta * C[i * numCColumns + j];
-    }
-  }
-}
-
-int main(int argc, char **argv) {
-  printf("Initializing...\n");
-
-  int dev = findCudaDevice(argc, (const char **)argv);
-
-  hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
-
-  // Tensor cores require a GPU of Volta (SM72) architecture or higher.
-  if (deviceProp.major < 7 || (deviceProp.major <= 7 && deviceProp.minor < 2)) {
-    printf(
-        "immaTensorCoreGemm requires SM 7.2 or higher to use Tensor Cores.  "
-        "Exiting...\n");
-    exit(EXIT_WAIVED);
-  }
-
-  printf("M: %d (%d x %d)\n", M_GLOBAL, M, M_TILES);
-  printf("N: %d (%d x %d)\n", N_GLOBAL, N, N_TILES);
-  printf("K: %d (%d x %d)\n", K_GLOBAL, K, K_TILES);
-
-  uint8_t *A_h = NULL;
-  uint8_t *B_h = NULL;
-  int *C_h = NULL;
-#if CPU_DEBUG
-  int *result_hD = NULL;
-  int *result_host = NULL;
-#endif
-
-  A_h = (uint8_t *)malloc(sizeof(uint8_t) * M_GLOBAL * K_GLOBAL);
-  B_h = (uint8_t *)malloc(sizeof(uint8_t) * K_GLOBAL * N_GLOBAL);
-  C_h = (int *)malloc(sizeof(int) * M_GLOBAL * N_GLOBAL);
-#if CPU_DEBUG
-  result_hD = (int *)malloc(sizeof(int) * M_GLOBAL * N_GLOBAL);
-  result_host = (int *)malloc(sizeof(int) * M_GLOBAL * N_GLOBAL);
-#endif
-
-  uint8_t *A = NULL;
-  uint8_t *B = NULL;
-  int *C = NULL;
-  int *D = NULL;
-
-  HIPCHECK(
-      hipMalloc(reinterpret_cast<void **>(&A), sizeof(uint8_t) * M_GLOBAL * K_GLOBAL));
-  HIPCHECK(
-      hipMalloc(reinterpret_cast<void **>(&B), sizeof(uint8_t) * N_GLOBAL * K_GLOBAL));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&C), sizeof(int) * M_GLOBAL * N_GLOBAL));
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&D), sizeof(int) * M_GLOBAL * N_GLOBAL));
-
-  assert(((unsigned long long)A) % 128 == 0);
-  assert(((unsigned long long)B) % 128 == 0);
-  assert(((unsigned long long)C) % 128 == 0);
-  assert(((unsigned long long)D) % 128 == 0);
-
-  init_host_matrices(A_h, B_h, C_h);
-
-  HIPCHECK(hipMemcpy(A, A_h, sizeof(uint8_t) * M_GLOBAL * K_GLOBAL,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(B, B_h, sizeof(uint8_t) * N_GLOBAL * K_GLOBAL,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(C, C_h, sizeof(int) * M_GLOBAL * N_GLOBAL,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemset(D, 0, sizeof(int) * M_GLOBAL * N_GLOBAL));
-
-  printf("Preparing data for GPU...\n");
-
-  assert(((unsigned long long)A) % 128 == 0);
-  assert(((unsigned long long)B) % 128 == 0);
-  assert(((unsigned long long)C) % 128 == 0);
-  assert(((unsigned long long)D) % 128 == 0);
-
-  enum {
-    // Compute the right amount of shared memory to request.
-    // We need shared memory to hold per-CTA C and D matrix tiles, and to cache
-    // per-CTA chunks
-    // of the A and B matrices. Therefore, the right amount to request is the
-    // maximum of those
-    // two numbers.
-    SHMEM_SZ = MAX(sizeof(uint8_t) * (BLOCK_COL_TILES * M) *
-                       (CHUNK_K * K + SKEW_UINT8) * 2,
-                   M * (BLOCK_ROW_WARPS * WARP_ROW_TILES) * N *
-                       (BLOCK_COL_WARPS * WARP_COL_TILES) * sizeof(int))
-  };
-
-  printf("Required shared memory size: %lu Kb\n", SHMEM_SZ / 1024UL);
-
-  int alpha = 1;
-  int beta = 1;
-
-  hipEvent_t start, stop;
-
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-  HIPCHECK(hipEventRecord(start));
-
-  // If enough shared memory available on the GPU use high performant kernel
-  if (deviceProp.sharedMemPerMultiprocessor >= SHMEM_SZ) {
-    printf("Computing... using high performance kernel compute_gemm_imma \n");
-
-    HIPCHECK(hipFuncSetAttribute(
-        compute_gemm_imma, hipFuncAttributeMaxDynamicSharedMemorySize,
-        SHMEM_SZ));
-    checkKernelErrors(
-        (compute_gemm_imma<<<deviceProp.multiProcessorCount, THREADS_PER_BLOCK,
-                             SHMEM_SZ>>>(A, B, C, D, alpha, beta)));
-#if CPU_DEBUG
-    HIPCHECK(hipMemcpy(result_hD, D, sizeof(int) * M_GLOBAL * N_GLOBAL,
-                               hipMemcpyDeviceToHost));
-#endif
-  } else {
-    dim3 gridDim;
-    dim3 blockDim;
-
-    // blockDim.x must be a multiple of warpSize
-    // 128x4 means we have 16 warps and a block computes a 64x64 output tile
-    blockDim.x = 128;
-    blockDim.y = 4;
-
-    gridDim.x = (M_GLOBAL + (WMMA_M * blockDim.x / 32 - 1)) /
-                (WMMA_M * blockDim.x / 32);
-    gridDim.y = (N_GLOBAL + WMMA_N * blockDim.y - 1) / (WMMA_N * blockDim.y);
-
-    printf("Computing... using simple_wmma_gemm_imma kernel\n");
-    simple_wmma_gemm_imma<<<gridDim, blockDim>>>(A, B, C, D, M_GLOBAL, N_GLOBAL,
-                                                 K_GLOBAL, alpha, beta);
-#if CPU_DEBUG
-    HIPCHECK(hipMemcpy(result_hD, D, sizeof(int) * M_GLOBAL * N_GLOBAL,
-                               hipMemcpyDeviceToHost));
-#endif
-  }
-
-  HIPCHECK(hipEventRecord(stop));
-  HIPCHECK(hipEventSynchronize(stop));
-
-#if CPU_DEBUG
-  printf("Verifying correctness of the computations...\n");
-
-  memcpy(result_host, C_h, sizeof(int) * M_GLOBAL * N_GLOBAL);
-
-  matMultiplyOnHost(A_h, B_h, result_host, alpha, beta, M_GLOBAL, K_GLOBAL,
-                    K_GLOBAL, N_GLOBAL, M_GLOBAL, N_GLOBAL);
-
-  for (int i = 0; i < N_GLOBAL * M_GLOBAL; i++) {
-    if (abs(result_hD[i] - result_host[i]) > 0) {
-      printf("mismatch i=%d result_hD=%d result_host=%d\n", i, result_hD[i],
-             result_host[i]);
-    }
-  }
-  free(result_host);
-  free(result_hD);
-#endif
-
-  float milliseconds = 0;
-
-  HIPCHECK(hipEventElapsedTime(&milliseconds, start, stop));
-
-    printf("Time: %f ms\n", milliseconds);
-    printf("TOPS: %.2f\n", (((double)M_GLOBAL * N_GLOBAL * K_GLOBAL * 2)/(milliseconds/1000.)) / 1e12);
-
-  free(A_h);
-  free(B_h);
-  free(C_h);
-  HIPCHECK(hipFree(reinterpret_cast<void *>(A)));
-  HIPCHECK(hipFree(reinterpret_cast<void *>(B)));
-  HIPCHECK(hipFree(reinterpret_cast<void *>(C)));
-  HIPCHECK(hipFree(reinterpret_cast<void *>(D)));
-
-  return EXIT_SUCCESS;
-}
diff --git a/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.cu.hip b/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.cu.hip
index 14570c1..e69de29 100644
--- a/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.cu.hip
+++ b/src/samples/Samples/3_CUDA_Features/simpleCudaGraphs/simpleCudaGraphs.cu.hip
@@ -1,413 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <hip/hip_cooperative_groups.h>
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-#include <vector>
-
-namespace cg = cooperative_groups;
-
-#define THREADS_PER_BLOCK 512
-#define GRAPH_LAUNCH_ITERATIONS 3
-
-typedef struct callBackData {
-  const char *fn_name;
-  double *data;
-} callBackData_t;
-
-__global__ void reduce(float *inputVec, double *outputVec, size_t inputSize,
-                       size_t outputSize) {
-  __shared__ double tmp[THREADS_PER_BLOCK];
-
-  cg::thread_block cta = cg::this_thread_block();
-  size_t globaltid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  double temp_sum = 0.0;
-  for (int i = globaltid; i < inputSize; i += gridDim.x * blockDim.x) {
-    temp_sum += (double)inputVec[i];
-  }
-  tmp[cta.thread_rank()] = temp_sum;
-
-  cg::sync(cta);
-
-  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);
-
-  double beta = temp_sum;
-  double temp;
-
-  for (int i = tile32.size() / 2; i > 0; i >>= 1) {
-    if (tile32.thread_rank() < i) {
-      temp = tmp[cta.thread_rank() + i];
-      beta += temp;
-      tmp[cta.thread_rank()] = beta;
-    }
-    cg::sync(tile32);
-  }
-  cg::sync(cta);
-
-  if (cta.thread_rank() == 0 && blockIdx.x < outputSize) {
-    beta = 0.0;
-    for (int i = 0; i < cta.size(); i += tile32.size()) {
-      beta += tmp[i];
-    }
-    outputVec[blockIdx.x] = beta;
-  }
-}
-
-__global__ void reduceFinal(double *inputVec, double *result,
-                            size_t inputSize) {
-  __shared__ double tmp[THREADS_PER_BLOCK];
-
-  cg::thread_block cta = cg::this_thread_block();
-  size_t globaltid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  double temp_sum = 0.0;
-  for (int i = globaltid; i < inputSize; i += gridDim.x * blockDim.x) {
-    temp_sum += (double)inputVec[i];
-  }
-  tmp[cta.thread_rank()] = temp_sum;
-
-  cg::sync(cta);
-
-  cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(cta);
-
-  // do reduction in shared mem
-  if ((blockDim.x >= 512) && (cta.thread_rank() < 256)) {
-    tmp[cta.thread_rank()] = temp_sum = temp_sum + tmp[cta.thread_rank() + 256];
-  }
-
-  cg::sync(cta);
-
-  if ((blockDim.x >= 256) && (cta.thread_rank() < 128)) {
-    tmp[cta.thread_rank()] = temp_sum = temp_sum + tmp[cta.thread_rank() + 128];
-  }
-
-  cg::sync(cta);
-
-  if ((blockDim.x >= 128) && (cta.thread_rank() < 64)) {
-    tmp[cta.thread_rank()] = temp_sum = temp_sum + tmp[cta.thread_rank() + 64];
-  }
-
-  cg::sync(cta);
-
-  if (cta.thread_rank() < 32) {
-    // Fetch final intermediate sum from 2nd warp
-    if (blockDim.x >= 64) temp_sum += tmp[cta.thread_rank() + 32];
-    // Reduce final warp using shuffle
-    for (int offset = tile32.size() / 2; offset > 0; offset /= 2) {
-      temp_sum += tile32.shfl_down(temp_sum, offset);
-    }
-  }
-  // write result for this block to global mem
-  if (cta.thread_rank() == 0) result[0] = temp_sum;
-}
-
-void init_input(float *a, size_t size) {
-  for (size_t i = 0; i < size; i++) a[i] = (rand() & 0xFF) / (float)RAND_MAX;
-}
-
-void CUDART_CB myHostNodeCallback(void *data) {
-  // Check status of GPU after stream operations are done
-  callBackData_t *tmp = (callBackData_t *)(data);
-  // HIPCHECK(tmp->status);
-
-  double *result = (double *)(tmp->data);
-  char *function = (char *)(tmp->fn_name);
-  printf("[%s] Host callback final reduced sum = %lf\n", function, *result);
-  *result = 0.0;  // reset the result
-}
-
-void cudaGraphsManual(float *inputVec_h, float *inputVec_d, double *outputVec_d,
-                      double *result_d, size_t inputSize, size_t numOfBlocks) {
-  hipStream_t streamForGraph;
-  hipGraph_t graph;
-  std::vector<hipGraphNode_t> nodeDependencies;
-  hipGraphNode_t memcpyNode, kernelNode, memsetNode;
-  double result_h = 0.0;
-
-  HIPCHECK(hipStreamCreate(&streamForGraph));
-
-  hipKernelNodeParams kernelNodeParams = {0};
-  hipMemcpy3DParms memcpyParams = {0};
-  hipMemsetParams memsetParams = {0};
-
-  memcpyParams.srcArray = NULL;
-  memcpyParams.srcPos = make_hipPos(0, 0, 0);
-  memcpyParams.srcPtr =
-      make_hipPitchedPtr(inputVec_h, sizeof(float) * inputSize, inputSize, 1);
-  memcpyParams.dstArray = NULL;
-  memcpyParams.dstPos = make_hipPos(0, 0, 0);
-  memcpyParams.dstPtr =
-      make_hipPitchedPtr(inputVec_d, sizeof(float) * inputSize, inputSize, 1);
-  memcpyParams.extent = make_hipExtent(sizeof(float) * inputSize, 1, 1);
-  memcpyParams.kind = hipMemcpyHostToDevice;
-
-  memsetParams.dst = (void *)outputVec_d;
-  memsetParams.value = 0;
-  memsetParams.pitch = 0;
-  memsetParams.elementSize = sizeof(float);  // elementSize can be max 4 bytes
-  memsetParams.width = numOfBlocks * 2;
-  memsetParams.height = 1;
-
-  HIPCHECK(hipGraphCreate(&graph, 0));
-  HIPCHECK(
-      hipGraphAddMemcpyNode(&memcpyNode, graph, NULL, 0, &memcpyParams));
-  HIPCHECK(
-      hipGraphAddMemsetNode(&memsetNode, graph, NULL, 0, &memsetParams));
-
-  nodeDependencies.push_back(memsetNode);
-  nodeDependencies.push_back(memcpyNode);
-
-  void *kernelArgs[4] = {(void *)&inputVec_d, (void *)&outputVec_d, &inputSize,
-                         &numOfBlocks};
-
-  kernelNodeParams.func = (void *)reduce;
-  kernelNodeParams.gridDim = dim3(numOfBlocks, 1, 1);
-  kernelNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
-  kernelNodeParams.sharedMemBytes = 0;
-  kernelNodeParams.kernelParams = (void **)kernelArgs;
-  kernelNodeParams.extra = NULL;
-
-  HIPCHECK(
-      hipGraphAddKernelNode(&kernelNode, graph, nodeDependencies.data(),
-                             nodeDependencies.size(), &kernelNodeParams));
-
-  nodeDependencies.clear();
-  nodeDependencies.push_back(kernelNode);
-
-  memset(&memsetParams, 0, sizeof(memsetParams));
-  memsetParams.dst = result_d;
-  memsetParams.value = 0;
-  memsetParams.elementSize = sizeof(float);
-  memsetParams.width = 2;
-  memsetParams.height = 1;
-  HIPCHECK(
-      hipGraphAddMemsetNode(&memsetNode, graph, NULL, 0, &memsetParams));
-
-  nodeDependencies.push_back(memsetNode);
-
-  memset(&kernelNodeParams, 0, sizeof(kernelNodeParams));
-  kernelNodeParams.func = (void *)reduceFinal;
-  kernelNodeParams.gridDim = dim3(1, 1, 1);
-  kernelNodeParams.blockDim = dim3(THREADS_PER_BLOCK, 1, 1);
-  kernelNodeParams.sharedMemBytes = 0;
-  void *kernelArgs2[3] = {(void *)&outputVec_d, (void *)&result_d,
-                          &numOfBlocks};
-  kernelNodeParams.kernelParams = kernelArgs2;
-  kernelNodeParams.extra = NULL;
-
-  HIPCHECK(
-      hipGraphAddKernelNode(&kernelNode, graph, nodeDependencies.data(),
-                             nodeDependencies.size(), &kernelNodeParams));
-  nodeDependencies.clear();
-  nodeDependencies.push_back(kernelNode);
-
-  memset(&memcpyParams, 0, sizeof(memcpyParams));
-
-  memcpyParams.srcArray = NULL;
-  memcpyParams.srcPos = make_hipPos(0, 0, 0);
-  memcpyParams.srcPtr = make_hipPitchedPtr(result_d, sizeof(double), 1, 1);
-  memcpyParams.dstArray = NULL;
-  memcpyParams.dstPos = make_hipPos(0, 0, 0);
-  memcpyParams.dstPtr = make_hipPitchedPtr(&result_h, sizeof(double), 1, 1);
-  memcpyParams.extent = make_hipExtent(sizeof(double), 1, 1);
-  memcpyParams.kind = hipMemcpyDeviceToHost;
-  HIPCHECK(
-      hipGraphAddMemcpyNode(&memcpyNode, graph, nodeDependencies.data(),
-                             nodeDependencies.size(), &memcpyParams));
-  nodeDependencies.clear();
-  nodeDependencies.push_back(memcpyNode);
-
-  hipGraphNode_t hostNode;
-  hipHostNodeParams hostParams = {0};
-  hostParams.fn = myHostNodeCallback;
-  callBackData_t hostFnData;
-  hostFnData.data = &result_h;
-  hostFnData.fn_name = "cudaGraphsManual";
-  hostParams.userData = &hostFnData;
-
-  HIPCHECK(hipGraphAddHostNode(&hostNode, graph,
-                                       nodeDependencies.data(),
-                                       nodeDependencies.size(), &hostParams));
-
-  hipGraphNode_t *nodes = NULL;
-  size_t numNodes = 0;
-  HIPCHECK(hipGraphGetNodes(graph, nodes, &numNodes));
-  printf("\nNum of nodes in the graph created manually = %zu\n", numNodes);
-
-  hipGraphExec_t graphExec;
-  HIPCHECK(hipGraphInstantiate(&graphExec, graph, NULL, NULL, 0));
-
-  hipGraph_t clonedGraph;
-  hipGraphExec_t clonedGraphExec;
-  HIPCHECK(hipGraphClone(&clonedGraph, graph));
-  HIPCHECK(
-      hipGraphInstantiate(&clonedGraphExec, clonedGraph, NULL, NULL, 0));
-
-  for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {
-    HIPCHECK(hipGraphLaunch(graphExec, streamForGraph));
-  }
-
-  HIPCHECK(hipStreamSynchronize(streamForGraph));
-
-  printf("Cloned Graph Output.. \n");
-  for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {
-    HIPCHECK(hipGraphLaunch(clonedGraphExec, streamForGraph));
-  }
-  HIPCHECK(hipStreamSynchronize(streamForGraph));
-
-  HIPCHECK(hipGraphExecDestroy(graphExec));
-  HIPCHECK(hipGraphExecDestroy(clonedGraphExec));
-  HIPCHECK(hipGraphDestroy(graph));
-  HIPCHECK(hipGraphDestroy(clonedGraph));
-  HIPCHECK(hipStreamDestroy(streamForGraph));
-}
-
-void cudaGraphsUsingStreamCapture(float *inputVec_h, float *inputVec_d,
-                                  double *outputVec_d, double *result_d,
-                                  size_t inputSize, size_t numOfBlocks) {
-  hipStream_t stream1, stream2, stream3, streamForGraph;
-  hipEvent_t forkStreamEvent, memsetEvent1, memsetEvent2;
-  hipGraph_t graph;
-  double result_h = 0.0;
-
-  HIPCHECK(hipStreamCreate(&stream1));
-  HIPCHECK(hipStreamCreate(&stream2));
-  HIPCHECK(hipStreamCreate(&stream3));
-  HIPCHECK(hipStreamCreate(&streamForGraph));
-
-  HIPCHECK(hipEventCreate(&forkStreamEvent));
-  HIPCHECK(hipEventCreate(&memsetEvent1));
-  HIPCHECK(hipEventCreate(&memsetEvent2));
-
-  HIPCHECK(hipStreamBeginCapture(stream1, hipStreamCaptureModeGlobal));
-
-  HIPCHECK(hipEventRecord(forkStreamEvent, stream1));
-  HIPCHECK(hipStreamWaitEvent(stream2, forkStreamEvent, 0));
-  HIPCHECK(hipStreamWaitEvent(stream3, forkStreamEvent, 0));
-
-  HIPCHECK(hipMemcpyAsync(inputVec_d, inputVec_h,
-                                  sizeof(float) * inputSize, hipMemcpyDefault,
-                                  stream1));
-
-  HIPCHECK(
-      hipMemsetAsync(outputVec_d, 0, sizeof(double) * numOfBlocks, stream2));
-
-  HIPCHECK(hipEventRecord(memsetEvent1, stream2));
-
-  HIPCHECK(hipMemsetAsync(result_d, 0, sizeof(double), stream3));
-  HIPCHECK(hipEventRecord(memsetEvent2, stream3));
-
-  HIPCHECK(hipStreamWaitEvent(stream1, memsetEvent1, 0));
-
-  reduce<<<numOfBlocks, THREADS_PER_BLOCK, 0, stream1>>>(
-      inputVec_d, outputVec_d, inputSize, numOfBlocks);
-
-  HIPCHECK(hipStreamWaitEvent(stream1, memsetEvent2, 0));
-
-  reduceFinal<<<1, THREADS_PER_BLOCK, 0, stream1>>>(outputVec_d, result_d,
-                                                    numOfBlocks);
-  HIPCHECK(hipMemcpyAsync(&result_h, result_d, sizeof(double),
-                                  hipMemcpyDefault, stream1));
-
-  callBackData_t hostFnData = {0};
-  hostFnData.data = &result_h;
-  hostFnData.fn_name = "cudaGraphsUsingStreamCapture";
-  hipHostFn_t fn = myHostNodeCallback;
-  HIPCHECK(hipLaunchHostFunc(stream1, fn, &hostFnData));
-  HIPCHECK(hipStreamEndCapture(stream1, &graph));
-
-  hipGraphNode_t *nodes = NULL;
-  size_t numNodes = 0;
-  HIPCHECK(hipGraphGetNodes(graph, nodes, &numNodes));
-  printf("\nNum of nodes in the graph created using stream capture API = %zu\n",
-         numNodes);
-
-  hipGraphExec_t graphExec;
-  HIPCHECK(hipGraphInstantiate(&graphExec, graph, NULL, NULL, 0));
-
-  hipGraph_t clonedGraph;
-  hipGraphExec_t clonedGraphExec;
-  HIPCHECK(hipGraphClone(&clonedGraph, graph));
-  HIPCHECK(
-      hipGraphInstantiate(&clonedGraphExec, clonedGraph, NULL, NULL, 0));
-
-  for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {
-    HIPCHECK(hipGraphLaunch(graphExec, streamForGraph));
-  }
-
-  HIPCHECK(hipStreamSynchronize(streamForGraph));
-
-  printf("Cloned Graph Output.. \n");
-  for (int i = 0; i < GRAPH_LAUNCH_ITERATIONS; i++) {
-    HIPCHECK(hipGraphLaunch(clonedGraphExec, streamForGraph));
-  }
-
-  HIPCHECK(hipStreamSynchronize(streamForGraph));
-
-  HIPCHECK(hipGraphExecDestroy(graphExec));
-  HIPCHECK(hipGraphExecDestroy(clonedGraphExec));
-  HIPCHECK(hipGraphDestroy(graph));
-  HIPCHECK(hipGraphDestroy(clonedGraph));
-  HIPCHECK(hipStreamDestroy(stream1));
-  HIPCHECK(hipStreamDestroy(stream2));
-  HIPCHECK(hipStreamDestroy(streamForGraph));
-}
-
-int main(int argc, char **argv) {
-  size_t size = 1 << 24;  // number of elements to reduce
-  size_t maxBlocks = 512;
-
-  // This will pick the best possible CUDA capable device
-  int devID = findCudaDevice(argc, (const char **)argv);
-
-  printf("%zu elements\n", size);
-  printf("threads per block  = %d\n", THREADS_PER_BLOCK);
-  printf("Graph Launch iterations = %d\n", GRAPH_LAUNCH_ITERATIONS);
-
-  float *inputVec_d = NULL, *inputVec_h = NULL;
-  double *outputVec_d = NULL, *result_d;
-
-  HIPCHECK(hipHostMalloc(&inputVec_h, sizeof(float) * size));
-  HIPCHECK(hipMalloc(&inputVec_d, sizeof(float) * size));
-  HIPCHECK(hipMalloc(&outputVec_d, sizeof(double) * maxBlocks));
-  HIPCHECK(hipMalloc(&result_d, sizeof(double)));
-
-  init_input(inputVec_h, size);
-
-  cudaGraphsManual(inputVec_h, inputVec_d, outputVec_d, result_d, size,
-                   maxBlocks);
-  cudaGraphsUsingStreamCapture(inputVec_h, inputVec_d, outputVec_d, result_d,
-                               size, maxBlocks);
-
-  HIPCHECK(hipFree(inputVec_d));
-  HIPCHECK(hipFree(outputVec_d));
-  HIPCHECK(hipFree(result_d));
-  HIPCHECK(hipHostFree(inputVec_h));
-  return EXIT_SUCCESS;
-}
diff --git a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip
index 3904639..e69de29 100644
--- a/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/conjugateGradientCudaGraphs/conjugateGradientCudaGraphs.cu.hip
@@ -1,437 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample implements a conjugate gradient solver on GPU
- * using CUBLAS and CUSPARSE with CUDA Graphs
- *
- */
-
-// includes, system
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-
-/* Using updated (v2) interfaces to cublas */
-#include <hipblas.h>
-#include <hip/hip_runtime.h>
-#include <hipsparse.h>
-
-// Utilities and system includes
-#include <helper_cuda.h>  // helper function CUDA error checking and initialization
-#include <helper_functions.h>  // helper for shared functions common to CUDA Samples
-
-const char *sSDKname = "conjugateGradientCudaGraphs";
-
-#ifndef WITH_GRAPH
-#define WITH_GRAPH 1
-#endif
-
-/* genTridiag: generate a random tridiagonal symmetric matrix */
-void genTridiag(int *I, int *J, float *val, int N, int nz) {
-  I[0] = 0, J[0] = 0, J[1] = 1;
-  val[0] = (float)rand() / RAND_MAX + 10.0f;
-  val[1] = (float)rand() / RAND_MAX;
-  int start;
-
-  for (int i = 1; i < N; i++) {
-    if (i > 1) {
-      I[i] = I[i - 1] + 3;
-    } else {
-      I[1] = 2;
-    }
-
-    start = (i - 1) * 3 + 2;
-    J[start] = i - 1;
-    J[start + 1] = i;
-
-    if (i < N - 1) {
-      J[start + 2] = i + 1;
-    }
-
-    val[start] = val[start - 1];
-    val[start + 1] = (float)rand() / RAND_MAX + 10.0f;
-
-    if (i < N - 1) {
-      val[start + 2] = (float)rand() / RAND_MAX;
-    }
-  }
-
-  I[N] = nz;
-}
-
-__global__ void initVectors(float *rhs, float *x, int N) {
-  size_t gid = blockIdx.x * blockDim.x + threadIdx.x;
-
-  for (size_t i = gid; i < N; i += gridDim.x * blockDim.x) {
-    rhs[i] = 1.0;
-    x[i] = 0.0;
-  }
-}
-
-__global__ void r1_div_x(float *r1, float *r0, float *b) {
-  int gid = blockIdx.x * blockDim.x + threadIdx.x;
-  if (gid == 0) {
-    b[0] = r1[0] / r0[0];
-  }
-}
-
-__global__ void a_minus(float *a, float *na) {
-  int gid = blockIdx.x * blockDim.x + threadIdx.x;
-  if (gid == 0) {
-    na[0] = -(a[0]);
-  }
-}
-
-int main(int argc, char **argv) {
-  int N = 0, nz = 0, *I = NULL, *J = NULL;
-  float *val = NULL;
-  const float tol = 1e-5f;
-  const int max_iter = 10000;
-  float *x;
-  float *rhs;
-  float r1;
-
-  int *d_col, *d_row;
-  float *d_val, *d_x;
-  float *d_r, *d_p, *d_Ax;
-  int k;
-  float alpha, beta, alpham1;
-
-  hipStream_t stream1, streamForGraph;
-
-  // This will pick the best possible CUDA capable device
-  hipDeviceProp_t deviceProp;
-  int devID = findCudaDevice(argc, (const char **)argv);
-
-  if (devID < 0) {
-    printf("exiting...\n");
-    exit(EXIT_SUCCESS);
-  }
-
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, devID));
-
-  // Statistics about the GPU device
-  printf(
-      "> GPU device has %d Multi-Processors, SM %d.%d compute capabilities\n\n",
-      deviceProp.multiProcessorCount, deviceProp.major, deviceProp.minor);
-
-  /* Generate a random tridiagonal symmetric matrix in CSR format */
-  N = 1048576;
-  nz = (N - 2) * 3 + 4;
-  HIPCHECK(hipHostMalloc(&I, sizeof(int) * (N + 1)));
-  HIPCHECK(hipHostMalloc(&J, sizeof(int) * nz));
-  HIPCHECK(hipHostMalloc(&val, sizeof(float) * nz));
-  genTridiag(I, J, val, N, nz);
-
-  HIPCHECK(hipHostMalloc(&x, sizeof(float) * N));
-  rhs = (float *)malloc(sizeof(float) * N);
-
-  for (int i = 0; i < N; i++) {
-    rhs[i] = 1.0;
-    x[i] = 0.0;
-  }
-
-  /* Get handle to the CUBLAS context */
-  hipblasHandle_t cublasHandle = 0;
-  hipblasStatus_t hipblasStatus_t;
-  hipblasStatus_t = hipblasCreate(&cublasHandle);
-
-  HIPCHECK(hipblasStatus_t);
-
-  /* Get handle to the CUSPARSE context */
-  hipsparseHandle_t cusparseHandle = 0;
-  hipsparseStatus_t cusparseStatus;
-  cusparseStatus = hipsparseCreate(&cusparseHandle);
-
-  HIPCHECK(cusparseStatus);
-
-  HIPCHECK(hipStreamCreate(&stream1));
-
-  HIPCHECK(hipMalloc((void **)&d_col, nz * sizeof(int)));
-  HIPCHECK(hipMalloc((void **)&d_row, (N + 1) * sizeof(int)));
-  HIPCHECK(hipMalloc((void **)&d_val, nz * sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_x, N * sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_r, N * sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_p, N * sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_Ax, N * sizeof(float)));
-
-  float *d_r1, *d_r0, *d_dot, *d_a, *d_na, *d_b;
-  HIPCHECK(hipMalloc((void **)&d_r1, sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_r0, sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_dot, sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_a, sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_na, sizeof(float)));
-  HIPCHECK(hipMalloc((void **)&d_b, sizeof(float)));
-
-  /* Wrap raw data into cuSPARSE generic API objects */
-  hipsparseSpMatDescr_t matA = NULL;
-  HIPCHECK(hipsparseCreateCsr(&matA, N, N, nz, d_row, d_col, d_val,
-                                    HIPSPARSE_INDEX_32I, HIPSPARSE_INDEX_32I,
-                                    HIPSPARSE_INDEX_BASE_ZERO, HIPBLAS_R_32F));
-  hipsparseDnVecDescr_t vecx = NULL;
-  HIPCHECK(hipsparseCreateDnVec(&vecx, N, d_x, HIPBLAS_R_32F));
-  hipsparseDnVecDescr_t vecp = NULL;
-  HIPCHECK(hipsparseCreateDnVec(&vecp, N, d_p, HIPBLAS_R_32F));
-  hipsparseDnVecDescr_t vecAx = NULL;
-  HIPCHECK(hipsparseCreateDnVec(&vecAx, N, d_Ax, HIPBLAS_R_32F));
-
-  /* Allocate workspace for cuSPARSE */
-  size_t bufferSize = 0;
-  HIPCHECK(hipsparseSpMV_bufferSize(
-      cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE, &alpha, matA, vecx,
-      &beta, vecAx, HIPBLAS_R_32F, HIPSPARSE_SPMV_ALG_DEFAULT, &bufferSize));
-  void *buffer = NULL;
-  HIPCHECK(hipMalloc(&buffer, bufferSize));
-
-  hipsparseMatDescr_t descr = 0;
-  HIPCHECK(hipsparseCreateMatDescr(&descr));
-
-  HIPCHECK(hipsparseSetMatType(descr, HIPSPARSE_MATRIX_TYPE_GENERAL));
-  HIPCHECK(hipsparseSetMatIndexBase(descr, HIPSPARSE_INDEX_BASE_ZERO));
-
-  int numBlocks = 0, blockSize = 0;
-  HIPCHECK(
-      hipOccupancyMaxPotentialBlockSize(&numBlocks, &blockSize, initVectors));
-
-  HIPCHECK(hipMemcpyAsync(d_col, J, nz * sizeof(int),
-                                  hipMemcpyHostToDevice, stream1));
-  HIPCHECK(hipMemcpyAsync(d_row, I, (N + 1) * sizeof(int),
-                                  hipMemcpyHostToDevice, stream1));
-  HIPCHECK(hipMemcpyAsync(d_val, val, nz * sizeof(float),
-                                  hipMemcpyHostToDevice, stream1));
-
-  initVectors<<<numBlocks, blockSize, 0, stream1>>>(d_r, d_x, N);
-
-  alpha = 1.0;
-  alpham1 = -1.0;
-  beta = 0.0;
-
-  HIPCHECK(hipsparseSetStream(cusparseHandle, stream1));
-  HIPCHECK(hipsparseSpMV(cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE,
-                               &alpha, matA, vecx, &beta, vecAx, HIPBLAS_R_32F,
-                               HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
-
-  HIPCHECK(hipblasSetStream(cublasHandle, stream1));
-  HIPCHECK(hipblasSaxpy(cublasHandle, N, &alpham1, d_Ax, 1, d_r, 1));
-
-  HIPCHECK(
-      hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE));
-  HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
-
-  k = 1;
-  // First Iteration when k=1 starts
-  HIPCHECK(hipblasScopy(cublasHandle, N, d_r, 1, d_p, 1));
-  HIPCHECK(hipsparseSpMV(cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE,
-                               &alpha, matA, vecp, &beta, vecAx, HIPBLAS_R_32F,
-                               HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
-
-  HIPCHECK(hipblasSdot(cublasHandle, N, d_p, 1, d_Ax, 1, d_dot));
-
-  r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_dot, d_a);
-
-  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_a, d_p, 1, d_x, 1));
-
-  a_minus<<<1, 1, 0, stream1>>>(d_a, d_na);
-
-  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_na, d_Ax, 1, d_r, 1));
-
-  HIPCHECK(hipMemcpyAsync(d_r0, d_r1, sizeof(float),
-                                  hipMemcpyDeviceToDevice, stream1));
-
-  HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
-
-  HIPCHECK(hipMemcpyAsync(&r1, d_r1, sizeof(float),
-                                  hipMemcpyDeviceToHost, stream1));
-  HIPCHECK(hipStreamSynchronize(stream1));
-  printf("iteration = %3d, residual = %e\n", k, sqrt(r1));
-  // First Iteration when k=1 ends
-  k++;
-
-#if WITH_GRAPH
-  hipGraph_t initGraph;
-  HIPCHECK(hipStreamCreate(&streamForGraph));
-  HIPCHECK(hipblasSetStream(cublasHandle, stream1));
-  HIPCHECK(hipsparseSetStream(cusparseHandle, stream1));
-  HIPCHECK(hipStreamBeginCapture(stream1, hipStreamCaptureModeGlobal));
-
-  r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_r0, d_b);
-  hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
-  HIPCHECK(hipblasSscal(cublasHandle, N, d_b, d_p, 1));
-  hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_HOST);
-  HIPCHECK(hipblasSaxpy(cublasHandle, N, &alpha, d_r, 1, d_p, 1));
-  hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
-
-  HIPCHECK(
-      hipsparseSetPointerMode(cusparseHandle, HIPSPARSE_POINTER_MODE_HOST));
-  HIPCHECK(hipsparseSpMV(cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE,
-                               &alpha, matA, vecp, &beta, vecAx, HIPBLAS_R_32F,
-                               HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
-
-  HIPCHECK(hipMemsetAsync(d_dot, 0, sizeof(float), stream1));
-  HIPCHECK(hipblasSdot(cublasHandle, N, d_p, 1, d_Ax, 1, d_dot));
-
-  r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_dot, d_a);
-
-  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_a, d_p, 1, d_x, 1));
-
-  a_minus<<<1, 1, 0, stream1>>>(d_a, d_na);
-
-  HIPCHECK(hipblasSaxpy(cublasHandle, N, d_na, d_Ax, 1, d_r, 1));
-
-  HIPCHECK(hipMemcpyAsync(d_r0, d_r1, sizeof(float),
-                                  hipMemcpyDeviceToDevice, stream1));
-  HIPCHECK(hipMemsetAsync(d_r1, 0, sizeof(float), stream1));
-
-  HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
-
-  HIPCHECK(hipMemcpyAsync((float *)&r1, d_r1, sizeof(float),
-                                  hipMemcpyDeviceToHost, stream1));
-
-  HIPCHECK(hipStreamEndCapture(stream1, &initGraph));
-  hipGraphExec_t graphExec;
-  HIPCHECK(hipGraphInstantiate(&graphExec, initGraph, NULL, NULL, 0));
-#endif
-
-  HIPCHECK(hipblasSetStream(cublasHandle, stream1));
-  HIPCHECK(hipsparseSetStream(cusparseHandle, stream1));
-
-  while (r1 > tol * tol && k <= max_iter) {
-#if WITH_GRAPH
-    HIPCHECK(hipGraphLaunch(graphExec, streamForGraph));
-    HIPCHECK(hipStreamSynchronize(streamForGraph));
-#else
-    r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_r0, d_b);
-    hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
-    HIPCHECK(hipblasSscal(cublasHandle, N, d_b, d_p, 1));
-
-    hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_HOST);
-    HIPCHECK(hipblasSaxpy(cublasHandle, N, &alpha, d_r, 1, d_p, 1));
-
-    HIPCHECK(hipsparseSpMV(
-        cusparseHandle, HIPSPARSE_OPERATION_NON_TRANSPOSE, &alpha, matA, vecp,
-        &beta, vecAx, HIPBLAS_R_32F, HIPSPARSE_SPMV_ALG_DEFAULT, buffer));
-
-    hipblasSetPointerMode(cublasHandle, HIPBLAS_POINTER_MODE_DEVICE);
-    HIPCHECK(hipblasSdot(cublasHandle, N, d_p, 1, d_Ax, 1, d_dot));
-
-    r1_div_x<<<1, 1, 0, stream1>>>(d_r1, d_dot, d_a);
-
-    HIPCHECK(hipblasSaxpy(cublasHandle, N, d_a, d_p, 1, d_x, 1));
-
-    a_minus<<<1, 1, 0, stream1>>>(d_a, d_na);
-    HIPCHECK(hipblasSaxpy(cublasHandle, N, d_na, d_Ax, 1, d_r, 1));
-
-    HIPCHECK(hipMemcpyAsync(d_r0, d_r1, sizeof(float),
-                                    hipMemcpyDeviceToDevice, stream1));
-
-    HIPCHECK(hipblasSdot(cublasHandle, N, d_r, 1, d_r, 1, d_r1));
-    HIPCHECK(hipMemcpyAsync((float *)&r1, d_r1, sizeof(float),
-                                    hipMemcpyDeviceToHost, stream1));
-    HIPCHECK(hipStreamSynchronize(stream1));
-#endif
-    printf("iteration = %3d, residual = %e\n", k, sqrt(r1));
-    k++;
-  }
-
-#if WITH_GRAPH
-  HIPCHECK(hipMemcpyAsync(x, d_x, N * sizeof(float),
-                                  hipMemcpyDeviceToHost, streamForGraph));
-  HIPCHECK(hipStreamSynchronize(streamForGraph));
-#else
-  HIPCHECK(hipMemcpyAsync(x, d_x, N * sizeof(float),
-                                  hipMemcpyDeviceToHost, stream1));
-  HIPCHECK(hipStreamSynchronize(stream1));
-#endif
-
-  float rsum, diff, err = 0.0;
-
-  for (int i = 0; i < N; i++) {
-    rsum = 0.0;
-
-    for (int j = I[i]; j < I[i + 1]; j++) {
-      rsum += val[j] * x[J[j]];
-    }
-
-    diff = fabs(rsum - rhs[i]);
-
-    if (diff > err) {
-      err = diff;
-    }
-  }
-
-#if WITH_GRAPH
-  HIPCHECK(hipGraphExecDestroy(graphExec));
-  HIPCHECK(hipGraphDestroy(initGraph));
-  HIPCHECK(hipStreamDestroy(streamForGraph));
-#endif
-  HIPCHECK(hipStreamDestroy(stream1));
-  hipsparseDestroy(cusparseHandle);
-  hipblasDestroy(cublasHandle);
-
-  if (matA) {
-    HIPCHECK(hipsparseDestroySpMat(matA));
-  }
-  if (vecx) {
-    HIPCHECK(hipsparseDestroyDnVec(vecx));
-  }
-  if (vecAx) {
-    HIPCHECK(hipsparseDestroyDnVec(vecAx));
-  }
-  if (vecp) {
-    HIPCHECK(hipsparseDestroyDnVec(vecp));
-  }
-
-  HIPCHECK(hipHostFree(I));
-  HIPCHECK(hipHostFree(J));
-  HIPCHECK(hipHostFree(val));
-  HIPCHECK(hipHostFree(x));
-  free(rhs);
-  HIPCHECK(hipFree(d_col));
-  HIPCHECK(hipFree(d_row));
-  HIPCHECK(hipFree(d_val));
-  HIPCHECK(hipFree(d_x));
-  HIPCHECK(hipFree(d_r));
-  HIPCHECK(hipFree(d_p));
-  HIPCHECK(hipFree(d_Ax));
-
-  printf("Test Summary:  Error amount = %f\n", err);
-  exit((k <= max_iter) ? 0 : 1);
-}
-rors(hipsparseDestroySpMat(matA));
-  }
-  if (vecx) {
-    checkCudaErrors(hipsparseDestroyDnVec(vecx));
-  }
-  if (vecAx) {
-    checkCudaErrors(hipsparseDestroyDnVec(vecAx));
-  }
-  if (vecp) {
-    checkCudaErrors(hipsparseDestroyDnVec(vecp));
-  }
diff --git a/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip b/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip
index 490f397..e69de29 100644
--- a/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/cudaNvSci/imageKernels.cu.hip
@@ -1,120 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-#include <helper_image.h>
-
-// convert floating point rgba color to 32-bit integer
-__device__ unsigned int rgbaFloatToInt(float4 rgba) {
-  rgba.x = __saturatef(rgba.x);  // clamp to [0.0, 1.0]
-  rgba.y = __saturatef(rgba.y);
-  rgba.z = __saturatef(rgba.z);
-  rgba.w = __saturatef(rgba.w);
-  return ((unsigned int)(rgba.w * 255.0f) << 24) |
-         ((unsigned int)(rgba.z * 255.0f) << 16) |
-         ((unsigned int)(rgba.y * 255.0f) << 8) |
-         ((unsigned int)(rgba.x * 255.0f));
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Rotate an image using texture lookups
-//! @param outputData  output data in global memory
-////////////////////////////////////////////////////////////////////////////////
-static __global__ void transformKernel(unsigned int *outputData, int width,
-                                       int height, float theta,
-                                       hipTextureObject_t tex) {
-  // calculate normalized texture coordinates
-  unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
-  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  float u = (float)x - (float)width / 2;
-  float v = (float)y - (float)height / 2;
-  float tu = u * cosf(theta) - v * sinf(theta);
-  float tv = v * cosf(theta) + u * sinf(theta);
-
-  tu /= (float)width;
-  tv /= (float)height;
-
-  // read from texture and write to global memory
-  float4 pix = tex2D<float4>(tex, tu + 0.5f, tv + 0.5f);
-  unsigned int pixelInt = rgbaFloatToInt(pix);
-  outputData[y * width + x] = pixelInt;
-}
-
-static __global__ void rgbToGrayscaleKernel(unsigned int *rgbaImage,
-                                            size_t imageWidth,
-                                            size_t imageHeight) {
-  size_t gidX = blockDim.x * blockIdx.x + threadIdx.x;
-
-  uchar4 *pixArray = (uchar4 *)rgbaImage;
-
-  for (int pixId = gidX; pixId < imageWidth * imageHeight;
-       pixId += gridDim.x * blockDim.x) {
-    uchar4 dataA = pixArray[pixId];
-    unsigned char grayscale =
-        (unsigned char)(dataA.x * 0.3 + dataA.y * 0.59 + dataA.z * 0.11);
-    uchar4 dataB = make_uchar4(grayscale, grayscale, grayscale, 0);
-    pixArray[pixId] = dataB;
-  }
-}
-
-void launchGrayScaleKernel(unsigned int *d_rgbaImage,
-                           std::string image_filename, size_t imageWidth,
-                           size_t imageHeight, hipStream_t stream) {
-  int numThreadsPerBlock = 1024;
-  int numOfBlocks = (imageWidth * imageHeight) / numThreadsPerBlock;
-
-  rgbToGrayscaleKernel<<<numOfBlocks, numThreadsPerBlock, 0, stream>>>(
-      d_rgbaImage, imageWidth, imageHeight);
-
-  unsigned int *outputData;
-  HIPCHECK(hipHostMalloc((void**)&outputData, sizeof(unsigned int) * imageWidth * imageHeight));
-  HIPCHECK(hipMemcpyAsync(
-      outputData, d_rgbaImage, sizeof(unsigned int) * imageWidth * imageHeight,
-      hipMemcpyDeviceToHost, stream));
-  HIPCHECK(hipStreamSynchronize(stream));
-
-  char outputFilename[1024];
-  strcpy(outputFilename, image_filename.c_str());
-  strcpy(outputFilename + image_filename.length() - 4, "_out.ppm");
-  sdkSavePPM4ub(outputFilename, (unsigned char *)outputData, imageWidth,
-                imageHeight);
-  printf("Wrote '%s'\n", outputFilename);
-
-  HIPCHECK(hipHostFree(outputData));
-}
-
-void rotateKernel(hipTextureObject_t &texObj, const float angle,
-                  unsigned int *d_outputData, const int imageWidth,
-                  const int imageHeight, hipStream_t stream) {
-  dim3 dimBlock(8, 8, 1);
-  dim3 dimGrid(imageWidth / dimBlock.x, imageHeight / dimBlock.y, 1);
-
-  transformKernel<<<dimGrid, dimBlock, 0, stream>>>(d_outputData, imageWidth,
-                                                    imageHeight, angle, texObj);
-}
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip
index 730a6c1..e69de29 100644
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT/simpleCUFFT.cu.hip
@@ -1,291 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Example showing the use of CUFFT for fast 1D-convolution using FFT. */
-
-// includes, system
-#include <math.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-
-// includes, project
-#include <hip/hip_runtime.h>
-#include <hipfft.h>
-#include <hipfftXt.h>
-#include <helper_cuda.h>
-#include <helper_functions.h>
-
-// Complex data type
-typedef float2 Complex;
-static __device__ __host__ inline Complex ComplexAdd(Complex, Complex);
-static __device__ __host__ inline Complex ComplexScale(Complex, float);
-static __device__ __host__ inline Complex ComplexMul(Complex, Complex);
-static __global__ void ComplexPointwiseMulAndScale(Complex *, const Complex *,
-                                                   int, float);
-
-// Filtering functions
-void Convolve(const Complex *, int, const Complex *, int, Complex *);
-
-// Padding functions
-int PadData(const Complex *, Complex **, int, const Complex *, Complex **, int);
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-void runTest(int argc, char **argv);
-
-// The filter size is assumed to be a number smaller than the signal size
-#define SIGNAL_SIZE 50
-#define FILTER_KERNEL_SIZE 11
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) { runTest(argc, argv); }
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run a simple test for CUDA
-////////////////////////////////////////////////////////////////////////////////
-void runTest(int argc, char **argv) {
-  printf("[simpleCUFFT] is starting...\n");
-
-  findCudaDevice(argc, (const char **)argv);
-
-  // Allocate host memory for the signal
-  Complex *h_signal =
-      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * SIGNAL_SIZE));
-
-  // Initialize the memory for the signal
-  for (unsigned int i = 0; i < SIGNAL_SIZE; ++i) {
-    h_signal[i].x = rand() / static_cast<float>(RAND_MAX);
-    h_signal[i].y = 0;
-  }
-
-  // Allocate host memory for the filter
-  Complex *h_filter_kernel =
-      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * FILTER_KERNEL_SIZE));
-
-  // Initialize the memory for the filter
-  for (unsigned int i = 0; i < FILTER_KERNEL_SIZE; ++i) {
-    h_filter_kernel[i].x = rand() / static_cast<float>(RAND_MAX);
-    h_filter_kernel[i].y = 0;
-  }
-
-  // Pad signal and filter kernel
-  Complex *h_padded_signal;
-  Complex *h_padded_filter_kernel;
-  int new_size =
-      PadData(h_signal, &h_padded_signal, SIGNAL_SIZE, h_filter_kernel,
-              &h_padded_filter_kernel, FILTER_KERNEL_SIZE);
-  int mem_size = sizeof(Complex) * new_size;
-
-  // Allocate device memory for signal
-  Complex *d_signal;
-  HIPCHECK(hipMalloc(reinterpret_cast<void **>(&d_signal), mem_size));
-  // Copy host memory to device
-  HIPCHECK(
-      hipMemcpy(d_signal, h_padded_signal, mem_size, hipMemcpyHostToDevice));
-
-  // Allocate device memory for filter kernel
-  Complex *d_filter_kernel;
-  HIPCHECK(
-      hipMalloc(reinterpret_cast<void **>(&d_filter_kernel), mem_size));
-
-  // Copy host memory to device
-  HIPCHECK(hipMemcpy(d_filter_kernel, h_padded_filter_kernel, mem_size,
-                             hipMemcpyHostToDevice));
-
-  // CUFFT plan simple API
-  hipfftHandle plan;
-  HIPCHECK(hipfftPlan1d(&plan, new_size, HIPFFT_C2C, 1));
-
-  // CUFFT plan advanced API
-  hipfftHandle plan_adv;
-  size_t workSize;
-  long long int new_size_long = new_size;
-
-  HIPCHECK(hipfftCreate(&plan_adv));
-  HIPCHECK(cufftXtMakePlanMany(plan_adv, 1, &new_size_long, NULL, 1, 1,
-                                      HIPBLAS_C_32F, NULL, 1, 1, HIPBLAS_C_32F, 1,
-                                      &workSize, HIPBLAS_C_32F));
-  printf("Temporary buffer size %li bytes\n", workSize);
-
-  // Transform signal and kernel
-  printf("Transforming signal cufftExecC2C\n");
-  HIPCHECK(hipfftExecC2C(plan, reinterpret_cast<hipfftComplex *>(d_signal),
-                               reinterpret_cast<hipfftComplex *>(d_signal),
-                               HIPFFT_FORWARD));
-  HIPCHECK(hipfftExecC2C(
-      plan_adv, reinterpret_cast<hipfftComplex *>(d_filter_kernel),
-      reinterpret_cast<hipfftComplex *>(d_filter_kernel), HIPFFT_FORWARD));
-
-  // Multiply the coefficients together and normalize the result
-  printf("Launching ComplexPointwiseMulAndScale<<< >>>\n");
-  ComplexPointwiseMulAndScale<<<32, 256>>>(d_signal, d_filter_kernel, new_size,
-                                           1.0f / new_size);
-
-  // Check if kernel execution generated and error
-  getLastCudaError("Kernel execution failed [ ComplexPointwiseMulAndScale ]");
-
-  // Transform signal back
-  printf("Transforming signal back cufftExecC2C\n");
-  HIPCHECK(hipfftExecC2C(plan, reinterpret_cast<hipfftComplex *>(d_signal),
-                               reinterpret_cast<hipfftComplex *>(d_signal),
-                               HIPFFT_BACKWARD));
-
-  // Copy device memory to host
-  Complex *h_convolved_signal = h_padded_signal;
-  HIPCHECK(hipMemcpy(h_convolved_signal, d_signal, mem_size,
-                             hipMemcpyDeviceToHost));
-
-  // Allocate host memory for the convolution result
-  Complex *h_convolved_signal_ref =
-      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * SIGNAL_SIZE));
-
-  // Convolve on the host
-  Convolve(h_signal, SIGNAL_SIZE, h_filter_kernel, FILTER_KERNEL_SIZE,
-           h_convolved_signal_ref);
-
-  // check result
-  bool bTestResult = sdkCompareL2fe(
-      reinterpret_cast<float *>(h_convolved_signal_ref),
-      reinterpret_cast<float *>(h_convolved_signal), 2 * SIGNAL_SIZE, 1e-5f);
-
-  // Destroy CUFFT context
-  HIPCHECK(hipfftDestroy(plan));
-  HIPCHECK(hipfftDestroy(plan_adv));
-
-  // cleanup memory
-  free(h_signal);
-  free(h_filter_kernel);
-  free(h_padded_signal);
-  free(h_padded_filter_kernel);
-  free(h_convolved_signal_ref);
-  HIPCHECK(hipFree(d_signal));
-  HIPCHECK(hipFree(d_filter_kernel));
-
-  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
-
-// Pad data
-int PadData(const Complex *signal, Complex **padded_signal, int signal_size,
-            const Complex *filter_kernel, Complex **padded_filter_kernel,
-            int filter_kernel_size) {
-  int minRadius = filter_kernel_size / 2;
-  int maxRadius = filter_kernel_size - minRadius;
-  int new_size = signal_size + maxRadius;
-
-  // Pad signal
-  Complex *new_data =
-      reinterpret_cast<Complex *>(malloc(sizeof(Complex) * new_size));
-  memcpy(new_data + 0, signal, signal_size * sizeof(Complex));
-  memset(new_data + signal_size, 0, (new_size - signal_size) * sizeof(Complex));
-  *padded_signal = new_data;
-
-  // Pad filter
-  new_data = reinterpret_cast<Complex *>(malloc(sizeof(Complex) * new_size));
-  memcpy(new_data + 0, filter_kernel + minRadius, maxRadius * sizeof(Complex));
-  memset(new_data + maxRadius, 0,
-         (new_size - filter_kernel_size) * sizeof(Complex));
-  memcpy(new_data + new_size - minRadius, filter_kernel,
-         minRadius * sizeof(Complex));
-  *padded_filter_kernel = new_data;
-
-  return new_size;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Filtering operations
-////////////////////////////////////////////////////////////////////////////////
-
-// Computes convolution on the host
-void Convolve(const Complex *signal, int signal_size,
-              const Complex *filter_kernel, int filter_kernel_size,
-              Complex *filtered_signal) {
-  int minRadius = filter_kernel_size / 2;
-  int maxRadius = filter_kernel_size - minRadius;
-
-  // Loop over output element indices
-  for (int i = 0; i < signal_size; ++i) {
-    filtered_signal[i].x = filtered_signal[i].y = 0;
-
-    // Loop over convolution indices
-    for (int j = -maxRadius + 1; j <= minRadius; ++j) {
-      int k = i + j;
-
-      if (k >= 0 && k < signal_size) {
-        filtered_signal[i] =
-            ComplexAdd(filtered_signal[i],
-                       ComplexMul(signal[k], filter_kernel[minRadius - j]));
-      }
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Complex operations
-////////////////////////////////////////////////////////////////////////////////
-
-// Complex addition
-static __device__ __host__ inline Complex ComplexAdd(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x + b.x;
-  c.y = a.y + b.y;
-  return c;
-}
-
-// Complex scale
-static __device__ __host__ inline Complex ComplexScale(Complex a, float s) {
-  Complex c;
-  c.x = s * a.x;
-  c.y = s * a.y;
-  return c;
-}
-
-// Complex multiplication
-static __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x * b.x - a.y * b.y;
-  c.y = a.x * b.y + a.y * b.x;
-  return c;
-}
-
-// Complex pointwise multiplication
-static __global__ void ComplexPointwiseMulAndScale(Complex *a, const Complex *b,
-                                                   int size, float scale) {
-  const int numThreads = blockDim.x * gridDim.x;
-  const int threadID = blockIdx.x * blockDim.x + threadIdx.x;
-
-  for (int i = threadID; i < size; i += numThreads) {
-    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
-  }
-}
-i = threadID; i < size; i += numThreads) {
-    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
-  }
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip
index 6ad007d..e69de29 100644
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_2d_MGPU/simpleCUFFT_2d_MGPU.cu.hip
@@ -1,384 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-////////////////////////////////////////////////////////////////////////////////
-//
-//  simpleCUFFT_2d_MGPU.cu
-//
-//  This sample code demonstrate the use of CUFFT library for 2D data on multiple GPU.
-//  Example showing the use of CUFFT for solving 2D-POISSON equation using FFT on multiple GPU.
-//  For reference we have used the equation given in http://www.bu.edu/pasi/files/2011/07/
-//  Lecture83.pdf
-//
-////////////////////////////////////////////////////////////////////////////////
-
-
-// System includes
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-#include <string.h>
-#include <math.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-//CUFFT Header file
-#include <hipfftXt.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-// Complex data type
-typedef float2 Complex;
-
-// Data configuration
-const int GPU_COUNT = 2;
-const int BSZ_Y = 4;
-const int BSZ_X = 4;
-
-// Forward Declaration
-void solvePoissonEquation(cudaLibXtDesc *, cudaLibXtDesc *, float **, int, int);
-
-__global__ void solvePoisson(hipfftComplex *, hipfftComplex *, float *, int, int,
-                             int n_gpu);
-
-///////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  printf(
-      "\nPoisson equation using CUFFT library on Multiple GPUs is "
-      "starting...\n\n");
-
-  int GPU_N;
-  HIPCHECK(hipGetDeviceCount(&GPU_N));
-
-  if (GPU_N < GPU_COUNT) {
-    printf("No. of GPU on node %d\n", GPU_N);
-    printf("Two GPUs are required to run simpleCUFFT_2d_MGPU sample code\n");
-    exit(EXIT_WAIVED);
-  }
-
-  int *major_minor = (int *)malloc(sizeof(int) * GPU_N * 2);
-  int found2IdenticalGPUs = 0;
-  int nGPUs = 2;
-  int *whichGPUs;
-  whichGPUs = (int *)malloc(sizeof(int) * nGPUs);
-
-  for (int i = 0; i < GPU_N; i++) {
-    hipDeviceProp_t deviceProp;
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
-    major_minor[i * 2] = deviceProp.major;
-    major_minor[i * 2 + 1] = deviceProp.minor;
-    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n", i,
-           deviceProp.name, deviceProp.major, deviceProp.minor);
-  }
-
-  for (int i = 0; i < GPU_N; i++) {
-    for (int j = i + 1; j < GPU_N; j++) {
-      if ((major_minor[i * 2] == major_minor[j * 2]) &&
-          (major_minor[i * 2 + 1] == major_minor[j * 2 + 1])) {
-        whichGPUs[0] = i;
-        whichGPUs[1] = j;
-        found2IdenticalGPUs = 1;
-        break;
-      }
-    }
-    if (found2IdenticalGPUs) {
-      break;
-    }
-  }
-
-  free(major_minor);
-  if (!found2IdenticalGPUs) {
-    printf(
-        "No Two GPUs with same architecture found\nWaiving simpleCUFFT_2d_MGPU "
-        "sample\n");
-    exit(EXIT_WAIVED);
-  }
-
-  int N = 64;
-  float xMAX = 1.0f, xMIN = 0.0f, yMIN = 0.0f, h = (xMAX - xMIN) / ((float)N),
-        s = 0.1f, s2 = s * s;
-  float *x, *y, *f, *u_a, r2;
-
-  x = (float *)malloc(sizeof(float) * N * N);
-  y = (float *)malloc(sizeof(float) * N * N);
-  f = (float *)malloc(sizeof(float) * N * N);
-  u_a = (float *)malloc(sizeof(float) * N * N);
-
-  for (int j = 0; j < N; j++)
-    for (int i = 0; i < N; i++) {
-      x[N * j + i] = xMIN + i * h;
-      y[N * j + i] = yMIN + j * h;
-      r2 = (x[N * j + i] - 0.5f) * (x[N * j + i] - 0.5f) +
-           (y[N * j + i] - 0.5f) * (y[N * j + i] - 0.5f);
-      f[N * j + i] = (r2 - 2 * s2) / (s2 * s2) * exp(-r2 / (2 * s2));
-      u_a[N * j + i] = exp(-r2 / (2 * s2));  // analytical solution
-    }
-
-  float *k, *d_k[GPU_COUNT];
-  k = (float *)malloc(sizeof(float) * N);
-  for (int i = 0; i <= N / 2; i++) {
-    k[i] = i * 2 * (float)M_PI;
-  }
-  for (int i = N / 2 + 1; i < N; i++) {
-    k[i] = (i - N) * 2 * (float)M_PI;
-  }
-
-  // Create a complex variable on host
-  Complex *h_f = (Complex *)malloc(sizeof(Complex) * N * N);
-
-  // Initialize the memory for the signal
-  for (int i = 0; i < (N * N); i++) {
-    h_f[i].x = f[i];
-    h_f[i].y = 0.0f;
-  }
-
-  // cufftCreate() - Create an empty plan
-  hipfftResult result;
-  hipfftHandle planComplex;
-  result = hipfftCreate(&planComplex);
-  if (result != HIPFFT_SUCCESS) {
-    printf("hipfftCreate failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // cufftXtSetGPUs() - Define which GPUs to use
-  result = cufftXtSetGPUs(planComplex, nGPUs, whichGPUs);
-
-  if (result == HIPFFT_INVALID_DEVICE) {
-    printf("This sample requires two GPUs on the same board.\n");
-    printf("No such board was found. Waiving sample.\n");
-    exit(EXIT_WAIVED);
-  } else if (result != HIPFFT_SUCCESS) {
-    printf("hipfftXtSetGPUs failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // Print the device information to run the code
-  printf("\nRunning on GPUs\n");
-  for (int i = 0; i < 2; i++) {
-    hipDeviceProp_t deviceProp;
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, whichGPUs[i]));
-    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n",
-           whichGPUs[i], deviceProp.name, deviceProp.major, deviceProp.minor);
-  }
-
-  size_t *worksize;
-  worksize = (size_t *)malloc(sizeof(size_t) * nGPUs);
-
-  // cufftMakePlan2d() - Create the plan
-  result = hipfftMakePlan2d(planComplex, N, N, HIPFFT_C2C, worksize);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*MakePlan* failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  for (int i = 0; i < nGPUs; i++) {
-    hipSetDevice(whichGPUs[i]);
-    hipMalloc((void **)&d_k[i], sizeof(float) * N);
-    hipMemcpy(d_k[i], k, sizeof(float) * N, hipMemcpyHostToDevice);
-  }
-
-  // Create a variable on device
-  // d_f - variable on device to store the input data
-  // d_d_f - variable that store the natural order of d_f data
-  // d_out - device output
-  cudaLibXtDesc *d_f, *d_d_f, *d_out;
-
-  // cufftXtMalloc() - Malloc data on multiple GPUs
-
-  result = cufftXtMalloc(planComplex, (cudaLibXtDesc **)&d_f,
-                         CUFFT_XT_FORMAT_INPLACE);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtMalloc failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  result = cufftXtMalloc(planComplex, (cudaLibXtDesc **)&d_d_f,
-                         CUFFT_XT_FORMAT_INPLACE);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtMalloc failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  result = cufftXtMalloc(planComplex, (cudaLibXtDesc **)&d_out,
-                         CUFFT_XT_FORMAT_INPLACE);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtMalloc failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // cufftXtMemcpy() - Copy the data from host to device
-  result = cufftXtMemcpy(planComplex, d_f, h_f, CUFFT_COPY_HOST_TO_DEVICE);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtMemcpy failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // cufftXtExecDescriptorC2C() - Execute FFT on data on multiple GPUs
-  printf("Forward 2d FFT on multiple GPUs\n");
-  result = cufftXtExecDescriptorC2C(planComplex, d_f, d_f, HIPFFT_FORWARD);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtExecC2C  failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // cufftXtMemcpy() - Copy the data to natural order on GPUs
-  result = cufftXtMemcpy(planComplex, d_d_f, d_f, CUFFT_COPY_DEVICE_TO_DEVICE);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtMemcpy failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  printf("Solve Poisson Equation\n");
-  solvePoissonEquation(d_d_f, d_out, d_k, N, nGPUs);
-
-  printf("Inverse 2d FFT on multiple GPUs\n");
-  // cufftXtExecDescriptorC2C() - Execute inverse  FFT on data on multiple GPUs
-  result = cufftXtExecDescriptorC2C(planComplex, d_out, d_out, HIPFFT_BACKWARD);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtExecC2C  failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // Create a variable on host to copy the data from device
-  // h_d_out - variable store the output of device
-  Complex *h_d_out = (Complex *)malloc(sizeof(Complex) * N * N);
-
-  // cufftXtMemcpy() - Copy data from multiple GPUs to host
-  result =
-      cufftXtMemcpy(planComplex, h_d_out, d_out, CUFFT_COPY_DEVICE_TO_HOST);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtMemcpy failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  float *out = (float *)malloc(sizeof(float) * N * N);
-  float constant = h_d_out[0].x / N * N;
-  for (int i = 0; i < N * N; i++) {
-    // subtract u[0] to force the arbitrary constant to be 0
-    out[i] = (h_d_out[i].x / (N * N)) - constant;
-  }
-
-  // cleanup memory
-
-  free(h_f);
-  free(k);
-  free(out);
-  free(h_d_out);
-  free(x);
-  free(whichGPUs);
-  free(y);
-  free(f);
-  free(u_a);
-  free(worksize);
-
-  // cudaXtFree() - Free GPU memory
-  for (int i = 0; i < GPU_COUNT; i++) {
-    hipFree(d_k[i]);
-  }
-  result = cufftXtFree(d_out);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtFree failed\n");
-    exit(EXIT_FAILURE);
-  }
-  result = cufftXtFree(d_f);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtFree failed\n");
-    exit(EXIT_FAILURE);
-  }
-  result = cufftXtFree(d_d_f);
-  if (result != HIPFFT_SUCCESS) {
-    printf("*XtFree failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // cufftDestroy() - Destroy FFT plan
-  result = hipfftDestroy(planComplex);
-  if (result != HIPFFT_SUCCESS) {
-    printf("hipfftDestroy failed: code %d\n", (int)result);
-    exit(EXIT_FAILURE);
-  }
-
-  exit(EXIT_SUCCESS);
-}
-
-////////////////////////////////////////////////////////////////////////////////////
-// Launch kernel on  multiple GPU
-///////////////////////////////////////////////////////////////////////////////////
-void solvePoissonEquation(cudaLibXtDesc *d_ft, cudaLibXtDesc *d_ft_k, float **k,
-                          int N, int nGPUs) {
-  int device;
-  dim3 dimGrid(int(N / BSZ_X), int((N / 2) / BSZ_Y));
-  dim3 dimBlock(BSZ_X, BSZ_Y);
-
-  for (int i = 0; i < nGPUs; i++) {
-    device = d_ft_k->descriptor->GPUs[i];
-    hipSetDevice(device);
-    solvePoisson<<<dimGrid, dimBlock>>>(
-        (hipfftComplex *)d_ft->descriptor->data[i],
-        (hipfftComplex *)d_ft_k->descriptor->data[i], k[i], N, i, nGPUs);
-  }
-
-  // Wait for device to finish all operation
-  for (int i = 0; i < nGPUs; i++) {
-    device = d_ft_k->descriptor->GPUs[i];
-    hipSetDevice(device);
-    hipDeviceSynchronize();
-
-    // Check if kernel execution generated and error
-    getLastCudaError("Kernel execution failed [ solvePoisson ]");
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Kernel for Solving Poisson equation on GPU
-////////////////////////////////////////////////////////////////////////////////
-__global__ void solvePoisson(hipfftComplex *ft, hipfftComplex *ft_k, float *k,
-                             int N, int gpu_id, int n_gpu) {
-  int i = threadIdx.x + blockIdx.x * blockDim.x;
-  int j = threadIdx.y + blockIdx.y * blockDim.y;
-  int index = j * N + i;
-  if (i < N && j < N / n_gpu) {
-    float k2 =
-        k[i] * k[i] + k[j + gpu_id * N / n_gpu] * k[j + gpu_id * N / n_gpu];
-    if (i == 0 && j == 0 && gpu_id == 0) {
-      k2 = 1.0f;
-    }
-
-    ft_k[index].x = -ft[index].x * 1 / k2;
-    ft_k[index].y = -ft[index].y * 1 / k2;
-  }
-}
-x].y * 1 / k2;
-  }
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip
index 1b699c8..e69de29 100644
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_MGPU/simpleCUFFT_MGPU.cu.hip
@@ -1,406 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* Example showing the use of CUFFT for fast 1D-convolution using FFT. */
-
-// System includes
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-
-#include <string.h>
-#include <math.h>
-
-// CUDA runtime
-#include <hip/hip_runtime.h>
-
-//CUFFT Header file
-#include <hipfftXt.h>
-
-// helper functions and utilities to work with CUDA
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-// Complex data type
-typedef float2 Complex;
-
-static __device__ __host__ inline Complex ComplexAdd(Complex, Complex);
-static __device__ __host__ inline Complex ComplexScale(Complex, float);
-static __device__ __host__ inline Complex ComplexMul(Complex, Complex);
-static __global__ void ComplexPointwiseMulAndScale(hipfftComplex *,
-                                                   hipfftComplex *, int, float);
-
-// Kernel for GPU
-void multiplyCoefficient(cudaLibXtDesc *, cudaLibXtDesc *, int, float, int);
-
-// Filtering functions
-void Convolve(const Complex *, int, const Complex *, int, Complex *);
-
-// Padding functions
-int PadData(const Complex *, Complex **, int, const Complex *, Complex **, int);
-
-////////////////////////////////////////////////////////////////////////////////
-// Data configuration
-// The filter size is assumed to be a number smaller than the signal size
-///////////////////////////////////////////////////////////////////////////////
-const int SIGNAL_SIZE = 1018;
-const int FILTER_KERNEL_SIZE = 11;
-const int GPU_COUNT = 2;
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  printf("\n[simpleCUFFT_MGPU] is starting...\n\n");
-
-  int GPU_N;
-  HIPCHECK(hipGetDeviceCount(&GPU_N));
-
-  if (GPU_N < GPU_COUNT) {
-    printf("No. of GPU on node %d\n", GPU_N);
-    printf("Two GPUs are required to run simpleCUFFT_MGPU sample code\n");
-    exit(EXIT_WAIVED);
-  }
-
-  int *major_minor = (int *)malloc(sizeof(int) * GPU_N * 2);
-  int found2IdenticalGPUs = 0;
-  int nGPUs = 2;
-  int *whichGPUs;
-  whichGPUs = (int *)malloc(sizeof(int) * nGPUs);
-
-  for (int i = 0; i < GPU_N; i++) {
-    hipDeviceProp_t deviceProp;
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, i));
-    major_minor[i * 2] = deviceProp.major;
-    major_minor[i * 2 + 1] = deviceProp.minor;
-    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n", i,
-           deviceProp.name, deviceProp.major, deviceProp.minor);
-  }
-
-  for (int i = 0; i < GPU_N; i++) {
-    for (int j = i + 1; j < GPU_N; j++) {
-      if ((major_minor[i * 2] == major_minor[j * 2]) &&
-          (major_minor[i * 2 + 1] == major_minor[j * 2 + 1])) {
-        whichGPUs[0] = i;
-        whichGPUs[1] = j;
-        found2IdenticalGPUs = 1;
-        break;
-      }
-    }
-    if (found2IdenticalGPUs) {
-      break;
-    }
-  }
-
-  free(major_minor);
-  if (!found2IdenticalGPUs) {
-    printf(
-        "No Two GPUs with same architecture found\nWaiving simpleCUFFT_2d_MGPU "
-        "sample\n");
-    exit(EXIT_WAIVED);
-  }
-
-  // Allocate host memory for the signal
-  Complex *h_signal = (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
-
-  // Initialize the memory for the signal
-  for (int i = 0; i < SIGNAL_SIZE; ++i) {
-    h_signal[i].x = rand() / (float)RAND_MAX;
-    h_signal[i].y = 0;
-  }
-
-  // Allocate host memory for the filter
-  Complex *h_filter_kernel =
-      (Complex *)malloc(sizeof(Complex) * FILTER_KERNEL_SIZE);
-
-  // Initialize the memory for the filter
-  for (int i = 0; i < FILTER_KERNEL_SIZE; ++i) {
-    h_filter_kernel[i].x = rand() / (float)RAND_MAX;
-    h_filter_kernel[i].y = 0;
-  }
-
-  // Pad signal and filter kernel
-  Complex *h_padded_signal;
-  Complex *h_padded_filter_kernel;
-  int new_size =
-      PadData(h_signal, &h_padded_signal, SIGNAL_SIZE, h_filter_kernel,
-              &h_padded_filter_kernel, FILTER_KERNEL_SIZE);
-
-  // cufftCreate() - Create an empty plan
-  hipfftResult result;
-  hipfftHandle plan_input;
-  HIPCHECK(hipfftCreate(&plan_input));
-
-  // cufftXtSetGPUs() - Define which GPUs to use
-  result = cufftXtSetGPUs(plan_input, nGPUs, whichGPUs);
-
-  if (result == HIPFFT_INVALID_DEVICE) {
-    printf("This sample requires two GPUs on the same board.\n");
-    printf("No such board was found. Waiving sample.\n");
-    exit(EXIT_WAIVED);
-  } else if (result != HIPFFT_SUCCESS) {
-    printf("hipfftXtSetGPUs failed\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // Print the device information to run the code
-  printf("\nRunning on GPUs\n");
-  for (int i = 0; i < nGPUs; i++) {
-    hipDeviceProp_t deviceProp;
-    HIPCHECK(hipGetDeviceProperties(&deviceProp, whichGPUs[i]));
-    printf("GPU Device %d: \"%s\" with compute capability %d.%d\n",
-           whichGPUs[i], deviceProp.name, deviceProp.major, deviceProp.minor);
-  }
-
-  size_t *worksize;
-  worksize = (size_t *)malloc(sizeof(size_t) * nGPUs);
-
-  // cufftMakePlan1d() - Create the plan
-  HIPCHECK(
-      hipfftMakePlan1d(plan_input, new_size, HIPFFT_C2C, 1, worksize));
-
-  // cufftXtMalloc() - Malloc data on multiple GPUs
-  cudaLibXtDesc *d_signal;
-  HIPCHECK(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_signal,
-                                CUFFT_XT_FORMAT_INPLACE));
-  cudaLibXtDesc *d_out_signal;
-  HIPCHECK(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_out_signal,
-                                CUFFT_XT_FORMAT_INPLACE));
-  cudaLibXtDesc *d_filter_kernel;
-  HIPCHECK(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_filter_kernel,
-                                CUFFT_XT_FORMAT_INPLACE));
-  cudaLibXtDesc *d_out_filter_kernel;
-  HIPCHECK(cufftXtMalloc(plan_input,
-                                (cudaLibXtDesc **)&d_out_filter_kernel,
-                                CUFFT_XT_FORMAT_INPLACE));
-
-  // cufftXtMemcpy() - Copy data from host to multiple GPUs
-  HIPCHECK(cufftXtMemcpy(plan_input, d_signal, h_padded_signal,
-                                CUFFT_COPY_HOST_TO_DEVICE));
-  HIPCHECK(cufftXtMemcpy(plan_input, d_filter_kernel,
-                                h_padded_filter_kernel,
-                                CUFFT_COPY_HOST_TO_DEVICE));
-
-  // cufftXtExecDescriptorC2C() - Execute FFT on data on multiple GPUs
-  HIPCHECK(
-      cufftXtExecDescriptorC2C(plan_input, d_signal, d_signal, HIPFFT_FORWARD));
-  HIPCHECK(cufftXtExecDescriptorC2C(plan_input, d_filter_kernel,
-                                           d_filter_kernel, HIPFFT_FORWARD));
-
-  // cufftXtMemcpy() - Copy the data to natural order on GPUs
-  HIPCHECK(cufftXtMemcpy(plan_input, d_out_signal, d_signal,
-                                CUFFT_COPY_DEVICE_TO_DEVICE));
-  HIPCHECK(cufftXtMemcpy(plan_input, d_out_filter_kernel,
-                                d_filter_kernel, CUFFT_COPY_DEVICE_TO_DEVICE));
-
-  printf("\n\nValue of Library Descriptor\n");
-  printf("Number of GPUs %d\n", d_out_signal->descriptor->nGPUs);
-  printf("Device id  %d %d\n", d_out_signal->descriptor->GPUs[0],
-         d_out_signal->descriptor->GPUs[1]);
-  printf("Data size on GPU %ld %ld\n",
-         (long)(d_out_signal->descriptor->size[0] / sizeof(hipfftComplex)),
-         (long)(d_out_signal->descriptor->size[1] / sizeof(hipfftComplex)));
-
-  // Multiply the coefficients together and normalize the result
-  printf("Launching ComplexPointwiseMulAndScale<<< >>>\n");
-  multiplyCoefficient(d_out_signal, d_out_filter_kernel, new_size,
-                      1.0f / new_size, nGPUs);
-
-  // cufftXtExecDescriptorC2C() - Execute inverse  FFT on data on multiple GPUs
-  printf("Transforming signal back cufftExecC2C\n");
-  HIPCHECK(cufftXtExecDescriptorC2C(plan_input, d_out_signal,
-                                           d_out_signal, HIPFFT_BACKWARD));
-
-  // Create host pointer pointing to padded signal
-  Complex *h_convolved_signal = h_padded_signal;
-
-  // Allocate host memory for the convolution result
-  Complex *h_convolved_signal_ref =
-      (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
-
-  // cufftXtMemcpy() - Copy data from multiple GPUs to host
-  HIPCHECK(cufftXtMemcpy(plan_input, h_convolved_signal, d_out_signal,
-                                CUFFT_COPY_DEVICE_TO_HOST));
-
-  // Convolve on the host
-  Convolve(h_signal, SIGNAL_SIZE, h_filter_kernel, FILTER_KERNEL_SIZE,
-           h_convolved_signal_ref);
-
-  // Compare CPU and GPU result
-  bool bTestResult =
-      sdkCompareL2fe((float *)h_convolved_signal_ref,
-                     (float *)h_convolved_signal, 2 * SIGNAL_SIZE, 1e-5f);
-  printf("\nvalue of TestResult %d\n", bTestResult);
-
-  // Cleanup memory
-  free(whichGPUs);
-  free(worksize);
-  free(h_signal);
-  free(h_filter_kernel);
-  free(h_padded_signal);
-  free(h_padded_filter_kernel);
-  free(h_convolved_signal_ref);
-
-  // cudaXtFree() - Free GPU memory
-  HIPCHECK(cufftXtFree(d_signal));
-  HIPCHECK(cufftXtFree(d_filter_kernel));
-  HIPCHECK(cufftXtFree(d_out_signal));
-  HIPCHECK(cufftXtFree(d_out_filter_kernel));
-
-  // cufftDestroy() - Destroy FFT plan
-  HIPCHECK(hipfftDestroy(plan_input));
-
-  exit(bTestResult ? EXIT_SUCCESS : EXIT_FAILURE);
-}
-
-///////////////////////////////////////////////////////////////////////////////////
-// Function for padding original data
-//////////////////////////////////////////////////////////////////////////////////
-int PadData(const Complex *signal, Complex **padded_signal, int signal_size,
-            const Complex *filter_kernel, Complex **padded_filter_kernel,
-            int filter_kernel_size) {
-  int minRadius = filter_kernel_size / 2;
-  int maxRadius = filter_kernel_size - minRadius;
-  int new_size = signal_size + maxRadius;
-
-  // Pad signal
-  Complex *new_data = (Complex *)malloc(sizeof(Complex) * new_size);
-  memcpy(new_data + 0, signal, signal_size * sizeof(Complex));
-  memset(new_data + signal_size, 0, (new_size - signal_size) * sizeof(Complex));
-  *padded_signal = new_data;
-
-  // Pad filter
-  new_data = (Complex *)malloc(sizeof(Complex) * new_size);
-  memcpy(new_data + 0, filter_kernel + minRadius, maxRadius * sizeof(Complex));
-  memset(new_data + maxRadius, 0,
-         (new_size - filter_kernel_size) * sizeof(Complex));
-  memcpy(new_data + new_size - minRadius, filter_kernel,
-         minRadius * sizeof(Complex));
-  *padded_filter_kernel = new_data;
-
-  return new_size;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Filtering operations - Computing Convolution on the host
-////////////////////////////////////////////////////////////////////////////////
-void Convolve(const Complex *signal, int signal_size,
-              const Complex *filter_kernel, int filter_kernel_size,
-              Complex *filtered_signal) {
-  int minRadius = filter_kernel_size / 2;
-  int maxRadius = filter_kernel_size - minRadius;
-
-  // Loop over output element indices
-  for (int i = 0; i < signal_size; ++i) {
-    filtered_signal[i].x = filtered_signal[i].y = 0;
-
-    // Loop over convolution indices
-    for (int j = -maxRadius + 1; j <= minRadius; ++j) {
-      int k = i + j;
-
-      if (k >= 0 && k < signal_size) {
-        filtered_signal[i] =
-            ComplexAdd(filtered_signal[i],
-                       ComplexMul(signal[k], filter_kernel[minRadius - j]));
-      }
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//  Launch Kernel on multiple GPU
-////////////////////////////////////////////////////////////////////////////////
-void multiplyCoefficient(cudaLibXtDesc *d_signal,
-                         cudaLibXtDesc *d_filter_kernel, int new_size,
-                         float val, int nGPUs) {
-  int device;
-  // Launch the ComplexPointwiseMulAndScale<<< >>> kernel on multiple GPU
-  for (int i = 0; i < nGPUs; i++) {
-    device = d_signal->descriptor->GPUs[i];
-
-    // Set device
-    HIPCHECK(hipSetDevice(device));
-
-    // Perform GPU computations
-    ComplexPointwiseMulAndScale<<<32, 256>>>(
-        (hipfftComplex *)d_signal->descriptor->data[i],
-        (hipfftComplex *)d_filter_kernel->descriptor->data[i],
-        int(d_signal->descriptor->size[i] / sizeof(hipfftComplex)), val);
-  }
-
-  // Wait for device to finish all operation
-  for (int i = 0; i < nGPUs; i++) {
-    device = d_signal->descriptor->GPUs[i];
-    HIPCHECK(hipSetDevice(device));
-    hipDeviceSynchronize();
-    // Check if kernel execution generated and error
-    getLastCudaError("Kernel execution failed [ ComplexPointwiseMulAndScale ]");
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Complex operations
-////////////////////////////////////////////////////////////////////////////////
-
-// Complex addition
-static __device__ __host__ inline Complex ComplexAdd(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x + b.x;
-  c.y = a.y + b.y;
-  return c;
-}
-
-// Complex scale
-static __device__ __host__ inline Complex ComplexScale(Complex a, float s) {
-  Complex c;
-  c.x = s * a.x;
-  c.y = s * a.y;
-  return c;
-}
-
-// Complex multiplication
-static __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x * b.x - a.y * b.y;
-  c.y = a.x * b.y + a.y * b.x;
-  return c;
-}
-// Complex pointwise multiplication
-static __global__ void ComplexPointwiseMulAndScale(hipfftComplex *a,
-                                                   hipfftComplex *b, int size,
-                                                   float scale) {
-  const int numThreads = blockDim.x * gridDim.x;
-  const int threadID = blockIdx.x * blockDim.x + threadIdx.x;
-  for (int i = threadID; i < size; i += numThreads) {
-    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
-  }
-}
-t threadID = blockIdx.x * blockDim.x + threadIdx.x;
-  for (int i = threadID; i < size; i += numThreads) {
-    a[i] = ComplexScale(ComplexMul(a[i], b[i]), scale);
-  }
diff --git a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip
index 3e24586..e69de29 100644
--- a/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip
+++ b/src/samples/Samples/4_CUDA_Libraries/simpleCUFFT_callback/simpleCUFFT_callback.cu.hip
@@ -1,344 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-/* 
- * Example showing the use of CUFFT for fast 1D-convolution using FFT. 
- * This sample is the same as simpleCUFFT, except that it uses a callback
- * function to perform the pointwise multiply and scale, on input to the
- * inverse transform.
- * 
-*/
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-// includes, project
-#include <hip/hip_runtime.h>
-#include <hipfft.h>
-#include <hipfftXt.h>
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-// Complex data type
-typedef float2 Complex;
-static __device__ __host__ inline Complex ComplexAdd(Complex, Complex);
-static __device__ __host__ inline Complex ComplexScale(Complex, float);
-static __device__ __host__ inline Complex ComplexMul(Complex, Complex);
-
-// This is the callback routine prototype
-static __device__ hipfftComplex ComplexPointwiseMulAndScale(void *a,
-                                                           size_t index,
-                                                           void *cb_info,
-                                                           void *sharedmem);
-
-typedef struct _cb_params {
-  Complex *filter;
-  float scale;
-} cb_params;
-
-// This is the callback routine. It does complex pointwise multiplication with
-// scaling.
-static __device__ hipfftComplex ComplexPointwiseMulAndScale(void *a,
-                                                           size_t index,
-                                                           void *cb_info,
-                                                           void *sharedmem) {
-  cb_params *my_params = (cb_params *)cb_info;
-  return (hipfftComplex)ComplexScale(
-      ComplexMul(((Complex *)a)[index], (my_params->filter)[index]),
-      my_params->scale);
-}
-
-// Define the device pointer to the callback routine. The host code will fetch
-// this and pass it to CUFFT
-__device__ hipfftCallbackLoadC myOwnCallbackPtr = ComplexPointwiseMulAndScale;
-// Filtering functions
-void Convolve(const Complex *, int, const Complex *, int, Complex *);
-
-// Padding functions
-int PadData(const Complex *, Complex **, int, const Complex *, Complex **, int);
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-int runTest(int argc, char **argv);
-
-// The filter size is assumed to be a number smaller than the signal size
-#define SIGNAL_SIZE 50
-#define FILTER_KERNEL_SIZE 11
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  struct hipDeviceProp_t properties;
-  int device;
-  HIPCHECK(hipGetDevice(&device));
-  HIPCHECK(hipGetDeviceProperties(&properties, device));
-  if (!(properties.major >= 2)) {
-    printf("simpleCUFFT_callback requires CUDA architecture SM2.0 or higher\n");
-    return EXIT_WAIVED;
-  }
-
-  return runTest(argc, argv);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Run a simple test for CUFFT callbacks
-////////////////////////////////////////////////////////////////////////////////
-int runTest(int argc, char **argv) {
-  printf("[simpleCUFFT_callback] is starting...\n");
-
-  findCudaDevice(argc, (const char **)argv);
-
-  // Allocate host memory for the signal
-  Complex *h_signal = (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
-
-  // Initialize the memory for the signal
-  for (unsigned int i = 0; i < SIGNAL_SIZE; ++i) {
-    h_signal[i].x = rand() / (float)RAND_MAX;
-    h_signal[i].y = 0;
-  }
-
-  // Allocate host memory for the filter
-  Complex *h_filter_kernel =
-      (Complex *)malloc(sizeof(Complex) * FILTER_KERNEL_SIZE);
-
-  // Initialize the memory for the filter
-  for (unsigned int i = 0; i < FILTER_KERNEL_SIZE; ++i) {
-    h_filter_kernel[i].x = rand() / (float)RAND_MAX;
-    h_filter_kernel[i].y = 0;
-  }
-
-  // Pad signal and filter kernel
-  Complex *h_padded_signal;
-  Complex *h_padded_filter_kernel;
-  int new_size =
-      PadData(h_signal, &h_padded_signal, SIGNAL_SIZE, h_filter_kernel,
-              &h_padded_filter_kernel, FILTER_KERNEL_SIZE);
-  int mem_size = sizeof(Complex) * new_size;
-
-  // Allocate device memory for signal
-  Complex *d_signal;
-  HIPCHECK(hipMalloc((void **)&d_signal, mem_size));
-  // Copy host memory to device
-  HIPCHECK(
-      hipMemcpy(d_signal, h_padded_signal, mem_size, hipMemcpyHostToDevice));
-
-  // Allocate device memory for filter kernel
-  Complex *d_filter_kernel;
-  HIPCHECK(hipMalloc((void **)&d_filter_kernel, mem_size));
-
-  // Copy host memory to device
-  HIPCHECK(hipMemcpy(d_filter_kernel, h_padded_filter_kernel, mem_size,
-                             hipMemcpyHostToDevice));
-
-  // Create one CUFFT plan for the forward transforms, and one for the reverse
-  // transform with load callback.
-  hipfftHandle plan, cb_plan;
-  size_t work_size;
-
-  HIPCHECK(hipfftCreate(&plan));
-  HIPCHECK(hipfftCreate(&cb_plan));
-
-  HIPCHECK(hipfftMakePlan1d(plan, new_size, HIPFFT_C2C, 1, &work_size));
-  HIPCHECK(hipfftMakePlan1d(cb_plan, new_size, HIPFFT_C2C, 1, &work_size));
-
-  // Define a structure used to pass in the device address of the filter kernel,
-  // and the scale factor
-  cb_params h_params;
-
-  h_params.filter = d_filter_kernel;
-  h_params.scale = 1.0f / new_size;
-
-  // Allocate device memory for parameters
-  cb_params *d_params;
-  HIPCHECK(hipMalloc((void **)&d_params, sizeof(cb_params)));
-
-  // Copy host memory to device
-  HIPCHECK(hipMemcpy(d_params, &h_params, sizeof(cb_params),
-                             hipMemcpyHostToDevice));
-
-  // The host needs to get a copy of the device pointer to the callback
-  hipfftCallbackLoadC hostCopyOfCallbackPtr;
-
-  HIP_SYMBOL(myOwnCallbackPtr)(hipMemcpyFromSymbol(&hostCopyOfCallbackPtr, myOwnCallbackPtr,
-                                       sizeof(hostCopyOfCallbackPtr)));
-
-  // Now associate the load callback with the plan.
-  hipfftResult status =
-      hipfftXtSetCallback(cb_plan, (void **)&hostCopyOfCallbackPtr,
-                         HIPFFT_CB_LD_COMPLEX, (void **)&d_params);
-  if (status == CUFFT_LICENSE_ERROR) {
-    printf("This sample requires a valid license file.\n");
-    printf(
-        "The file was either not found, out of date, or otherwise invalid.\n");
-    return EXIT_WAIVED;
-  }
-
-  HIPCHECK(hipfftXtSetCallback(cb_plan, (void **)&hostCopyOfCallbackPtr,
-                                     HIPFFT_CB_LD_COMPLEX, (void **)&d_params));
-
-  // Transform signal and kernel
-  printf("Transforming signal cufftExecC2C\n");
-  HIPCHECK(hipfftExecC2C(plan, (hipfftComplex *)d_signal,
-                               (hipfftComplex *)d_signal, HIPFFT_FORWARD));
-  HIPCHECK(hipfftExecC2C(plan, (hipfftComplex *)d_filter_kernel,
-                               (hipfftComplex *)d_filter_kernel, HIPFFT_FORWARD));
-
-  // Transform signal back, using the callback to do the pointwise multiply on
-  // the way in.
-  printf("Transforming signal back cufftExecC2C\n");
-  HIPCHECK(hipfftExecC2C(cb_plan, (hipfftComplex *)d_signal,
-                               (hipfftComplex *)d_signal, HIPFFT_BACKWARD));
-
-  // Copy device memory to host
-  Complex *h_convolved_signal = h_padded_signal;
-  HIPCHECK(hipMemcpy(h_convolved_signal, d_signal, mem_size,
-                             hipMemcpyDeviceToHost));
-
-  // Allocate host memory for the convolution result
-  Complex *h_convolved_signal_ref =
-      (Complex *)malloc(sizeof(Complex) * SIGNAL_SIZE);
-
-  // Convolve on the host
-  Convolve(h_signal, SIGNAL_SIZE, h_filter_kernel, FILTER_KERNEL_SIZE,
-           h_convolved_signal_ref);
-
-  // check result
-  bool bTestResult =
-      sdkCompareL2fe((float *)h_convolved_signal_ref,
-                     (float *)h_convolved_signal, 2 * SIGNAL_SIZE, 1e-5f);
-
-  // Destroy CUFFT context
-  HIPCHECK(hipfftDestroy(plan));
-  HIPCHECK(hipfftDestroy(cb_plan));
-
-  // cleanup memory
-  free(h_signal);
-  free(h_filter_kernel);
-  free(h_padded_signal);
-  free(h_padded_filter_kernel);
-  free(h_convolved_signal_ref);
-  HIPCHECK(hipFree(d_signal));
-  HIPCHECK(hipFree(d_filter_kernel));
-  HIPCHECK(hipFree(d_params));
-
-  return bTestResult ? EXIT_SUCCESS : EXIT_FAILURE;
-}
-
-// Pad data
-int PadData(const Complex *signal, Complex **padded_signal, int signal_size,
-            const Complex *filter_kernel, Complex **padded_filter_kernel,
-            int filter_kernel_size) {
-  int minRadius = filter_kernel_size / 2;
-  int maxRadius = filter_kernel_size - minRadius;
-  int new_size = signal_size + maxRadius;
-
-  // Pad signal
-  Complex *new_data = (Complex *)malloc(sizeof(Complex) * new_size);
-  memcpy(new_data + 0, signal, signal_size * sizeof(Complex));
-  memset(new_data + signal_size, 0, (new_size - signal_size) * sizeof(Complex));
-  *padded_signal = new_data;
-
-  // Pad filter
-  new_data = (Complex *)malloc(sizeof(Complex) * new_size);
-  memcpy(new_data + 0, filter_kernel + minRadius, maxRadius * sizeof(Complex));
-  memset(new_data + maxRadius, 0,
-         (new_size - filter_kernel_size) * sizeof(Complex));
-  memcpy(new_data + new_size - minRadius, filter_kernel,
-         minRadius * sizeof(Complex));
-  *padded_filter_kernel = new_data;
-
-  return new_size;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Filtering operations
-////////////////////////////////////////////////////////////////////////////////
-
-// Computes convolution on the host
-void Convolve(const Complex *signal, int signal_size,
-              const Complex *filter_kernel, int filter_kernel_size,
-              Complex *filtered_signal) {
-  int minRadius = filter_kernel_size / 2;
-  int maxRadius = filter_kernel_size - minRadius;
-
-  // Loop over output element indices
-  for (int i = 0; i < signal_size; ++i) {
-    filtered_signal[i].x = filtered_signal[i].y = 0;
-
-    // Loop over convolution indices
-    for (int j = -maxRadius + 1; j <= minRadius; ++j) {
-      int k = i + j;
-
-      if (k >= 0 && k < signal_size) {
-        filtered_signal[i] =
-            ComplexAdd(filtered_signal[i],
-                       ComplexMul(signal[k], filter_kernel[minRadius - j]));
-      }
-    }
-  }
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Complex operations
-////////////////////////////////////////////////////////////////////////////////
-
-// Complex addition
-static __device__ __host__ inline Complex ComplexAdd(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x + b.x;
-  c.y = a.y + b.y;
-  return c;
-}
-
-// Complex scale
-static __device__ __host__ inline Complex ComplexScale(Complex a, float s) {
-  Complex c;
-  c.x = s * a.x;
-  c.y = s * a.y;
-  return c;
-}
-
-// Complex multiplication
-static __device__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x * b.x - a.y * b.y;
-  c.y = a.x * b.y + a.y * b.x;
-  return c;
-}
-ice__ __host__ inline Complex ComplexMul(Complex a, Complex b) {
-  Complex c;
-  c.x = a.x * b.x - a.y * b.y;
-  c.y = a.x * b.y + a.y * b.x;
-  return c;
-}
diff --git a/src/samples/Samples/5_Domain_Specific/BlackScholes/BlackScholes.cu.hip b/src/samples/Samples/5_Domain_Specific/BlackScholes/BlackScholes.cu.hip
index c066a41..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/BlackScholes/BlackScholes.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/BlackScholes/BlackScholes.cu.hip
@@ -1,245 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * This sample evaluates fair call and put prices for a
- * given set of European options by Black-Scholes formula.
- * See supplied whitepaper for more explanations.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <helper_functions.h>  // helper functions for string parsing
-#include <helper_cuda.h>  // helper functions CUDA error checking and initialization
-
-////////////////////////////////////////////////////////////////////////////////
-// Process an array of optN options on CPU
-////////////////////////////////////////////////////////////////////////////////
-extern "C" void BlackScholesCPU(float *h_CallResult, float *h_PutResult,
-                                float *h_StockPrice, float *h_OptionStrike,
-                                float *h_OptionYears, float Riskfree,
-                                float Volatility, int optN);
-
-////////////////////////////////////////////////////////////////////////////////
-// Process an array of OptN options on GPU
-////////////////////////////////////////////////////////////////////////////////
-#include "BlackScholes_kernel.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// Helper function, returning uniformly distributed
-// random float in [low, high] range
-////////////////////////////////////////////////////////////////////////////////
-float RandFloat(float low, float high) {
-  float t = (float)rand() / (float)RAND_MAX;
-  return (1.0f - t) * low + t * high;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Data configuration
-////////////////////////////////////////////////////////////////////////////////
-const int OPT_N = 4000000;
-const int NUM_ITERATIONS = 512;
-
-const int OPT_SZ = OPT_N * sizeof(float);
-const float RISKFREE = 0.02f;
-const float VOLATILITY = 0.30f;
-
-#define DIV_UP(a, b) (((a) + (b)-1) / (b))
-
-////////////////////////////////////////////////////////////////////////////////
-// Main program
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  // Start logs
-  printf("[%s] - Starting...\n", argv[0]);
-
-  //'h_' prefix - CPU (host) memory space
-  float
-      // Results calculated by CPU for reference
-      *h_CallResultCPU,
-      *h_PutResultCPU,
-      // CPU copy of GPU results
-      *h_CallResultGPU, *h_PutResultGPU,
-      // CPU instance of input data
-      *h_StockPrice, *h_OptionStrike, *h_OptionYears;
-
-  //'d_' prefix - GPU (device) memory space
-  float
-      // Results calculated by GPU
-      *d_CallResult,
-      *d_PutResult,
-      // GPU instance of input data
-      *d_StockPrice, *d_OptionStrike, *d_OptionYears;
-
-  double delta, ref, sum_delta, sum_ref, max_delta, L1norm, gpuTime;
-
-  StopWatchInterface *hTimer = NULL;
-  int i;
-
-  findCudaDevice(argc, (const char **)argv);
-
-  sdkCreateTimer(&hTimer);
-
-  printf("Initializing data...\n");
-  printf("...allocating CPU memory for options.\n");
-  h_CallResultCPU = (float *)malloc(OPT_SZ);
-  h_PutResultCPU = (float *)malloc(OPT_SZ);
-  h_CallResultGPU = (float *)malloc(OPT_SZ);
-  h_PutResultGPU = (float *)malloc(OPT_SZ);
-  h_StockPrice = (float *)malloc(OPT_SZ);
-  h_OptionStrike = (float *)malloc(OPT_SZ);
-  h_OptionYears = (float *)malloc(OPT_SZ);
-
-  printf("...allocating GPU memory for options.\n");
-  HIPCHECK(hipMalloc((void **)&d_CallResult, OPT_SZ));
-  HIPCHECK(hipMalloc((void **)&d_PutResult, OPT_SZ));
-  HIPCHECK(hipMalloc((void **)&d_StockPrice, OPT_SZ));
-  HIPCHECK(hipMalloc((void **)&d_OptionStrike, OPT_SZ));
-  HIPCHECK(hipMalloc((void **)&d_OptionYears, OPT_SZ));
-
-  printf("...generating input data in CPU mem.\n");
-  srand(5347);
-
-  // Generate options set
-  for (i = 0; i < OPT_N; i++) {
-    h_CallResultCPU[i] = 0.0f;
-    h_PutResultCPU[i] = -1.0f;
-    h_StockPrice[i] = RandFloat(5.0f, 30.0f);
-    h_OptionStrike[i] = RandFloat(1.0f, 100.0f);
-    h_OptionYears[i] = RandFloat(0.25f, 10.0f);
-  }
-
-  printf("...copying input data to GPU mem.\n");
-  // Copy options data to GPU memory for further processing
-  HIPCHECK(
-      hipMemcpy(d_StockPrice, h_StockPrice, OPT_SZ, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(d_OptionStrike, h_OptionStrike, OPT_SZ,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(
-      hipMemcpy(d_OptionYears, h_OptionYears, OPT_SZ, hipMemcpyHostToDevice));
-  printf("Data init done.\n\n");
-
-  printf("Executing Black-Scholes GPU kernel (%i iterations)...\n",
-         NUM_ITERATIONS);
-  HIPCHECK(hipDeviceSynchronize());
-  sdkResetTimer(&hTimer);
-  sdkStartTimer(&hTimer);
-
-  for (i = 0; i < NUM_ITERATIONS; i++) {
-    BlackScholesGPU<<<DIV_UP((OPT_N / 2), 128), 128 /*480, 128*/>>>(
-        (float2 *)d_CallResult, (float2 *)d_PutResult, (float2 *)d_StockPrice,
-        (float2 *)d_OptionStrike, (float2 *)d_OptionYears, RISKFREE, VOLATILITY,
-        OPT_N);
-    getLastCudaError("BlackScholesGPU() execution failed\n");
-  }
-
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&hTimer);
-  gpuTime = sdkGetTimerValue(&hTimer) / NUM_ITERATIONS;
-
-  // Both call and put is calculated
-  printf("Options count             : %i     \n", 2 * OPT_N);
-  printf("BlackScholesGPU() time    : %f msec\n", gpuTime);
-  printf("Effective memory bandwidth: %f GB/s\n",
-         ((double)(5 * OPT_N * sizeof(float)) * 1E-9) / (gpuTime * 1E-3));
-  printf("Gigaoptions per second    : %f     \n\n",
-         ((double)(2 * OPT_N) * 1E-9) / (gpuTime * 1E-3));
-
-  printf(
-      "BlackScholes, Throughput = %.4f GOptions/s, Time = %.5f s, Size = %u "
-      "options, NumDevsUsed = %u, Workgroup = %u\n",
-      (((double)(2.0 * OPT_N) * 1.0E-9) / (gpuTime * 1.0E-3)), gpuTime * 1e-3,
-      (2 * OPT_N), 1, 128);
-
-  printf("\nReading back GPU results...\n");
-  // Read back GPU results to compare them to CPU results
-  HIPCHECK(hipMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ,
-                             hipMemcpyDeviceToHost));
-  HIPCHECK(
-      hipMemcpy(h_PutResultGPU, d_PutResult, OPT_SZ, hipMemcpyDeviceToHost));
-
-  printf("Checking the results...\n");
-  printf("...running CPU calculations.\n\n");
-  // Calculate options values on CPU
-  BlackScholesCPU(h_CallResultCPU, h_PutResultCPU, h_StockPrice, h_OptionStrike,
-                  h_OptionYears, RISKFREE, VOLATILITY, OPT_N);
-
-  printf("Comparing the results...\n");
-  // Calculate max absolute difference and L1 distance
-  // between CPU and GPU results
-  sum_delta = 0;
-  sum_ref = 0;
-  max_delta = 0;
-
-  for (i = 0; i < OPT_N; i++) {
-    ref = h_CallResultCPU[i];
-    delta = fabs(h_CallResultCPU[i] - h_CallResultGPU[i]);
-
-    if (delta > max_delta) {
-      max_delta = delta;
-    }
-
-    sum_delta += delta;
-    sum_ref += fabs(ref);
-  }
-
-  L1norm = sum_delta / sum_ref;
-  printf("L1 norm: %E\n", L1norm);
-  printf("Max absolute error: %E\n\n", max_delta);
-
-  printf("Shutting down...\n");
-  printf("...releasing GPU memory.\n");
-  HIPCHECK(hipFree(d_OptionYears));
-  HIPCHECK(hipFree(d_OptionStrike));
-  HIPCHECK(hipFree(d_StockPrice));
-  HIPCHECK(hipFree(d_PutResult));
-  HIPCHECK(hipFree(d_CallResult));
-
-  printf("...releasing CPU memory.\n");
-  free(h_OptionYears);
-  free(h_OptionStrike);
-  free(h_StockPrice);
-  free(h_PutResultGPU);
-  free(h_CallResultGPU);
-  free(h_PutResultCPU);
-  free(h_CallResultCPU);
-  sdkDeleteTimer(&hTimer);
-  printf("Shutdown done.\n");
-
-  printf("\n[BlackScholes] - Test Summary\n");
-
-  if (L1norm > 1e-6) {
-    printf("Test failed!\n");
-    exit(EXIT_FAILURE);
-  }
-
-  printf(
-      "\nNOTE: The CUDA Samples are not meant for performance measurements. "
-      "Results may vary when GPU Boost is enabled.\n\n");
-  printf("Test passed\n");
-  exit(EXIT_SUCCESS);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip b/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip
index 15158aa..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/HSOpticalFlow/flowCUDA.cu.hip
@@ -1,218 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include "common.h"
-
-// include kernels
-#include "downscaleKernel.cuh"
-#include "upscaleKernel.cuh"
-#include "warpingKernel.cuh"
-#include "derivativesKernel.cuh"
-#include "solverKernel.cuh"
-#include "addKernel.cuh"
-
-///////////////////////////////////////////////////////////////////////////////
-/// \brief method logic
-///
-/// handles memory allocations, control flow
-/// \param[in]  I0           source image
-/// \param[in]  I1           tracked image
-/// \param[in]  width        images width
-/// \param[in]  height       images height
-/// \param[in]  stride       images stride
-/// \param[in]  alpha        degree of displacement field smoothness
-/// \param[in]  nLevels      number of levels in a pyramid
-/// \param[in]  nWarpIters   number of warping iterations per pyramid level
-/// \param[in]  nSolverIters number of solver iterations (Jacobi iterations)
-/// \param[out] u            horizontal displacement
-/// \param[out] v            vertical displacement
-///////////////////////////////////////////////////////////////////////////////
-void ComputeFlowCUDA(const float *I0, const float *I1, int width, int height,
-                     int stride, float alpha, int nLevels, int nWarpIters,
-                     int nSolverIters, float *u, float *v) {
-  printf("Computing optical flow on GPU...\n");
-
-  // pI0 and pI1 will hold device pointers
-  const float **pI0 = new const float *[nLevels];
-  const float **pI1 = new const float *[nLevels];
-
-  int *pW = new int[nLevels];
-  int *pH = new int[nLevels];
-  int *pS = new int[nLevels];
-
-  // device memory pointers
-  float *d_tmp;
-  float *d_du0;
-  float *d_dv0;
-  float *d_du1;
-  float *d_dv1;
-
-  float *d_Ix;
-  float *d_Iy;
-  float *d_Iz;
-
-  float *d_u;
-  float *d_v;
-  float *d_nu;
-  float *d_nv;
-
-  const int dataSize = stride * height * sizeof(float);
-
-  HIPCHECK(hipMalloc(&d_tmp, dataSize));
-  HIPCHECK(hipMalloc(&d_du0, dataSize));
-  HIPCHECK(hipMalloc(&d_dv0, dataSize));
-  HIPCHECK(hipMalloc(&d_du1, dataSize));
-  HIPCHECK(hipMalloc(&d_dv1, dataSize));
-
-  HIPCHECK(hipMalloc(&d_Ix, dataSize));
-  HIPCHECK(hipMalloc(&d_Iy, dataSize));
-  HIPCHECK(hipMalloc(&d_Iz, dataSize));
-
-  HIPCHECK(hipMalloc(&d_u, dataSize));
-  HIPCHECK(hipMalloc(&d_v, dataSize));
-  HIPCHECK(hipMalloc(&d_nu, dataSize));
-  HIPCHECK(hipMalloc(&d_nv, dataSize));
-
-  // prepare pyramid
-
-  int currentLevel = nLevels - 1;
-  // allocate GPU memory for input images
-  HIPCHECK(hipMalloc(pI0 + currentLevel, dataSize));
-  HIPCHECK(hipMalloc(pI1 + currentLevel, dataSize));
-
-  HIPCHECK(hipMemcpy((void *)pI0[currentLevel], I0, dataSize,
-                             hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy((void *)pI1[currentLevel], I1, dataSize,
-                             hipMemcpyHostToDevice));
-
-  pW[currentLevel] = width;
-  pH[currentLevel] = height;
-  pS[currentLevel] = stride;
-
-  for (; currentLevel > 0; --currentLevel) {
-    int nw = pW[currentLevel] / 2;
-    int nh = pH[currentLevel] / 2;
-    int ns = iAlignUp(nw);
-
-    HIPCHECK(
-        hipMalloc(pI0 + currentLevel - 1, ns * nh * sizeof(float)));
-    HIPCHECK(
-        hipMalloc(pI1 + currentLevel - 1, ns * nh * sizeof(float)));
-
-    Downscale(pI0[currentLevel], pW[currentLevel], pH[currentLevel],
-              pS[currentLevel], nw, nh, ns, (float *)pI0[currentLevel - 1]);
-
-    Downscale(pI1[currentLevel], pW[currentLevel], pH[currentLevel],
-              pS[currentLevel], nw, nh, ns, (float *)pI1[currentLevel - 1]);
-
-    pW[currentLevel - 1] = nw;
-    pH[currentLevel - 1] = nh;
-    pS[currentLevel - 1] = ns;
-  }
-
-  HIPCHECK(hipMemset(d_u, 0, stride * height * sizeof(float)));
-  HIPCHECK(hipMemset(d_v, 0, stride * height * sizeof(float)));
-
-  // compute flow
-  for (; currentLevel < nLevels; ++currentLevel) {
-    for (int warpIter = 0; warpIter < nWarpIters; ++warpIter) {
-      HIPCHECK(hipMemset(d_du0, 0, dataSize));
-      HIPCHECK(hipMemset(d_dv0, 0, dataSize));
-
-      HIPCHECK(hipMemset(d_du1, 0, dataSize));
-      HIPCHECK(hipMemset(d_dv1, 0, dataSize));
-
-      // on current level we compute optical flow
-      // between frame 0 and warped frame 1
-      WarpImage(pI1[currentLevel], pW[currentLevel], pH[currentLevel],
-                pS[currentLevel], d_u, d_v, d_tmp);
-
-      ComputeDerivatives(pI0[currentLevel], d_tmp, pW[currentLevel],
-                         pH[currentLevel], pS[currentLevel], d_Ix, d_Iy, d_Iz);
-
-      for (int iter = 0; iter < nSolverIters; ++iter) {
-        SolveForUpdate(d_du0, d_dv0, d_Ix, d_Iy, d_Iz, pW[currentLevel],
-                       pH[currentLevel], pS[currentLevel], alpha, d_du1, d_dv1);
-
-        Swap(d_du0, d_du1);
-        Swap(d_dv0, d_dv1);
-      }
-
-      // update u, v
-      Add(d_u, d_du0, pH[currentLevel] * pS[currentLevel], d_u);
-      Add(d_v, d_dv0, pH[currentLevel] * pS[currentLevel], d_v);
-    }
-
-    if (currentLevel != nLevels - 1) {
-      // prolongate solution
-      float scaleX = (float)pW[currentLevel + 1] / (float)pW[currentLevel];
-
-      Upscale(d_u, pW[currentLevel], pH[currentLevel], pS[currentLevel],
-              pW[currentLevel + 1], pH[currentLevel + 1], pS[currentLevel + 1],
-              scaleX, d_nu);
-
-      float scaleY = (float)pH[currentLevel + 1] / (float)pH[currentLevel];
-
-      Upscale(d_v, pW[currentLevel], pH[currentLevel], pS[currentLevel],
-              pW[currentLevel + 1], pH[currentLevel + 1], pS[currentLevel + 1],
-              scaleY, d_nv);
-
-      Swap(d_u, d_nu);
-      Swap(d_v, d_nv);
-    }
-  }
-
-  HIPCHECK(hipMemcpy(u, d_u, dataSize, hipMemcpyDeviceToHost));
-  HIPCHECK(hipMemcpy(v, d_v, dataSize, hipMemcpyDeviceToHost));
-
-  // cleanup
-  for (int i = 0; i < nLevels; ++i) {
-    HIPCHECK(hipFree((void *)pI0[i]));
-    HIPCHECK(hipFree((void *)pI1[i]));
-  }
-
-  delete[] pI0;
-  delete[] pI1;
-  delete[] pW;
-  delete[] pH;
-  delete[] pS;
-
-  HIPCHECK(hipFree(d_tmp));
-  HIPCHECK(hipFree(d_du0));
-  HIPCHECK(hipFree(d_dv0));
-  HIPCHECK(hipFree(d_du1));
-  HIPCHECK(hipFree(d_dv1));
-  HIPCHECK(hipFree(d_Ix));
-  HIPCHECK(hipFree(d_Iy));
-  HIPCHECK(hipFree(d_Iz));
-  HIPCHECK(hipFree(d_nu));
-  HIPCHECK(hipFree(d_nv));
-  HIPCHECK(hipFree(d_u));
-  HIPCHECK(hipFree(d_v));
-}
diff --git a/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip
index d071409..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/Mandelbrot/Mandelbrot_cuda.cu.hip
@@ -1,398 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include "helper_cuda.h"
-#include "Mandelbrot_kernel.h"
-#include "Mandelbrot_kernel.cuh"
-
-// The Mandelbrot CUDA GPU thread function
-
-template <class T>
-__global__ void Mandelbrot0(uchar4 *dst, const int imageW, const int imageH,
-                            const int crunch, const T xOff, const T yOff,
-                            const T xJP, const T yJP, const T scale,
-                            const uchar4 colors, const int frame,
-                            const int animationFrame, const int gridWidth,
-                            const int numBlocks, const bool isJ) {
-  // loop until all blocks completed
-  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
-       blockIndex += gridDim.x) {
-    unsigned int blockX = blockIndex % gridWidth;
-    unsigned int blockY = blockIndex / gridWidth;
-
-    // process this block
-    const int ix = blockDim.x * blockX + threadIdx.x;
-    const int iy = blockDim.y * blockY + threadIdx.y;
-
-    if ((ix < imageW) && (iy < imageH)) {
-      // Calculate the location
-      const T xPos = (T)ix * scale + xOff;
-      const T yPos = (T)iy * scale + yOff;
-
-      // Calculate the Mandelbrot index for the current location
-      int m = CalcMandelbrot<T>(xPos, yPos, xJP, yJP, crunch, isJ);
-      //            int m = blockIdx.x;         // uncomment to see scheduling
-      //            order
-      m = m > 0 ? crunch - m : 0;
-
-      // Convert the Mandelbrot index into a color
-      uchar4 color;
-
-      if (m) {
-        m += animationFrame;
-        color.x = m * colors.x;
-        color.y = m * colors.y;
-        color.z = m * colors.z;
-      } else {
-        color.x = 0;
-        color.y = 0;
-        color.z = 0;
-      }
-
-      // Output the pixel
-      int pixel = imageW * iy + ix;
-
-      if (frame == 0) {
-        color.w = 0;
-        dst[pixel] = color;
-      } else {
-        int frame1 = frame + 1;
-        int frame2 = frame1 / 2;
-        dst[pixel].x = (dst[pixel].x * frame + color.x + frame2) / frame1;
-        dst[pixel].y = (dst[pixel].y * frame + color.y + frame2) / frame1;
-        dst[pixel].z = (dst[pixel].z * frame + color.z + frame2) / frame1;
-      }
-    }
-  }
-
-}  // Mandelbrot0
-
-// The Mandelbrot CUDA GPU thread function (double single version)
-__global__ void MandelbrotDS0(uchar4 *dst, const int imageW, const int imageH,
-                              const int crunch, const float xOff0,
-                              const float xOff1, const float yOff0,
-                              const float yOff1, const float xJP,
-                              const float yJP, const float scale,
-                              const uchar4 colors, const int frame,
-                              const int animationFrame, const int gridWidth,
-                              const int numBlocks, const bool isJ) {
-  // loop until all blocks completed
-  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
-       blockIndex += gridDim.x) {
-    unsigned int blockX = blockIndex % gridWidth;
-    unsigned int blockY = blockIndex / gridWidth;
-
-    // process this block
-    const int ix = blockDim.x * blockX + threadIdx.x;
-    const int iy = blockDim.y * blockY + threadIdx.y;
-
-    if ((ix < imageW) && (iy < imageH)) {
-      // Calculate the location
-      float xPos0 = (float)ix * scale;
-      float xPos1 = 0.0f;
-      float yPos0 = (float)iy * scale;
-      float yPos1 = 0.0f;
-      dsadd(xPos0, xPos1, xPos0, xPos1, xOff0, xOff1);
-      dsadd(yPos0, yPos1, yPos0, yPos1, yOff0, yOff1);
-
-      // Calculate the Mandelbrot index for the current location
-      int m =
-          CalcMandelbrotDS(xPos0, xPos1, yPos0, yPos1, xJP, yJP, crunch, isJ);
-      m = m > 0 ? crunch - m : 0;
-
-      // Convert the Mandelbrot index into a color
-      uchar4 color;
-
-      if (m) {
-        m += animationFrame;
-        color.x = m * colors.x;
-        color.y = m * colors.y;
-        color.z = m * colors.z;
-      } else {
-        color.x = 0;
-        color.y = 0;
-        color.z = 0;
-      }
-
-      // Output the pixel
-      int pixel = imageW * iy + ix;
-
-      if (frame == 0) {
-        color.w = 0;
-        dst[pixel] = color;
-      } else {
-        int frame1 = frame + 1;
-        int frame2 = frame1 / 2;
-        dst[pixel].x = (dst[pixel].x * frame + color.x + frame2) / frame1;
-        dst[pixel].y = (dst[pixel].y * frame + color.y + frame2) / frame1;
-        dst[pixel].z = (dst[pixel].z * frame + color.z + frame2) / frame1;
-      }
-    }
-  }
-}  // MandelbrotDS0
-
-// The Mandelbrot secondary AA pass CUDA GPU thread function
-template <class T>
-__global__ void Mandelbrot1(uchar4 *dst, const int imageW, const int imageH,
-                            const int crunch, const T xOff, const T yOff,
-                            const T xJP, const T yJP, const T scale,
-                            const uchar4 colors, const int frame,
-                            const int animationFrame, const int gridWidth,
-                            const int numBlocks, const bool isJ) {
-  // loop until all blocks completed
-  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
-       blockIndex += gridDim.x) {
-    unsigned int blockX = blockIndex % gridWidth;
-    unsigned int blockY = blockIndex / gridWidth;
-
-    // process this block
-    const int ix = blockDim.x * blockX + threadIdx.x;
-    const int iy = blockDim.y * blockY + threadIdx.y;
-
-    if ((ix < imageW) && (iy < imageH)) {
-      // Get the current pixel color
-      int pixel = imageW * iy + ix;
-      uchar4 pixelColor = dst[pixel];
-      int count = 0;
-
-      // Search for pixels out of tolerance surrounding the current pixel
-      if (ix > 0) {
-        count += CheckColors(pixelColor, dst[pixel - 1]);
-      }
-
-      if (ix + 1 < imageW) {
-        count += CheckColors(pixelColor, dst[pixel + 1]);
-      }
-
-      if (iy > 0) {
-        count += CheckColors(pixelColor, dst[pixel - imageW]);
-      }
-
-      if (iy + 1 < imageH) {
-        count += CheckColors(pixelColor, dst[pixel + imageW]);
-      }
-
-      if (count) {
-        // Calculate the location
-        const T xPos = (T)ix * scale + xOff;
-        const T yPos = (T)iy * scale + yOff;
-
-        // Calculate the Mandelbrot index for the current location
-        int m = CalcMandelbrot(xPos, yPos, xJP, yJP, crunch, isJ);
-        m = m > 0 ? crunch - m : 0;
-
-        // Convert the Mandelbrot index into a color
-        uchar4 color;
-
-        if (m) {
-          m += animationFrame;
-          color.x = m * colors.x;
-          color.y = m * colors.y;
-          color.z = m * colors.z;
-        } else {
-          color.x = 0;
-          color.y = 0;
-          color.z = 0;
-        }
-
-        // Output the pixel
-        int frame1 = frame + 1;
-        int frame2 = frame1 / 2;
-        dst[pixel].x = (pixelColor.x * frame + color.x + frame2) / frame1;
-        dst[pixel].y = (pixelColor.y * frame + color.y + frame2) / frame1;
-        dst[pixel].z = (pixelColor.z * frame + color.z + frame2) / frame1;
-      }
-    }
-  }
-
-}  // Mandelbrot1
-
-// The Mandelbrot secondary AA pass CUDA GPU thread function (double single
-// version)
-__global__ void MandelbrotDS1(uchar4 *dst, const int imageW, const int imageH,
-                              const int crunch, const float xOff0,
-                              const float xOff1, const float yOff0,
-                              const float yOff1, const float xJP,
-                              const float yJP, const float scale,
-                              const uchar4 colors, const int frame,
-                              const int animationFrame, const int gridWidth,
-                              const int numBlocks, const bool isJ) {
-  // loop until all blocks completed
-  for (unsigned int blockIndex = blockIdx.x; blockIndex < numBlocks;
-       blockIndex += gridDim.x) {
-    unsigned int blockX = blockIndex % gridWidth;
-    unsigned int blockY = blockIndex / gridWidth;
-
-    // process this block
-    const int ix = blockDim.x * blockX + threadIdx.x;
-    const int iy = blockDim.y * blockY + threadIdx.y;
-
-    if ((ix < imageW) && (iy < imageH)) {
-      // Get the current pixel color
-      int pixel = imageW * iy + ix;
-      uchar4 pixelColor = dst[pixel];
-      int count = 0;
-
-      // Search for pixels out of tolerance surrounding the current pixel
-      if (ix > 0) {
-        count += CheckColors(pixelColor, dst[pixel - 1]);
-      }
-
-      if (ix + 1 < imageW) {
-        count += CheckColors(pixelColor, dst[pixel + 1]);
-      }
-
-      if (iy > 0) {
-        count += CheckColors(pixelColor, dst[pixel - imageW]);
-      }
-
-      if (iy + 1 < imageH) {
-        count += CheckColors(pixelColor, dst[pixel + imageW]);
-      }
-
-      if (count) {
-        // Calculate the location
-        float xPos0 = (float)ix * scale;
-        float xPos1 = 0.0f;
-        float yPos0 = (float)iy * scale;
-        float yPos1 = 0.0f;
-        dsadd(xPos0, xPos1, xPos0, xPos1, xOff0, xOff1);
-        dsadd(yPos0, yPos1, yPos0, yPos1, yOff0, yOff1);
-
-        // Calculate the Mandelbrot index for the current location
-        int m =
-            CalcMandelbrotDS(xPos0, xPos1, yPos0, yPos1, xJP, yJP, crunch, isJ);
-        m = m > 0 ? crunch - m : 0;
-
-        // Convert the Mandelbrot index into a color
-        uchar4 color;
-
-        if (m) {
-          m += animationFrame;
-          color.x = m * colors.x;
-          color.y = m * colors.y;
-          color.z = m * colors.z;
-        } else {
-          color.x = 0;
-          color.y = 0;
-          color.z = 0;
-        }
-
-        // Output the pixel
-        int frame1 = frame + 1;
-        int frame2 = frame1 / 2;
-        dst[pixel].x = (pixelColor.x * frame + color.x + frame2) / frame1;
-        dst[pixel].y = (pixelColor.y * frame + color.y + frame2) / frame1;
-        dst[pixel].z = (pixelColor.z * frame + color.z + frame2) / frame1;
-      }
-    }
-  }
-
-}  // MandelbrotDS1
-
-// The host CPU Mandelbrot thread spawner
-void RunMandelbrot0(uchar4 *dst, const int imageW, const int imageH,
-                    const int crunch, const double xOff, const double yOff,
-                    const double xjp, const double yjp, const double scale,
-                    const uchar4 colors, const int frame,
-                    const int animationFrame, const int mode, const int numSMs,
-                    const bool isJ, int version) {
-  dim3 threads(BLOCKDIM_X, BLOCKDIM_Y);
-  dim3 grid(iDivUp(imageW, BLOCKDIM_X), iDivUp(imageH, BLOCKDIM_Y));
-
-  int numWorkerBlocks = numSMs;
-
-  switch (mode) {
-    default:
-    case 0:
-      Mandelbrot0<float><<<numWorkerBlocks, threads>>>(
-          dst, imageW, imageH, crunch, (float)xOff, (float)yOff, (float)xjp,
-          (float)yjp, (float)scale, colors, frame, animationFrame, grid.x,
-          grid.x * grid.y, isJ);
-      break;
-    case 1:
-      float x0, x1, y0, y1;
-      dsdeq(x0, x1, xOff);
-      dsdeq(y0, y1, yOff);
-      MandelbrotDS0<<<numWorkerBlocks, threads>>>(
-          dst, imageW, imageH, crunch, x0, x1, y0, y1, (float)xjp, (float)yjp,
-          (float)scale, colors, frame, animationFrame, grid.x, grid.x * grid.y,
-          isJ);
-      break;
-    case 2:
-      Mandelbrot0<double><<<numWorkerBlocks, threads>>>(
-          dst, imageW, imageH, crunch, xOff, yOff, xjp, yjp, scale, colors,
-          frame, animationFrame, grid.x, grid.x * grid.y, isJ);
-      break;
-  }
-
-  getLastCudaError("Mandelbrot0 kernel execution failed.\n");
-}  // RunMandelbrot0
-
-// The host CPU Mandelbrot thread spawner
-void RunMandelbrot1(uchar4 *dst, const int imageW, const int imageH,
-                    const int crunch, const double xOff, const double yOff,
-                    const double xjp, const double yjp, const double scale,
-                    const uchar4 colors, const int frame,
-                    const int animationFrame, const int mode, const int numSMs,
-                    const bool isJ, int version) {
-  dim3 threads(BLOCKDIM_X, BLOCKDIM_Y);
-  dim3 grid(iDivUp(imageW, BLOCKDIM_X), iDivUp(imageH, BLOCKDIM_Y));
-
-  int numWorkerBlocks = numSMs;
-
-  switch (mode) {
-    default:
-    case 0:
-      Mandelbrot1<float><<<numWorkerBlocks, threads>>>(
-          dst, imageW, imageH, crunch, (float)xOff, (float)yOff, (float)xjp,
-          (float)yjp, (float)scale, colors, frame, animationFrame, grid.x,
-          grid.x * grid.y, isJ);
-      break;
-    case 1:
-      float x0, x1, y0, y1;
-      dsdeq(x0, x1, xOff);
-      dsdeq(y0, y1, yOff);
-      MandelbrotDS1<<<numWorkerBlocks, threads>>>(
-          dst, imageW, imageH, crunch, x0, x1, y0, y1, (float)xjp, (float)yjp,
-          (float)scale, colors, frame, animationFrame, grid.x, grid.x * grid.y,
-          isJ);
-      break;
-    case 2:
-      Mandelbrot1<double><<<numWorkerBlocks, threads>>>(
-          dst, imageW, imageH, crunch, xOff, yOff, xjp, yjp, scale, colors,
-          frame, animationFrame, grid.x, grid.x * grid.y, isJ);
-      break;
-  }
-
-  getLastCudaError("Mandelbrot1 kernel execution failed.\n");
-}  // RunMandelbrot1
diff --git a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/bgr_resize.cu.hip b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/bgr_resize.cu.hip
index 157d583..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/bgr_resize.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/bgr_resize.cu.hip
@@ -1,134 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-// Implements BGR 3 progressive planars frames batch resize
-
-#include <hip/hip_runtime.h>
-
-#include "resize_convert.h"
-
-__global__ void resizeBGRplanarBatchKernel(hipTextureObject_t texSrc,
-    float *pDst, int nDstPitch, int nDstHeight, int nSrcHeight,
-    int batch, float scaleX, float scaleY,
-    int cropX, int cropY, int cropW, int cropH) {
-    int x = threadIdx.x + blockIdx.x * blockDim.x;
-    int y = threadIdx.y + blockIdx.y * blockDim.y;
-
-    if (x >= (int)(cropW/scaleX) || y >= (int)(cropH/scaleY))
-        return;
-
-    int frameSize = nDstPitch*nDstHeight;
-    float *p = NULL;
-    for (int i = blockIdx.z; i < batch; i += gridDim.z) {
-        #pragma unroll
-        for (int channel=0; channel < 3; channel++){
-            p = pDst + i * 3 * frameSize + y * nDstPitch + x + channel * frameSize;
-            *p = tex2D<float>(texSrc, x * scaleX + cropX,
-                                ((3 * i + channel) * nSrcHeight + y * scaleY + cropY));
-        }
-    }
-}
-
-
-static void resizeBGRplanarBatchCore(
-        float *dpSrc, int nSrcPitch, int nSrcWidth, int nSrcHeight,
-        float *dpDst, int nDstPitch, int nDstWidth, int nDstHeight,
-        int nBatchSize, hipStream_t stream, bool whSameResizeRatio,
-        int cropX, int cropY, int cropW, int cropH) {
-    hipTextureObject_t texSrc[2];
-    int nTiles = 1, h, iTile;
-
-    h = nSrcHeight * 3 * nBatchSize;
-    while ((h + nTiles - 1) / nTiles > 65536)
-        nTiles++;
-
-    if (nTiles > 2)
-        return;
-
-    int batchTile = nBatchSize / nTiles;
-    int batchTileLast = nBatchSize - batchTile * (nTiles-1);
-
-    for (iTile = 0; iTile < nTiles; ++iTile) {
-        int bs = (iTile == nTiles - 1) ? batchTileLast : batchTile;
-        float *dpSrcNew = dpSrc +
-            iTile * (batchTile * 3 * nSrcHeight * nSrcPitch);
-
-        hipResourceDesc resDesc = {};
-        resDesc.resType = hipResourceTypePitch2D;
-        resDesc.res.pitch2D.devPtr = dpSrcNew;
-        resDesc.res.pitch2D.desc = hipCreateChannelDesc<float>();
-        resDesc.res.pitch2D.width = nSrcWidth;
-        resDesc.res.pitch2D.height = bs * 3 * nSrcHeight;
-        resDesc.res.pitch2D.pitchInBytes = nSrcPitch * sizeof(float);
-        hipTextureDesc texDesc = {};
-        texDesc.filterMode = hipFilterModeLinear;
-        texDesc.readMode = hipReadModeElementType;
-
-        HIPCHECK(hipCreateTextureObject(&texSrc[iTile], &resDesc, &texDesc, NULL));
-        float *dpDstNew = dpDst +
-            iTile * (batchTile * 3 * nDstHeight * nDstPitch);
-
-        if(cropW == 0 || cropH == 0) {
-            cropX = 0;
-            cropY = 0;
-            cropW = nSrcWidth;
-            cropH = nSrcHeight;
-        }
-
-        float scaleX = (cropW*1.0f / nDstWidth);
-        float scaleY = (cropH*1.0f / nDstHeight);
-
-        if(whSameResizeRatio == true)
-            scaleX = scaleY = scaleX > scaleY ? scaleX : scaleY;
-        dim3 block(32, 32, 1);
-
-        size_t blockDimZ = bs;
-        // Restricting blocks in Z-dim till 32 to not launch too many blocks
-        blockDimZ = (blockDimZ > 32) ? 32 : blockDimZ;
-        dim3 grid((cropW*1.0f/scaleX + block.x - 1) / block.x,
-                  (cropH*1.0f/scaleY + block.y - 1) / block.y, blockDimZ);
-
-        resizeBGRplanarBatchKernel<<<grid, block, 0, stream>>>
-                (texSrc[iTile], dpDstNew, nDstPitch, nDstHeight, nSrcHeight,
-                bs, scaleX, scaleY, cropX, cropY, cropW, cropH);
-
-    }
-
-    for (iTile = 0; iTile < nTiles; ++iTile)
-        HIPCHECK(hipDestroyTextureObject(texSrc[iTile]));
-}
-
-void resizeBGRplanarBatch(
-        float *dpSrc, int nSrcPitch, int nSrcWidth, int nSrcHeight,
-        float *dpDst, int nDstPitch, int nDstWidth, int nDstHeight,
-        int nBatchSize, hipStream_t stream,
-        int cropX, int cropY, int cropW, int cropH, bool whSameResizeRatio) {
-    resizeBGRplanarBatchCore(dpSrc, nSrcPitch, nSrcWidth, nSrcHeight,
-        dpDst, nDstPitch, nDstWidth, nDstHeight, nBatchSize, stream,
-        whSameResizeRatio, cropX, cropY, cropW, cropH);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_resize.cu.hip b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_resize.cu.hip
index 2affb2c..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_resize.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_resize.cu.hip
@@ -1,112 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// Implements interlace NV12 frames batch resize
-
-#include <hip/hip_runtime.h>
-
-#include "resize_convert.h"
-
-__global__ static void resizeNV12BatchKernel(hipTextureObject_t texSrcLuma,
-                                             hipTextureObject_t texSrcChroma,
-                                             uint8_t *pDstNv12, int nSrcWidth,
-                                             int nSrcHeight, int nDstPitch,
-                                             int nDstWidth, int nDstHeight,
-                                             int nBatchSize) {
-  int x = threadIdx.x + blockIdx.x * blockDim.x;
-  int y = threadIdx.y + blockIdx.y * blockDim.y;
-
-  int px = x * 2, py = y * 2;
-
-  if ((px + 1) >= nDstWidth || (py + 1) >= nDstHeight) return;
-
-  float fxScale = 1.0f * nSrcWidth / nDstWidth;
-  float fyScale = 1.0f * nSrcHeight / nDstHeight;
-
-  uint8_t *p = pDstNv12 + px + py * nDstPitch;
-  int hh = nDstHeight * 3 / 2;
-  int nByte = nDstPitch * hh;
-  int px_fxScale = px * fxScale;
-  int px_fxScale_1 = (px + 1) * fxScale;
-  int py_fyScale = py * fyScale;
-  int py_fyScale_1 = (py + 1) * fyScale;
-
-  for (int i = blockIdx.z; i < nBatchSize; i+=gridDim.z) {
-    *(uchar2 *)p = make_uchar2(tex2D<uint8_t>(texSrcLuma, px_fxScale, py_fyScale),
-                          tex2D<uint8_t>(texSrcLuma, px_fxScale_1, py_fyScale));
-    *(uchar2 *)(p + nDstPitch) =
-        make_uchar2(tex2D<uint8_t>(texSrcLuma, px_fxScale, py_fyScale_1),
-               tex2D<uint8_t>(texSrcLuma, px_fxScale_1, py_fyScale_1));
-    *(uchar2 *)(p + (nDstHeight - y) * nDstPitch) = tex2D<uchar2>(
-        texSrcChroma, x * fxScale, (hh * i + nDstHeight + y) * fyScale);
-    p += nByte;
-    py += hh;
-  }
-}
-
-void resizeNV12Batch(uint8_t *dpSrc, int nSrcPitch, int nSrcWidth,
-                     int nSrcHeight, uint8_t *dpDst, int nDstPitch,
-                     int nDstWidth, int nDstHeight, int nBatchSize,
-                     hipStream_t stream) {
-  int hhSrc = ceilf(nSrcHeight * 3.0f / 2.0f);
-  hipResourceDesc resDesc = {};
-  resDesc.resType = hipResourceTypePitch2D;
-  resDesc.res.pitch2D.devPtr = dpSrc;
-  resDesc.res.pitch2D.desc = hipCreateChannelDesc<uint8_t>();
-  resDesc.res.pitch2D.width = nSrcWidth;
-  resDesc.res.pitch2D.height = hhSrc * nBatchSize;
-  resDesc.res.pitch2D.pitchInBytes = nSrcPitch;
-
-  hipTextureDesc texDesc = {};
-  texDesc.filterMode = hipFilterModePoint;
-  texDesc.readMode = hipReadModeElementType;
-
-  hipTextureObject_t texLuma = 0;
-  HIPCHECK(hipCreateTextureObject(&texLuma, &resDesc, &texDesc, NULL));
-
-  resDesc.res.pitch2D.desc = hipCreateChannelDesc<uchar2>();
-  resDesc.res.pitch2D.width /= 2;
-
-  hipTextureObject_t texChroma = 0;
-  HIPCHECK(hipCreateTextureObject(&texChroma, &resDesc, &texDesc, NULL));
-
-  dim3 block(32, 32, 1);
-
-  size_t blockDimZ = nBatchSize;
-
-  // Restricting blocks in Z-dim till 32 to not launch too many blocks
-  blockDimZ = (blockDimZ > 32) ? 32 : blockDimZ;
-
-  dim3 grid((nDstWidth / 2 + block.x) / block.x,
-            (nDstHeight / 2 + block.y) / block.y, blockDimZ);
-  resizeNV12BatchKernel<<<grid, block, 0, stream>>>(
-      texLuma, texChroma, dpDst, nSrcWidth, nSrcHeight, nDstPitch, nDstWidth,
-      nDstHeight, nBatchSize);
-
-  HIPCHECK(hipDestroyTextureObject(texLuma));
-  HIPCHECK(hipDestroyTextureObject(texChroma));
-}
diff --git a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_to_bgr_planar.cu.hip b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_to_bgr_planar.cu.hip
index af91cfe..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_to_bgr_planar.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/nv12_to_bgr_planar.cu.hip
@@ -1,154 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-// Implements NV12 to BGR batch conversion
-
-#include <hip/hip_runtime.h>
-
-
-#include "resize_convert.h"
-
-#define CONV_THREADS_X 64
-#define CONV_THREADS_Y 10
-
-__forceinline__ __device__ static float clampF(float x, float lower,
-                                               float upper) {
-  return x < lower ? lower : (x > upper ? upper : x);
-}
-
-__global__ static void nv12ToBGRplanarBatchKernel(const uint8_t *pNv12,
-                                                  int nNv12Pitch, float *pBgr,
-                                                  int nRgbPitch, int nWidth,
-                                                  int nHeight, int nBatchSize) {
-  int x = threadIdx.x + blockIdx.x * blockDim.x;
-  int y = threadIdx.y + blockIdx.y * blockDim.y;
-
-  if ((x << 2) + 1 > nWidth || (y << 1) + 1 > nHeight) return;
-
-  const uint8_t *__restrict__ pSrc = pNv12;
-
-  for (int i = blockIdx.z; i < nBatchSize; i += gridDim.z) {
-    pSrc = pNv12 + i * ((nHeight * nNv12Pitch * 3) >> 1) + (x << 2) +
-           (y << 1) * nNv12Pitch;
-    uchar4 luma2x01, luma2x23, uv2;
-    *(uint32_t *)&luma2x01 = *(uint32_t *)pSrc;
-    *(uint32_t *)&luma2x23 = *(uint32_t *)(pSrc + nNv12Pitch);
-    *(uint32_t *)&uv2 = *(uint32_t *)(pSrc + (nHeight - y) * nNv12Pitch);
-
-    float *pDstBlock = (pBgr + i * ((nHeight * nRgbPitch * 3) >> 2) +
-                        ((blockIdx.x * blockDim.x) << 2) +
-                        ((blockIdx.y * blockDim.y) << 1) * (nRgbPitch >> 2));
-
-    float2 add1;
-    float2 add2;
-    float2 add3;
-    float2 add00, add01, add02, add03;
-    float2 d, e;
-
-    add00.x = 1.1644f * luma2x01.x;
-    add01.x = 1.1644f * luma2x01.y;
-    add00.y = 1.1644f * luma2x01.z;
-    add01.y = 1.1644f * luma2x01.w;
-
-    add02.x = 1.1644f * luma2x23.x;
-    add03.x = 1.1644f * luma2x23.y;
-    add02.y = 1.1644f * luma2x23.z;
-    add03.y = 1.1644f * luma2x23.w;
-
-    d.x = uv2.x - 128.0f;
-    e.x = uv2.y - 128.0f;
-    d.y = uv2.z - 128.0f;
-    e.y = uv2.w - 128.0f;
-
-    add1.x = 2.0172f * d.x;
-    add1.y = 2.0172f * d.y;
-
-    add2.x = (-0.3918f) * d.x + (-0.8130f) * e.x;
-    add2.y = (-0.3918f) * d.y + (-0.8130f) * e.y;
-
-    add3.x = 1.5960f * e.x;
-    add3.y = 1.5960f * e.y;
-
-    int rowStride = (threadIdx.y << 1) * (nRgbPitch >> 2);
-    int nextRowStride = ((threadIdx.y << 1) + 1) * (nRgbPitch >> 2);
-    // B
-    *((float4 *)&pDstBlock[rowStride + (threadIdx.x << 2)]) =
-        make_float4(clampF(add00.x + add1.x, 0.0f, 255.0f),
-                    clampF(add01.x + add1.x, 0.0f, 255.0f),
-                    clampF(add00.y + add1.y, 0.0f, 255.0f),
-                    clampF(add01.y + add1.y, 0.0f, 255.0f));
-    *((float4 *)&pDstBlock[nextRowStride + (threadIdx.x << 2)]) =
-        make_float4(clampF(add02.x + add1.x, 0.0f, 255.0f),
-                    clampF(add03.x + add1.x, 0.0f, 255.0f),
-                    clampF(add02.y + add1.y, 0.0f, 255.0f),
-                    clampF(add03.y + add1.y, 0.0f, 255.0f));
-
-    int planeStride = nHeight * nRgbPitch >> 2;
-    // G
-    *((float4 *)&pDstBlock[planeStride + rowStride + (threadIdx.x << 2)]) =
-        make_float4(clampF(add00.x + add2.x, 0.0f, 255.0f),
-                    clampF(add01.x + add2.x, 0.0f, 255.0f),
-                    clampF(add00.y + add2.y, 0.0f, 255.0f),
-                    clampF(add01.y + add2.y, 0.0f, 255.0f));
-    *((float4 *)&pDstBlock[planeStride + nextRowStride + (threadIdx.x << 2)]) =
-        make_float4(clampF(add02.x + add2.x, 0.0f, 255.0f),
-                    clampF(add03.x + add2.x, 0.0f, 255.0f),
-                    clampF(add02.y + add2.y, 0.0f, 255.0f),
-                    clampF(add03.y + add2.y, 0.0f, 255.0f));
-
-    // R
-    *((float4
-           *)&pDstBlock[(planeStride << 1) + rowStride + (threadIdx.x << 2)]) =
-        make_float4(clampF(add00.x + add3.x, 0.0f, 255.0f),
-                    clampF(add01.x + add3.x, 0.0f, 255.0f),
-                    clampF(add00.y + add3.y, 0.0f, 255.0f),
-                    clampF(add01.y + add3.y, 0.0f, 255.0f));
-    *((float4 *)&pDstBlock[(planeStride << 1) + nextRowStride +
-                           (threadIdx.x << 2)]) =
-        make_float4(clampF(add02.x + add3.x, 0.0f, 255.0f),
-                    clampF(add03.x + add3.x, 0.0f, 255.0f),
-                    clampF(add02.y + add3.y, 0.0f, 255.0f),
-                    clampF(add03.y + add3.y, 0.0f, 255.0f));
-  }
-}
-
-void nv12ToBGRplanarBatch(uint8_t *pNv12, int nNv12Pitch, float *pBgr,
-                          int nRgbPitch, int nWidth, int nHeight,
-                          int nBatchSize, hipStream_t stream) {
-  dim3 threads(CONV_THREADS_X, CONV_THREADS_Y);
-
-  size_t blockDimZ = nBatchSize;
-
-  // Restricting blocks in Z-dim till 32 to not launch too many blocks
-  blockDimZ = (blockDimZ > 32) ? 32 : blockDimZ;
-
-  dim3 blocks((nWidth / 4 - 1) / threads.x + 1,
-              (nHeight / 2 - 1) / threads.y + 1, blockDimZ);
-  nv12ToBGRplanarBatchKernel<<<blocks, threads, 0, stream>>>(
-      pNv12, nNv12Pitch, pBgr, nRgbPitch, nWidth, nHeight, nBatchSize);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/utils.cu.hip b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/utils.cu.hip
index ad2def1..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/utils.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/NV12toBGRandResize/utils.cu.hip
@@ -1,152 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <stdlib.h>
-#include <sys/stat.h>
-#include <sys/types.h>
-#include <fstream>
-#include <iostream>
-
-#include <hip/hip_runtime.h>
-
-
-#include "resize_convert.h"
-#include "utils.h"
-
-__global__ void floatToChar(float *src, unsigned char *dst, int height,
-                            int width, int batchSize) {
-  int x = threadIdx.x + blockIdx.x * blockDim.x;
-
-  if (x >= height * width) return;
-
-  int offset = height * width * 3;
-
-  for (int j = 0; j < batchSize; j++) {
-    // b
-    *(dst + j * offset + x * 3 + 0) =
-        (unsigned char)*(src + j * offset + height * width * 0 + x);
-    // g
-    *(dst + j * offset + x * 3 + 1) =
-        (unsigned char)*(src + j * offset + height * width * 1 + x);
-    // r
-    *(dst + j * offset + x * 3 + 2) =
-        (unsigned char)*(src + j * offset + height * width * 2 + x);
-  }
-}
-
-void floatPlanarToChar(float *src, unsigned char *dst, int height, int width,
-                       int batchSize) {
-  floatToChar<<<(height * width - 1) / 1024 + 1, 1024, 0, NULL>>>(
-      src, dst, height, width, batchSize);
-}
-
-void dumpRawBGR(float *d_srcBGR, int pitch, int width, int height,
-                int batchSize, char *folder, char *tag) {
-  float *bgr, *d_bgr;
-  int frameSize;
-  char directory[120];
-  char mkdir_cmd[256];
-#if !defined(_WIN32)
-  sprintf(directory, "output/%s", folder);
-  sprintf(mkdir_cmd, "mkdir -p %s 2> /dev/null", directory);
-#else
-  sprintf(directory, "output\\%s", folder);
-  sprintf(mkdir_cmd, "mkdir %s 2> nul", directory);
-#endif
-
-  int ret = system(mkdir_cmd);
-
-  frameSize = width * height * 3 * sizeof(float);
-  bgr = (float *)malloc(frameSize);
-  if (bgr == NULL) {
-    std::cerr << "Failed malloc for bgr\n";
-    return;
-  }
-
-  d_bgr = d_srcBGR;
-  for (int i = 0; i < batchSize; i++) {
-    char filename[120];
-    std::ofstream *outputFile;
-
-    HIPCHECK(hipMemcpy((void *)bgr, (void *)d_bgr, frameSize,
-                               hipMemcpyDeviceToHost));
-    sprintf(filename, "%s/%s_%d.raw", directory, tag, (i + 1));
-
-    outputFile = new std::ofstream(filename);
-    if (outputFile) {
-      outputFile->write((char *)bgr, frameSize);
-      delete outputFile;
-    }
-
-    d_bgr += pitch * height * 3;
-  }
-
-  free(bgr);
-}
-
-void dumpBGR(float *d_srcBGR, int pitch, int width, int height, int batchSize,
-             char *folder, char *tag) {
-  dumpRawBGR(d_srcBGR, pitch, width, height, batchSize, folder, tag);
-}
-
-void dumpYUV(unsigned char *d_nv12, int size, char *folder, char *tag) {
-  unsigned char *nv12Data;
-  std::ofstream *nv12File;
-  char filename[120];
-  char directory[60];
-  char mkdir_cmd[256];
-#if !defined(_WIN32)
-  sprintf(directory, "output/%s", folder);
-  sprintf(mkdir_cmd, "mkdir -p %s 2> /dev/null", directory);
-#else
-  sprintf(directory, "output\\%s", folder);
-  sprintf(mkdir_cmd, "mkdir %s 2> nul", directory);
-#endif
-
-  int ret = system(mkdir_cmd);
-
-  sprintf(filename, "%s/%s.nv12", directory, tag);
-
-  nv12File = new std::ofstream(filename);
-  if (nv12File == NULL) {
-    std::cerr << "Failed to new " << filename;
-    return;
-  }
-
-  nv12Data = (unsigned char *)malloc(size * (sizeof(char)));
-  if (nv12Data == NULL) {
-    std::cerr << "Failed to allcoate memory\n";
-    return;
-  }
-
-  hipMemcpy((void *)nv12Data, (void *)d_nv12, size, hipMemcpyDeviceToHost);
-
-  nv12File->write((const char *)nv12Data, size);
-
-  free(nv12Data);
-  delete nv12File;
-}
diff --git a/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip b/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip
index 64003f9..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/SobolQRNG/sobol_gpu.cu.hip
@@ -1,209 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * Portions Copyright (c) 2009 Mike Giles, Oxford University.  All rights
- * reserved.
- * Portions Copyright (c) 2008 Frances Y. Kuo and Stephen Joe.  All rights
- * reserved.
- *
- * Sobol Quasi-random Number Generator example
- *
- * Based on CUDA code submitted by Mike Giles, Oxford University, United Kingdom
- * http://people.maths.ox.ac.uk/~gilesm/
- *
- * and C code developed by Stephen Joe, University of Waikato, New Zealand
- * and Frances Kuo, University of New South Wales, Australia
- * http://web.maths.unsw.edu.au/~fkuo/sobol/
- *
- * For theoretical background see:
- *
- * P. Bratley and B.L. Fox.
- * Implementing Sobol's quasirandom sequence generator
- * http://portal.acm.org/citation.cfm?id=42288
- * ACM Trans. on Math. Software, 14(1):88-100, 1988
- *
- * S. Joe and F. Kuo.
- * Remark on algorithm 659: implementing Sobol's quasirandom sequence generator.
- * http://portal.acm.org/citation.cfm?id=641879
- * ACM Trans. on Math. Software, 29(1):49-57, 2003
- *
- */
-
-
-#include <hip/hip_runtime.h>
-#include "sobol.h"
-#include "sobol_gpu.h"
-#include <hip/hip_cooperative_groups.h>
-
-namespace cg = cooperative_groups;
-#include <helper_cuda.h>
-
-#define k_2powneg32 2.3283064E-10F
-
-__global__ void sobolGPU_kernel(unsigned n_vectors, unsigned n_dimensions,
-                                unsigned *d_directions, float *d_output) {
-  // Handle to thread block group
-  cg::thread_block cta = cg::this_thread_block();
-  __shared__ unsigned int v[n_directions];
-
-  // Offset into the correct dimension as specified by the
-  // block y coordinate
-  d_directions = d_directions + n_directions * blockIdx.y;
-  d_output = d_output + n_vectors * blockIdx.y;
-
-  // Copy the direction numbers for this dimension into shared
-  // memory - there are only 32 direction numbers so only the
-  // first 32 (n_directions) threads need participate.
-  if (threadIdx.x < n_directions) {
-    v[threadIdx.x] = d_directions[threadIdx.x];
-  }
-
-  cg::sync(cta);
-
-  // Set initial index (i.e. which vector this thread is
-  // computing first) and stride (i.e. step to the next vector
-  // for this thread)
-  int i0 = threadIdx.x + blockIdx.x * blockDim.x;
-  int stride = gridDim.x * blockDim.x;
-
-  // Get the gray code of the index
-  // c.f. Numerical Recipes in C, chapter 20
-  // http://www.nrbook.com/a/bookcpdf/c20-2.pdf
-  unsigned int g = i0 ^ (i0 >> 1);
-
-  // Initialisation for first point x[i0]
-  // In the Bratley and Fox paper this is equation (*), where
-  // we are computing the value for x[n] without knowing the
-  // value of x[n-1].
-  unsigned int X = 0;
-  unsigned int mask;
-
-  for (unsigned int k = 0; k < __ffs(stride) - 1; k++) {
-    // We want X ^= g_k * v[k], where g_k is one or zero.
-    // We do this by setting a mask with all bits equal to
-    // g_k. In reality we keep shifting g so that g_k is the
-    // LSB of g. This way we avoid multiplication.
-    mask = -(g & 1);
-    X ^= mask & v[k];
-    g = g >> 1;
-  }
-
-  if (i0 < n_vectors) {
-    d_output[i0] = (float)X * k_2powneg32;
-  }
-
-  // Now do rest of points, using the stride
-  // Here we want to generate x[i] from x[i-stride] where we
-  // don't have any of the x in between, therefore we have to
-  // revisit the equation (**), this is easiest with an example
-  // so assume stride is 16.
-  // From x[n] to x[n+16] there will be:
-  //   8 changes in the first bit
-  //   4 changes in the second bit
-  //   2 changes in the third bit
-  //   1 change in the fourth
-  //   1 change in one of the remaining bits
-  //
-  // What this means is that in the equation:
-  //   x[n+1] = x[n] ^ v[p]
-  //   x[n+2] = x[n+1] ^ v[q] = x[n] ^ v[p] ^ v[q]
-  //   ...
-  // We will apply xor with v[1] eight times, v[2] four times,
-  // v[3] twice, v[4] once and one other direction number once.
-  // Since two xors cancel out, we can skip even applications
-  // and just apply xor with v[4] (i.e. log2(16)) and with
-  // the current applicable direction number.
-  // Note that all these indices count from 1, so we need to
-  // subtract 1 from them all to account for C arrays counting
-  // from zero.
-  unsigned int v_log2stridem1 = v[__ffs(stride) - 2];
-  unsigned int v_stridemask = stride - 1;
-
-  for (unsigned int i = i0 + stride; i < n_vectors; i += stride) {
-    // x[i] = x[i-stride] ^ v[b] ^ v[c]
-    //  where b is log2(stride) minus 1 for C array indexing
-    //  where c is the index of the rightmost zero bit in i,
-    //  not including the bottom log2(stride) bits, minus 1
-    //  for C array indexing
-    // In the Bratley and Fox paper this is equation (**)
-    X ^= v_log2stridem1 ^ v[__ffs(~((i - stride) | v_stridemask)) - 1];
-    d_output[i] = (float)X * k_2powneg32;
-  }
-}
-
-extern "C" void sobolGPU(int n_vectors, int n_dimensions,
-                         unsigned int *d_directions, float *d_output) {
-  const int threadsperblock = 64;
-
-  // Set up the execution configuration
-  dim3 dimGrid;
-  dim3 dimBlock;
-
-  int device;
-  hipDeviceProp_t prop;
-  HIPCHECK(hipGetDevice(&device));
-  HIPCHECK(hipGetDeviceProperties(&prop, device));
-
-  // This implementation of the generator outputs all the draws for
-  // one dimension in a contiguous region of memory, followed by the
-  // next dimension and so on.
-  // Therefore all threads within a block will be processing different
-  // vectors from the same dimension. As a result we want the total
-  // number of blocks to be a multiple of the number of dimensions.
-  dimGrid.y = n_dimensions;
-
-  // If the number of dimensions is large then we will set the number
-  // of blocks to equal the number of dimensions (i.e. dimGrid.x = 1)
-  // but if the number of dimensions is small (e.g. less than four per
-  // multiprocessor) then we'll partition the vectors across blocks
-  // (as well as threads).
-  if (n_dimensions < (4 * prop.multiProcessorCount)) {
-    dimGrid.x = 4 * prop.multiProcessorCount;
-  } else {
-    dimGrid.x = 1;
-  }
-
-  // Cap the dimGrid.x if the number of vectors is small
-  if (dimGrid.x > (unsigned int)(n_vectors / threadsperblock)) {
-    dimGrid.x = (n_vectors + threadsperblock - 1) / threadsperblock;
-  }
-
-  // Round up to a power of two, required for the algorithm so that
-  // stride is a power of two.
-  unsigned int targetDimGridX = dimGrid.x;
-
-  for (dimGrid.x = 1; dimGrid.x < targetDimGridX; dimGrid.x *= 2)
-    ;
-
-  // Fix the number of threads
-  dimBlock.x = threadsperblock;
-
-  // Execute GPU kernel
-  sobolGPU_kernel<<<dimGrid, dimBlock>>>(n_vectors, n_dimensions, d_directions,
-                                         d_output);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip
index 9f1163c..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/VFlockingD3D10/VFlocking_kernel.cu.hip
@@ -1,292 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-
-#include <helper_cuda.h>
-#include <helper_math.h>
-#include <VFlockingD3D10.h>
-
-#define PI 3.1415926536f
-
-typedef unsigned int uint;
-
-__device__ bool isInsideQuad_D(float2 pos0, float2 pos1, float width,
-                               float height) {
-  if (fabs(pos0.x - pos1.x) < 0.5f * width &&
-      fabs(pos0.y - pos1.y) < 0.5f * height) {
-    return true;
-  } else {
-    return false;
-  }
-}
-
-__device__ bool isInsideBird(float2 pixel, float2 pos, float width,
-                             float height, float radius) {
-  if (abs(pixel.x - pos.x) < 0.5f * width &&
-          abs(pixel.y - pos.y) < 0.5f * height ||
-      (pixel.x - pos.x) * (pixel.x - pos.x) +
-              (pixel.y - pos.y) * (pixel.y - pos.y) <
-          radius * radius) {
-    return true;
-  } else {
-    return false;
-  }
-}
-
-__global__ void cuda_kernel_update(float2 *newPos, float2 *curPos,
-                                   uint numBirds, bool *hasproxy,
-                                   bool *neighbors, bool *rightgoals,
-                                   bool *leftgoals, Params *params) {
-  uint i = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (i >= numBirds) {
-    return;
-  }
-
-  float minDist = 50000.f;
-  float2 dij = make_float2(0.f);
-
-  if (!hasproxy[i]) {
-    for (uint j = 0; j < numBirds; j++) {
-      if (j == i) {
-        continue;
-      }
-
-      if (leftgoals[i * numBirds + j]) {
-        dij = params->dX * normalize(curPos[j] - curPos[i]);
-        break;
-      }
-    }
-  } else {
-    bool collision = false;
-
-    for (uint j = 0; j < numBirds; j++) {
-      float d;
-
-      if (leftgoals[i * numBirds + j]) {
-        d = curPos[j].x - (params->wingspan + params->lambda) - curPos[i].x;
-
-        if (fabs(d) < fabs(minDist)) {
-          minDist = d;
-        }
-      }
-
-      if (rightgoals[i * numBirds + j]) {
-        d = curPos[j].x + (params->wingspan + params->lambda) - curPos[i].x;
-
-        if (fabs(d) < fabs(minDist)) {
-          minDist = d;
-        }
-      }
-
-      if (neighbors[i * numBirds + j] && !collision) {
-        if (curPos[j].y >= curPos[i].y &&
-            curPos[j].y < curPos[i].y + params->epsilon) {
-          dij.y = -params->dY;
-          collision = true;
-        }
-      }
-    }
-
-    if (fabs(minDist) <= params->dX) {
-      return;
-    }
-
-    dij.x = minDist > 0 ? params->dX : -params->dX;
-  }
-
-  newPos[i].x = curPos[i].x + dij.x;
-  newPos[i].y = curPos[i].y + dij.y;
-}
-
-__global__ void cuda_kernel_checktriples(float2 *pos, uint numBirds,
-                                         bool *hasproxy, bool *neighbors,
-                                         bool *rightgoals, bool *leftgoals,
-                                         uint3 *triples, Params *params) {
-  uint ith = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (ith >= numBirds * (numBirds - 1) * (numBirds - 2) / 6) {
-    return;
-  }
-
-  uint a[3];
-  a[0] = triples[ith].x;
-  a[1] = triples[ith].y;
-  a[2] = triples[ith].z;
-
-  uint i, j, x;
-
-  for (i = 0; i < 3; i++) {
-    for (j = 2; j > i; j--) {
-      if (pos[a[j - 1]].y > pos[a[j]].y) {
-        x = a[j - 1];
-        a[j - 1] = a[j];
-        a[j] = x;
-      }
-    }
-  }
-
-  if (hasproxy[a[0]]) {
-    float a2a1 = pos[a[2]].x - pos[a[1]].x;
-
-    if (fabs(a2a1) < 2.f * (params->wingspan + params->lambda))
-      if (a2a1 >= 0) {
-        if (leftgoals[a[0] * numBirds + a[2]]) {
-          leftgoals[a[0] * numBirds + a[2]] = false;
-        }
-
-        if (rightgoals[a[0] * numBirds + a[1]]) {
-          rightgoals[a[0] * numBirds + a[1]] = false;
-        }
-      } else {
-        if (leftgoals[a[0] * numBirds + a[1]]) {
-          leftgoals[a[0] * numBirds + a[1]] = false;
-        }
-
-        if (rightgoals[a[0] * numBirds + a[2]]) {
-          rightgoals[a[0] * numBirds + a[2]] = false;
-        }
-      }
-  } else {
-    if ((leftgoals[a[0] * numBirds + a[2]]) &&
-        (leftgoals[a[0] * numBirds + a[1]]))
-      if ((length(pos[a[1]] - pos[a[0]]) < length(pos[a[2]] - pos[a[0]]))) {
-        leftgoals[a[0] * numBirds + a[2]] = false;
-      } else {
-        leftgoals[a[0] * numBirds + a[1]] = false;
-      }
-  }
-}
-
-__global__ void cuda_kernel_checkpairs(float2 *pos, uint numBirds,
-                                       bool *hasproxy, bool *neighbors,
-                                       bool *rightgoals, bool *leftgoals,
-                                       uint2 *pairs, Params *params) {
-  uint i = blockIdx.x * blockDim.x + threadIdx.x;
-
-  if (i >= numBirds * (numBirds - 1) / 2) {
-    return;
-  }
-
-  uint front, back;
-
-  if (pos[pairs[i].y].y > pos[pairs[i].x].y) {
-    front = pairs[i].y;
-    back = pairs[i].x;
-  } else {
-    front = pairs[i].x;
-    back = pairs[i].y;
-  }
-
-  leftgoals[back * numBirds + front] = true;
-  rightgoals[back * numBirds + front] = true;
-
-  float2 stepback;
-  stepback.x = pos[front].x;
-  stepback.y = pos[front].y - 0.5f * params->upwashY;
-
-  if (isInsideQuad_D(
-          pos[back], stepback,
-          2.f * (params->wingspan + params->lambda + params->upwashX),
-          params->upwashY)) {
-    neighbors[back * numBirds + front] = true;
-
-    if (!hasproxy[back]) {
-      hasproxy[back] = true;
-    }
-  }
-}
-
-extern "C" void cuda_simulate(float2 *newPos, float2 *curPos, uint numBirds,
-                              bool *d_hasproxy, bool *d_neighbors,
-                              bool *d_leftgoals, bool *d_rightgoals,
-                              uint2 *d_pairs, uint3 *d_triples,
-                              Params *d_params) {
-  hipError_t error = hipSuccess;
-  float tempms;
-  static float ms = 0.f;
-  static uint step = 0;
-  int smallblockSize = 32, midblockSize = 128, bigblockSize = 32;
-
-  hipEvent_t e_start, e_stop;
-  hipEventCreate(&e_start);
-  hipEventCreate(&e_stop);
-  hipEventRecord(e_start, 0);
-
-  hipMemset(d_leftgoals, 0, numBirds * numBirds * sizeof(bool));
-  hipMemset(d_rightgoals, 0, numBirds * numBirds * sizeof(bool));
-  hipMemset(d_hasproxy, 0, numBirds * sizeof(bool));
-  hipMemset(d_neighbors, 0, numBirds * numBirds * sizeof(bool));
-
-  dim3 Db = dim3(bigblockSize);
-  dim3 Dg =
-      dim3((numBirds * (numBirds - 1) / 2 + bigblockSize - 1) / bigblockSize);
-  cuda_kernel_checkpairs<<<Dg, Db>>>(curPos, numBirds, d_hasproxy, d_neighbors,
-                                     d_rightgoals, d_leftgoals, d_pairs,
-                                     d_params);
-
-  Db = dim3(midblockSize);
-  Dg =
-      dim3((numBirds * (numBirds - 1) * (numBirds - 2) / 6 + bigblockSize - 1) /
-           bigblockSize);
-  cuda_kernel_checktriples<<<Dg, Db>>>(curPos, numBirds, d_hasproxy,
-                                       d_neighbors, d_rightgoals, d_leftgoals,
-                                       d_triples, d_params);
-
-  Db = dim3(smallblockSize);
-  Dg = dim3((numBirds + smallblockSize - 1) / smallblockSize);
-  cuda_kernel_update<<<Dg, Db>>>(newPos, curPos, numBirds, d_hasproxy,
-                                 d_neighbors, d_rightgoals, d_leftgoals,
-                                 d_params /*, d_pWingTips */);
-
-  hipDeviceSynchronize();
-
-  hipEventRecord(e_stop, 0);
-  hipEventSynchronize(e_stop);
-  hipEventElapsedTime(&tempms, e_start, e_stop);
-  ms += tempms;
-
-  if (!(step % 100) && step) {
-    printf("GPU, step %d \ntime per step %6.3f ms \n", step, ms / 100.f);
-    ms = 0.f;
-  }
-
-  step++;
-
-  error = hipGetLastError();
-
-  if (error != hipSuccess) {
-    printf("one of the cuda kernels failed to launch, error = %d\n", error);
-  }
-}
diff --git a/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip
index d86dba5..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/bicubicTexture/bicubicTexture_cuda.cu.hip
@@ -1,134 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _BICUBICTEXTURE_CU_
-#define _BICUBICTEXTURE_CU_
-
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-
-#include <helper_math.h>
-
-// includes, cuda
-#include <helper_cuda.h>
-
-typedef unsigned int uint;
-typedef unsigned char uchar;
-
-#include "bicubicTexture_kernel.cuh"
-
-hipArray *d_imageArray = 0;
-
-extern "C" void initTexture(int imageWidth, int imageHeight, uchar *h_data) {
-  // allocate array and copy image data
-  hipChannelFormatDesc channelDesc =
-      hipCreateChannelDesc(8, 0, 0, 0, hipChannelFormatKindUnsigned);
-  HIPCHECK(
-      hipMallocArray(&d_imageArray, &channelDesc, imageWidth, imageHeight));
-  HIPCHECK(hipMemcpy2DToArray(
-      d_imageArray, 0, 0, h_data, imageWidth * sizeof(uchar),
-      imageWidth * sizeof(uchar), imageHeight, hipMemcpyHostToDevice));
-  free(h_data);
-
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = d_imageArray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeClamp;
-  texDescr.addressMode[1] = hipAddressModeClamp;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texObjLinear, &texRes, &texDescr, NULL));
-
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModePoint;
-  texDescr.addressMode[0] = hipAddressModeClamp;
-  texDescr.addressMode[1] = hipAddressModeClamp;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texObjPoint, &texRes, &texDescr, NULL));
-}
-
-extern "C" void freeTexture() {
-  HIPCHECK(hipDestroyTextureObject(texObjPoint));
-  HIPCHECK(hipDestroyTextureObject(texObjLinear));
-  HIPCHECK(hipFreeArray(d_imageArray));
-}
-
-// render image using CUDA
-extern "C" void render(int width, int height, float tx, float ty, float scale,
-                       float cx, float cy, dim3 blockSize, dim3 gridSize,
-                       int filter_mode, uchar4 *output) {
-  // call CUDA kernel, writing results to PBO memory
-  switch (filter_mode) {
-    case MODE_NEAREST:
-      d_render<<<gridSize, blockSize>>>(output, width, height, tx, ty, scale,
-                                        cx, cy, texObjPoint);
-      break;
-
-    case MODE_BILINEAR:
-      d_render<<<gridSize, blockSize>>>(output, width, height, tx, ty, scale,
-                                        cx, cy, texObjLinear);
-      break;
-
-    case MODE_BICUBIC:
-      d_renderBicubic<<<gridSize, blockSize>>>(output, width, height, tx, ty,
-                                               scale, cx, cy, texObjPoint);
-      break;
-
-    case MODE_FAST_BICUBIC:
-      d_renderFastBicubic<<<gridSize, blockSize>>>(
-          output, width, height, tx, ty, scale, cx, cy, texObjLinear);
-      break;
-
-    case MODE_CATROM:
-      d_renderCatRom<<<gridSize, blockSize>>>(output, width, height, tx, ty,
-                                              scale, cx, cy, texObjPoint);
-      break;
-  }
-
-  getLastCudaError("kernel failed");
-}
-
-#endif
-
-
-  getLastCudaError("kernel failed");
-}
diff --git a/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip
index df5e15f..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/bilateralFilter/bilateral_kernel.cu.hip
@@ -1,266 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <helper_math.h>
-#include <helper_functions.h>
-#include <helper_cuda.h>  // CUDA device initialization helper functions
-
-__constant__ float cGaussian[64];  // gaussian array in device side
-
-hipTextureObject_t rgbaTexdImage;
-hipTextureObject_t rgbaTexdTemp;
-
-uint *dImage = NULL;  // original image
-uint *dTemp = NULL;  // temp array for iterations
-size_t pitch;
-
-/*
-    Perform a simple bilateral filter.
-
-    Bilateral filter is a nonlinear filter that is a mixture of range
-    filter and domain filter, the previous one preserves crisp edges and
-    the latter one filters noise. The intensity value at each pixel in
-    an image is replaced by a weighted average of intensity values from
-    nearby pixels.
-
-    The weight factor is calculated by the product of domain filter
-    component(using the gaussian distribution as a spatial distance) as
-    well as range filter component(Euclidean distance between center pixel
-    and the current neighbor pixel). Because this process is nonlinear,
-    the sample just uses a simple pixel by pixel step.
-
-    Texture fetches automatically clamp to edge of image. 1D gaussian array
-    is mapped to a 1D texture instead of using shared memory, which may
-    cause severe bank conflict.
-
-    Threads are y-pass(column-pass), because the output is coalesced.
-
-    Parameters
-    od - pointer to output data in global memory
-    d_f - pointer to the 1D gaussian array
-    e_d - euclidean delta
-    w  - image width
-    h  - image height
-    r  - filter radius
-*/
-
-// Euclidean Distance (x, y, d) = exp((|x - y| / d)^2 / 2)
-__device__ float euclideanLen(float4 a, float4 b, float d) {
-  float mod = (b.x - a.x) * (b.x - a.x) + (b.y - a.y) * (b.y - a.y) +
-              (b.z - a.z) * (b.z - a.z);
-
-  return __expf(-mod / (2.f * d * d));
-}
-
-__device__ uint rgbaFloatToInt(float4 rgba) {
-  rgba.x = __saturatef(fabs(rgba.x));  // clamp to [0.0, 1.0]
-  rgba.y = __saturatef(fabs(rgba.y));
-  rgba.z = __saturatef(fabs(rgba.z));
-  rgba.w = __saturatef(fabs(rgba.w));
-  return (uint(rgba.w * 255.0f) << 24) | (uint(rgba.z * 255.0f) << 16) |
-         (uint(rgba.y * 255.0f) << 8) | uint(rgba.x * 255.0f);
-}
-
-__device__ float4 rgbaIntToFloat(uint c) {
-  float4 rgba;
-  rgba.x = (c & 0xff) * 0.003921568627f;          //  /255.0f;
-  rgba.y = ((c >> 8) & 0xff) * 0.003921568627f;   //  /255.0f;
-  rgba.z = ((c >> 16) & 0xff) * 0.003921568627f;  //  /255.0f;
-  rgba.w = ((c >> 24) & 0xff) * 0.003921568627f;  //  /255.0f;
-  return rgba;
-}
-
-// column pass using coalesced global memory reads
-__global__ void d_bilateral_filter(uint *od, int w, int h, float e_d, int r,
-                                   hipTextureObject_t rgbaTex) {
-  int x = blockIdx.x * blockDim.x + threadIdx.x;
-  int y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  if (x >= w || y >= h) {
-    return;
-  }
-
-  float sum = 0.0f;
-  float factor;
-  float4 t = {0.f, 0.f, 0.f, 0.f};
-  float4 center = tex2D<float4>(rgbaTex, x, y);
-
-  for (int i = -r; i <= r; i++) {
-    for (int j = -r; j <= r; j++) {
-      float4 curPix = tex2D<float4>(rgbaTex, x + j, y + i);
-      factor = cGaussian[i + r] * cGaussian[j + r] *  // domain factor
-               euclideanLen(curPix, center, e_d);  // range factor
-
-      t += factor * curPix;
-      sum += factor;
-    }
-  }
-
-  od[y * w + x] = rgbaFloatToInt(t / sum);
-}
-
-extern "C" void initTexture(int width, int height, uint *hImage) {
-  // copy image data to array
-  HIPCHECK(
-      hipMallocPitch(&dImage, &pitch, sizeof(uint) * width, height));
-  HIPCHECK(
-      hipMallocPitch(&dTemp, &pitch, sizeof(uint) * width, height));
-  HIPCHECK(hipMemcpy2D(dImage, pitch, hImage, sizeof(uint) * width,
-                               sizeof(uint) * width, height,
-                               hipMemcpyHostToDevice));
-
-  // texture<uchar4, 2, cudaReadModeNormalizedFloat> rgbaTex;
-  hipChannelFormatDesc desc = hipCreateChannelDesc<uchar4>();
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypePitch2D;
-  texRes.res.pitch2D.devPtr = dImage;
-  texRes.res.pitch2D.desc = desc;
-  texRes.res.pitch2D.width = width;
-  texRes.res.pitch2D.height = height;
-  texRes.res.pitch2D.pitchInBytes = pitch;
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModePoint;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(
-      hipCreateTextureObject(&rgbaTexdImage, &texRes, &texDescr, NULL));
-
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypePitch2D;
-  texRes.res.pitch2D.devPtr = dTemp;
-  texRes.res.pitch2D.desc = desc;
-  texRes.res.pitch2D.width = width;
-  texRes.res.pitch2D.height = height;
-  texRes.res.pitch2D.pitchInBytes = pitch;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(
-      hipCreateTextureObject(&rgbaTexdTemp, &texRes, &texDescr, NULL));
-}
-
-extern "C" void freeTextures() {
-  HIPCHECK(hipDestroyTextureObject(rgbaTexdImage));
-  HIPCHECK(hipDestroyTextureObject(rgbaTexdTemp));
-  HIPCHECK(hipFree(dImage));
-  HIPCHECK(hipFree(dTemp));
-}
-
-/*
-    Because a 2D gaussian mask is symmetry in row and column,
-    here only generate a 1D mask, and use the product by row
-    and column index later.
-
-    1D gaussian distribution :
-        g(x, d) -- C * exp(-x^2/d^2), C is a constant amplifier
-
-    parameters:
-    og - output gaussian array in global memory
-    delta - the 2nd parameter 'd' in the above function
-    radius - half of the filter size
-             (total filter size = 2 * radius + 1)
-*/
-extern "C" void updateGaussian(float delta, int radius) {
-  float fGaussian[64];
-
-  for (int i = 0; i < 2 * radius + 1; ++i) {
-    float x = (float)(i - radius);
-    fGaussian[i] = expf(-(x * x) / (2 * delta * delta));
-  }
-
-  HIP_SYMBOL(cGaussian)(hipMemcpyToSymbol(cGaussian, fGaussian,
-                                     sizeof(float) * (2 * radius + 1)));
-}
-
-/*
-    Perform 2D bilateral filter on image using CUDA
-
-    Parameters:
-    d_dest - pointer to destination image in device memory
-    width  - image width
-    height - image height
-    e_d    - euclidean delta
-    radius - filter radius
-    iterations - number of iterations
-*/
-
-// RGBA version
-extern "C" double bilateralFilterRGBA(uint *dDest, int width, int height,
-                                      float e_d, int radius, int iterations,
-                                      StopWatchInterface *timer) {
-  // var for kernel computation timing
-  double dKernelTime;
-
-  for (int i = 0; i < iterations; i++) {
-    // sync host and start kernel computation timer
-    dKernelTime = 0.0;
-    HIPCHECK(hipDeviceSynchronize());
-    sdkResetTimer(&timer);
-
-    dim3 gridSize((width + 16 - 1) / 16, (height + 16 - 1) / 16);
-    dim3 blockSize(16, 16);
-
-    if (iterations > 1) {
-      d_bilateral_filter<<<gridSize, blockSize>>>(dDest, width, height, e_d,
-                                                  radius, rgbaTexdTemp);
-    } else {
-      d_bilateral_filter<<<gridSize, blockSize>>>(dDest, width, height, e_d,
-                                                  radius, rgbaTexdImage);
-    }
-
-    // sync host and stop computation timer
-    HIPCHECK(hipDeviceSynchronize());
-    dKernelTime += sdkGetTimerValue(&timer);
-
-    if (iterations > 1) {
-      // copy result back from global memory to array
-      HIPCHECK(hipMemcpy2D(dTemp, pitch, dDest, sizeof(int) * width,
-                                   sizeof(int) * width, height,
-                                   hipMemcpyDeviceToDevice));
-    }
-  }
-
-  return ((dKernelTime / 1000.) / (double)iterations);
-}
-viceToDevice));
-    }
-  }
diff --git a/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip b/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip
index f90f6cf..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/convolutionFFT2D/convolutionFFT2D.cu.hip
@@ -1,323 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <assert.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-#include <helper_cuda.h>
-#include "convolutionFFT2D_common.h"
-#include "convolutionFFT2D.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-/// Position convolution kernel center at (0, 0) in the image
-////////////////////////////////////////////////////////////////////////////////
-extern "C" void padKernel(float *d_Dst, float *d_Src, int fftH, int fftW,
-                          int kernelH, int kernelW, int kernelY, int kernelX) {
-  assert(d_Src != d_Dst);
-  dim3 threads(32, 8);
-  dim3 grid(iDivUp(kernelW, threads.x), iDivUp(kernelH, threads.y));
-
-  SET_FLOAT_BASE;
-#if (USE_TEXTURE)
-  hipTextureObject_t texFloat;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeLinear;
-  texRes.res.linear.devPtr = d_Src;
-  texRes.res.linear.sizeInBytes = sizeof(float) * kernelH * kernelW;
-  texRes.res.linear.desc = hipCreateChannelDesc<float>();
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&texFloat, &texRes, &texDescr, NULL));
-#endif
-
-  padKernel_kernel<<<grid, threads>>>(d_Dst, d_Src, fftH, fftW, kernelH,
-                                      kernelW, kernelY, kernelX
-#if (USE_TEXTURE)
-                                      ,
-                                      texFloat
-#endif
-                                      );
-  getLastCudaError("padKernel_kernel<<<>>> execution failed\n");
-
-#if (USE_TEXTURE)
-  HIPCHECK(hipDestroyTextureObject(texFloat));
-#endif
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Prepare data for "pad to border" addressing mode
-////////////////////////////////////////////////////////////////////////////////
-extern "C" void padDataClampToBorder(float *d_Dst, float *d_Src, int fftH,
-                                     int fftW, int dataH, int dataW,
-                                     int kernelW, int kernelH, int kernelY,
-                                     int kernelX) {
-  assert(d_Src != d_Dst);
-  dim3 threads(32, 8);
-  dim3 grid(iDivUp(fftW, threads.x), iDivUp(fftH, threads.y));
-
-#if (USE_TEXTURE)
-  hipTextureObject_t texFloat;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeLinear;
-  texRes.res.linear.devPtr = d_Src;
-  texRes.res.linear.sizeInBytes = sizeof(float) * dataH * dataW;
-  texRes.res.linear.desc = hipCreateChannelDesc<float>();
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&texFloat, &texRes, &texDescr, NULL));
-#endif
-
-  padDataClampToBorder_kernel<<<grid, threads>>>(
-      d_Dst, d_Src, fftH, fftW, dataH, dataW, kernelH, kernelW, kernelY, kernelX
-#if (USE_TEXTURE)
-      ,
-      texFloat
-#endif
-      );
-  getLastCudaError("padDataClampToBorder_kernel<<<>>> execution failed\n");
-
-#if (USE_TEXTURE)
-  HIPCHECK(hipDestroyTextureObject(texFloat));
-#endif
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Modulate Fourier image of padded data by Fourier image of padded kernel
-// and normalize by FFT size
-////////////////////////////////////////////////////////////////////////////////
-extern "C" void modulateAndNormalize(fComplex *d_Dst, fComplex *d_Src, int fftH,
-                                     int fftW, int padding) {
-  assert(fftW % 2 == 0);
-  const int dataSize = fftH * (fftW / 2 + padding);
-
-  modulateAndNormalize_kernel<<<iDivUp(dataSize, 256), 256>>>(
-      d_Dst, d_Src, dataSize, 1.0f / (float)(fftW * fftH));
-  getLastCudaError("modulateAndNormalize() execution failed\n");
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// 2D R2C / C2R post/preprocessing kernels
-////////////////////////////////////////////////////////////////////////////////
-static const double PI = 3.1415926535897932384626433832795;
-static const uint BLOCKDIM = 256;
-
-extern "C" void spPostprocess2D(void *d_Dst, void *d_Src, uint DY, uint DX,
-                                uint padding, int dir) {
-  assert(d_Src != d_Dst);
-  assert(DX % 2 == 0);
-
-#if (POWER_OF_TWO)
-  uint log2DX, log2DY;
-  uint factorizationRemX = factorRadix2(log2DX, DX);
-  uint factorizationRemY = factorRadix2(log2DY, DY);
-  assert(factorizationRemX == 1 && factorizationRemY == 1);
-#endif
-
-  const uint threadCount = DY * (DX / 2);
-  const double phaseBase = dir * PI / (double)DX;
-
-#if (USE_TEXTURE)
-  hipTextureObject_t texComplex;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeLinear;
-  texRes.res.linear.devPtr = d_Src;
-  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * (DX + padding);
-  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texComplex, &texRes, &texDescr, NULL));
-#endif
-
-  spPostprocess2D_kernel<<<iDivUp(threadCount, BLOCKDIM), BLOCKDIM>>>(
-      (fComplex *)d_Dst, (fComplex *)d_Src, DY, DX, threadCount, padding,
-      (float)phaseBase
-#if (USE_TEXTURE)
-      ,
-      texComplex
-#endif
-      );
-  getLastCudaError("spPostprocess2D_kernel<<<>>> execution failed\n");
-
-#if (USE_TEXTURE)
-  HIPCHECK(hipDestroyTextureObject(texComplex));
-#endif
-}
-
-extern "C" void spPreprocess2D(void *d_Dst, void *d_Src, uint DY, uint DX,
-                               uint padding, int dir) {
-  assert(d_Src != d_Dst);
-  assert(DX % 2 == 0);
-
-#if (POWER_OF_TWO)
-  uint log2DX, log2DY;
-  uint factorizationRemX = factorRadix2(log2DX, DX);
-  uint factorizationRemY = factorRadix2(log2DY, DY);
-  assert(factorizationRemX == 1 && factorizationRemY == 1);
-#endif
-
-  const uint threadCount = DY * (DX / 2);
-  const double phaseBase = -dir * PI / (double)DX;
-
-#if (USE_TEXTURE)
-  hipTextureObject_t texComplex;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeLinear;
-  texRes.res.linear.devPtr = d_Src;
-  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * (DX + padding);
-  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texComplex, &texRes, &texDescr, NULL));
-#endif
-  spPreprocess2D_kernel<<<iDivUp(threadCount, BLOCKDIM), BLOCKDIM>>>(
-      (fComplex *)d_Dst, (fComplex *)d_Src, DY, DX, threadCount, padding,
-      (float)phaseBase
-#if (USE_TEXTURE)
-      ,
-      texComplex
-#endif
-      );
-  getLastCudaError("spPreprocess2D_kernel<<<>>> execution failed\n");
-
-#if (USE_TEXTURE)
-  HIPCHECK(hipDestroyTextureObject(texComplex));
-#endif
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Combined spPostprocess2D + modulateAndNormalize + spPreprocess2D
-////////////////////////////////////////////////////////////////////////////////
-extern "C" void spProcess2D(void *d_Dst, void *d_SrcA, void *d_SrcB, uint DY,
-                            uint DX, int dir) {
-  assert(DY % 2 == 0);
-
-#if (POWER_OF_TWO)
-  uint log2DX, log2DY;
-  uint factorizationRemX = factorRadix2(log2DX, DX);
-  uint factorizationRemY = factorRadix2(log2DY, DY);
-  assert(factorizationRemX == 1 && factorizationRemY == 1);
-#endif
-
-  const uint threadCount = (DY / 2) * DX;
-  const double phaseBase = dir * PI / (double)DX;
-
-#if (USE_TEXTURE)
-  hipTextureObject_t texComplexA, texComplexB;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeLinear;
-  texRes.res.linear.devPtr = d_SrcA;
-  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * DX;
-  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texComplexA, &texRes, &texDescr, NULL));
-
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeLinear;
-  texRes.res.linear.devPtr = d_SrcB;
-  texRes.res.linear.sizeInBytes = sizeof(fComplex) * DY * DX;
-  texRes.res.linear.desc = hipCreateChannelDesc<fComplex>();
-
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texComplexB, &texRes, &texDescr, NULL));
-#endif
-  spProcess2D_kernel<<<iDivUp(threadCount, BLOCKDIM), BLOCKDIM>>>(
-      (fComplex *)d_Dst, (fComplex *)d_SrcA, (fComplex *)d_SrcB, DY, DX,
-      threadCount, (float)phaseBase, 0.5f / (float)(DY * DX)
-#if (USE_TEXTURE)
-                                         ,
-      texComplexA, texComplexB
-#endif
-      );
-  getLastCudaError("spProcess2D_kernel<<<>>> execution failed\n");
-
-#if (USE_TEXTURE)
-  HIPCHECK(hipDestroyTextureObject(texComplexA));
-  HIPCHECK(hipDestroyTextureObject(texComplexB));
-#endif
-}
diff --git a/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip b/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip
index 4350f7e..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/dwtHaar1D/dwtHaar1D.cu.hip
@@ -1,398 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
-* 1D DWT for Haar wavelet and signals with a length which is a power of 2.
-* The code reduces bank conflicts and non-coalesced reads / writes as
-* appropriate but does not fully remove them because the computational
-* overhead to achieve this would outweighs the benefit (see inline comments
-* for more details).
-* Large signals are subdivided into sub-signals with 512 elements and the
-* wavelet transform for these is computed with one block over 10 decomposition
-* levels. The resulting signal consisting of the approximation coefficients at
-* level X is then processed in a subsequent step on the device. This requires
-* interblock synchronization which is only possible on host side.
-* Detail coefficients which have been computed are not further referenced
-* during the decomposition so that they can be stored directly in their final
-* position in global memory. The transform and its storing scheme preserve
-* locality in the coefficients so that these writes are coalesced.
-* Approximation coefficients are stored in shared memory because they are
-* needed to compute the subsequent decomposition step. The top most
-* approximation coefficient for a sub-signal processed by one block is stored
-* in a special global memory location to simplify the processing after the
-* interblock synchronization.
-* Most books on wavelets explain the Haar wavelet decomposition. A good freely
-* available resource is the Wavelet primer by Stollnitz et al.
-* http://grail.cs.washington.edu/projects/wavelets/article/wavelet1.pdf
-* http://grail.cs.washington.edu/projects/wavelets/article/wavelet2.pdf
-* The basic of all Wavelet transforms is to decompose a signal into
-* approximation (a) and detail (d) coefficients where the detail tends to be
-* small or zero which allows / simplifies compression. The following "graphs"
-* demonstrate the transform for a signal
-* of length eight. The index always describes the decomposition level where
-* a coefficient arises. The input signal is interpreted as approximation signal
-* at level 0. The coefficients computed on the device are stored in the same
-* scheme as in the example. This data structure is particularly well suited for
-* compression and also preserves the hierarchical structure of the
-decomposition.
-
--------------------------------------------------
-| a_0 | a_0 | a_0 | a_0 | a_0 | a_0 | a_0 | a_0 |
--------------------------------------------------
-
--------------------------------------------------
-| a_1 | a_1 | a_1 | a_1 | d_1 | d_1 | d_1 | d_1 |
--------------------------------------------------
-
--------------------------------------------------
-| a_2 | a_2 | d_2 | d_2 | d_1 | d_1 | d_1 | d_1 |
--------------------------------------------------
-
--------------------------------------------------
-| a_3 | d_3 | d_2 | d_2 | d_1 | d_1 | d_1 | d_1 |
--------------------------------------------------
-
-* Host code.
-*/
-
-#ifdef _WIN32
-#define NOMINMAX
-#endif
-
-// includes, system
-
-#include <hip/hip_runtime.h>
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-#include <assert.h>
-
-// includes, project
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-// constants which are used in host and device code
-#define INV_SQRT_2 0.70710678118654752440f;
-const unsigned int LOG_NUM_BANKS = 4;
-const unsigned int NUM_BANKS = 16;
-
-////////////////////////////////////////////////////////////////////////////////
-// includes, kernels
-#include "dwtHaar1D_kernel.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-void runTest(int argc, char **argv);
-bool getLevels(unsigned int len, unsigned int *levels);
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  // run test
-  runTest(argc, argv);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Perform the wavelet decomposition
-////////////////////////////////////////////////////////////////////////////////
-void runTest(int argc, char **argv) {
-  bool bResult = false;  // flag for final validation of the results
-
-  char *s_fname = NULL, *r_gold_fname = NULL;
-  char r_fname[256];
-  const char usage[] = {
-      "\nUsage:\n"
-      "  dwtHaar1D --signal=<signal_file> --result=<result_file> "
-      "--gold=<gold_file>\n\n"
-      "  <signal_file> Input file containing the signal\n"
-      "  <result_file> Output file storing the result of the wavelet "
-      "decomposition\n"
-      "  <gold_file>   Input file containing the reference result of the "
-      "wavelet decomposition\n"
-      "\nExample:\n"
-      "  ./dwtHaar1D\n"
-      "       --signal=signal.dat\n"
-      "       --result=result.dat\n"
-      "       --gold=regression.gold.dat\n"};
-
-  printf("%s Starting...\n\n", argv[0]);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  findCudaDevice(argc, (const char **)argv);
-
-  // file names, either specified as cmd line args or use default
-  if (argc == 4) {
-    char *tmp_sfname, *tmp_rfname, *tmp_goldfname;
-
-    if ((getCmdLineArgumentString(argc, (const char **)argv, "signal",
-                                  &tmp_sfname) != true) ||
-        (getCmdLineArgumentString(argc, (const char **)argv, "result",
-                                  &tmp_rfname) != true) ||
-        (getCmdLineArgumentString(argc, (const char **)argv, "gold",
-                                  &tmp_goldfname) != true)) {
-      fprintf(stderr, "Invalid input syntax.\n%s", usage);
-      exit(EXIT_FAILURE);
-    }
-
-    s_fname = sdkFindFilePath(tmp_sfname, argv[0]);
-    r_gold_fname = sdkFindFilePath(tmp_goldfname, argv[0]);
-    strcpy(r_fname, tmp_rfname);
-  } else {
-    s_fname = sdkFindFilePath("signal.dat", argv[0]);
-    r_gold_fname = sdkFindFilePath("regression.gold.dat", argv[0]);
-    strcpy(r_fname, "result.dat");
-  }
-
-  printf("source file    = \"%s\"\n", s_fname);
-  printf("reference file = \"%s\"\n", r_fname);
-  printf("gold file      = \"%s\"\n", r_gold_fname);
-
-  // read in signal
-  unsigned int slength = 0;
-  float *signal = NULL;
-
-  if (s_fname == NULL) {
-    fprintf(stderr, "Cannot find the file containing the signal.\n%s", usage);
-
-    exit(EXIT_FAILURE);
-  }
-
-  if (sdkReadFile(s_fname, &signal, &slength, false) == true) {
-    printf("Reading signal from \"%s\"\n", s_fname);
-  } else {
-    exit(EXIT_FAILURE);
-  }
-
-  // get the number of decompositions necessary to perform a full decomposition
-  unsigned int dlevels_complete = 0;
-
-  if (true != getLevels(slength, &dlevels_complete)) {
-    // error message
-    fprintf(stderr, "Signal length not supported.\n");
-    // cleanup and abort
-    free(signal);
-    exit(EXIT_FAILURE);
-  }
-
-  // device in data
-  float *d_idata = NULL;
-  // device out data
-  float *d_odata = NULL;
-  // device approx_final data
-  float *approx_final = NULL;
-  // The very final approximation coefficient has to be written to the output
-  // data, all others are reused as input data in the next global step and
-  // therefore have to be written to the input data again.
-  // The following flag indicates where to copy approx_final data
-  //   - 0 is input, 1 is output
-  int approx_is_input;
-
-  // allocate device mem
-  const unsigned int smem_size = sizeof(float) * slength;
-  HIPCHECK(hipMalloc((void **)&d_idata, smem_size));
-  HIPCHECK(hipMalloc((void **)&d_odata, smem_size));
-  HIPCHECK(hipMalloc((void **)&approx_final, smem_size));
-  // copy input data to device
-  HIPCHECK(
-      hipMemcpy(d_idata, signal, smem_size, hipMemcpyHostToDevice));
-
-  // total number of threads
-  // in the first decomposition step always one thread computes the average and
-  // detail signal for one pair of adjacent values
-  unsigned int num_threads_total_left = slength / 2;
-  // decomposition levels performed in the current / next step
-  unsigned int dlevels_step = dlevels_complete;
-
-  // 1D signal so the arrangement of elements is also 1D
-  dim3 block_size;
-  dim3 grid_size;
-
-  // number of decomposition levels left after one iteration on the device
-  unsigned int dlevels_left = dlevels_complete;
-
-  // if less or equal 1k elements, then the data can be processed in one block,
-  // this avoids the Wait-For-Idle (WFI) on host side which is necessary if the
-  // computation is split across multiple SM's if enough input data
-  if (dlevels_complete <= 10) {
-    // decomposition can be performed at once
-    block_size.x = num_threads_total_left;
-    approx_is_input = 0;
-  } else {
-    // 512 threads per block
-    grid_size.x = (num_threads_total_left / 512);
-    block_size.x = 512;
-
-    // 512 threads corresponds to 10 decomposition steps
-    dlevels_step = 10;
-    dlevels_left -= 10;
-
-    approx_is_input = 1;
-  }
-
-  // Initialize d_odata to 0.0f
-  initValue<<<grid_size, block_size>>>(d_odata, 0.0f);
-
-  // do until full decomposition is accomplished
-  while (0 != num_threads_total_left) {
-    // double the number of threads as bytes
-    unsigned int mem_shared = (2 * block_size.x) * sizeof(float);
-    // extra memory requirements to avoid bank conflicts
-    mem_shared += ((2 * block_size.x) / NUM_BANKS) * sizeof(float);
-
-    // run kernel
-    dwtHaar1D<<<grid_size, block_size, mem_shared>>>(
-        d_idata, d_odata, approx_final, dlevels_step, num_threads_total_left,
-        block_size.x);
-
-    // Copy approx_final to appropriate location
-    if (approx_is_input) {
-      HIPCHECK(hipMemcpy(d_idata, approx_final, grid_size.x * 4,
-                                 hipMemcpyDeviceToDevice));
-    } else {
-      HIPCHECK(hipMemcpy(d_odata, approx_final, grid_size.x * 4,
-                                 hipMemcpyDeviceToDevice));
-    }
-
-    // update level variables
-    if (dlevels_left < 10) {
-      // approx_final = d_odata;
-      approx_is_input = 0;
-    }
-
-    // more global steps necessary
-    dlevels_step = (dlevels_left > 10) ? dlevels_left - 10 : dlevels_left;
-    dlevels_left -= 10;
-
-    // after each step only half the threads are used any longer
-    // therefore after 10 steps 2^10 less threads
-    num_threads_total_left = num_threads_total_left >> 10;
-
-    // update block and grid size
-    grid_size.x =
-        (num_threads_total_left / 512) + (0 != (num_threads_total_left % 512))
-            ? 1
-            : 0;
-
-    if (grid_size.x <= 1) {
-      block_size.x = num_threads_total_left;
-    }
-  }
-
-  // get the result back from the server
-  // allocate mem for the result
-  float *odata = (float *)malloc(smem_size);
-  HIPCHECK(
-      hipMemcpy(odata, d_odata, smem_size, hipMemcpyDeviceToHost));
-
-  // post processing
-  // write file for regression test
-  if (r_fname == NULL) {
-    fprintf(stderr,
-            "Cannot write the output file storing the result of the wavelet "
-            "decomposition.\n%s",
-            usage);
-    exit(EXIT_FAILURE);
-  }
-
-  if (sdkWriteFile(r_fname, odata, slength, 0.001f, false) == true) {
-    printf("Writing result to \"%s\"\n", r_fname);
-  } else {
-    exit(EXIT_FAILURE);
-  }
-
-  // load the reference solution
-  unsigned int len_reference = 0;
-  float *reference = NULL;
-
-  if (r_gold_fname == NULL) {
-    fprintf(stderr,
-            "Cannot read the file containing the reference result of the "
-            "wavelet decomposition.\n%s",
-            usage);
-
-    exit(EXIT_FAILURE);
-  }
-
-  if (sdkReadFile(r_gold_fname, &reference, &len_reference, false) == true) {
-    printf("Reading reference result from \"%s\"\n", r_gold_fname);
-  } else {
-    exit(EXIT_FAILURE);
-  }
-
-  assert(slength == len_reference);
-
-  // compare the computed solution and the reference
-  bResult = (bool)sdkCompareL2fe(reference, odata, slength, 0.001f);
-  free(reference);
-
-  // free allocated host and device memory
-  HIPCHECK(hipFree(d_odata));
-  HIPCHECK(hipFree(d_idata));
-  HIPCHECK(hipFree(approx_final));
-
-  free(signal);
-  free(odata);
-  free(s_fname);
-  free(r_gold_fname);
-
-  printf(bResult ? "Test success!\n" : "Test failure!\n");
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! Get number of decomposition levels to perform a full decomposition
-//! Also check if the input signal size is suitable
-//! @return  true if the number of decomposition levels could be determined
-//!          and the signal length is supported by the implementation,
-//!          otherwise false
-//! @param   len  length of input signal
-//! @param   levels  number of decomposition levels necessary to perform a full
-//!           decomposition
-////////////////////////////////////////////////////////////////////////////////
-bool getLevels(unsigned int len, unsigned int *levels) {
-  bool retval = false;
-
-  // currently signals up to a length of 2^20 supported
-  for (unsigned int i = 0; i < 20; ++i) {
-    if (len == (1 << i)) {
-      *levels = i;
-      retval = true;
-      break;
-    }
-  }
-
-  return retval;
-}
- = i;
-      retval = true;
-      break;
-    }
-  }
diff --git a/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.cu.hip b/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.cu.hip
index c275ac0..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/fastWalshTransform/fastWalshTransform.cu.hip
@@ -1,173 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * Walsh transforms belong to a class of generalized Fourier transformations.
- * They have applications in various fields of electrical engineering
- * and numeric theory. In this sample we demonstrate efficient implementation
- * of naturally-ordered Walsh transform
- * (also known as Walsh-Hadamard or Hadamard transform) in CUDA and its
- * particular application to dyadic convolution computation.
- * Refer to excellent Jorg Arndt's "Algorithms for Programmers" textbook
- * http://www.jjj.de/fxt/fxtbook.pdf (Chapter 22)
- *
- * Victor Podlozhnyuk (vpodlozhnyuk@nvidia.com)
- */
-
-
-#include <hip/hip_runtime.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-#include <helper_functions.h>
-#include <helper_cuda.h>
-
-////////////////////////////////////////////////////////////////////////////////
-// Reference CPU FWT
-////////////////////////////////////////////////////////////////////////////////
-extern "C" void fwtCPU(float *h_Output, float *h_Input, int log2N);
-extern "C" void slowWTcpu(float *h_Output, float *h_Input, int log2N);
-extern "C" void dyadicConvolutionCPU(float *h_Result, float *h_Data,
-                                     float *h_Kernel, int log2dataN,
-                                     int log2kernelN);
-
-////////////////////////////////////////////////////////////////////////////////
-// GPU FWT
-////////////////////////////////////////////////////////////////////////////////
-#include "fastWalshTransform_kernel.cuh"
-
-////////////////////////////////////////////////////////////////////////////////
-// Data configuration
-////////////////////////////////////////////////////////////////////////////////
-const int log2Kernel = 7;
-const int log2Data = 23;
-
-const int dataN = 1 << log2Data;
-const int kernelN = 1 << log2Kernel;
-
-const int DATA_SIZE = dataN * sizeof(float);
-const int KERNEL_SIZE = kernelN * sizeof(float);
-
-const double NOPS = 3.0 * (double)dataN * (double)log2Data / 2.0;
-
-////////////////////////////////////////////////////////////////////////////////
-// Main program
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char *argv[]) {
-  float *h_Data, *h_Kernel, *h_ResultCPU, *h_ResultGPU;
-
-  float *d_Data, *d_Kernel;
-
-  double delta, ref, sum_delta2, sum_ref2, L2norm, gpuTime;
-
-  StopWatchInterface *hTimer = NULL;
-  int i;
-
-  printf("%s Starting...\n\n", argv[0]);
-
-  // use command-line specified CUDA device, otherwise use device with highest
-  // Gflops/s
-  findCudaDevice(argc, (const char **)argv);
-
-  sdkCreateTimer(&hTimer);
-
-  printf("Initializing data...\n");
-  printf("...allocating CPU memory\n");
-  h_Kernel = (float *)malloc(KERNEL_SIZE);
-  h_Data = (float *)malloc(DATA_SIZE);
-  h_ResultCPU = (float *)malloc(DATA_SIZE);
-  h_ResultGPU = (float *)malloc(DATA_SIZE);
-  printf("...allocating GPU memory\n");
-  HIPCHECK(hipMalloc((void **)&d_Kernel, DATA_SIZE));
-  HIPCHECK(hipMalloc((void **)&d_Data, DATA_SIZE));
-
-  printf("...generating data\n");
-  printf("Data length: %i; kernel length: %i\n", dataN, kernelN);
-  srand(2007);
-
-  for (i = 0; i < kernelN; i++) {
-    h_Kernel[i] = (float)rand() / (float)RAND_MAX;
-  }
-
-  for (i = 0; i < dataN; i++) {
-    h_Data[i] = (float)rand() / (float)RAND_MAX;
-  }
-
-  HIPCHECK(hipMemset(d_Kernel, 0, DATA_SIZE));
-  HIPCHECK(
-      hipMemcpy(d_Kernel, h_Kernel, KERNEL_SIZE, hipMemcpyHostToDevice));
-  HIPCHECK(
-      hipMemcpy(d_Data, h_Data, DATA_SIZE, hipMemcpyHostToDevice));
-
-  printf("Running GPU dyadic convolution using Fast Walsh Transform...\n");
-  HIPCHECK(hipDeviceSynchronize());
-  sdkResetTimer(&hTimer);
-  sdkStartTimer(&hTimer);
-  fwtBatchGPU(d_Data, 1, log2Data);
-  fwtBatchGPU(d_Kernel, 1, log2Data);
-  modulateGPU(d_Data, d_Kernel, dataN);
-  fwtBatchGPU(d_Data, 1, log2Data);
-  HIPCHECK(hipDeviceSynchronize());
-  sdkStopTimer(&hTimer);
-  gpuTime = sdkGetTimerValue(&hTimer);
-  printf("GPU time: %f ms; GOP/s: %f\n", gpuTime,
-         NOPS / (gpuTime * 0.001 * 1E+9));
-
-  printf("Reading back GPU results...\n");
-  HIPCHECK(
-      hipMemcpy(h_ResultGPU, d_Data, DATA_SIZE, hipMemcpyDeviceToHost));
-
-  printf("Running straightforward CPU dyadic convolution...\n");
-  dyadicConvolutionCPU(h_ResultCPU, h_Data, h_Kernel, log2Data, log2Kernel);
-
-  printf("Comparing the results...\n");
-  sum_delta2 = 0;
-  sum_ref2 = 0;
-
-  for (i = 0; i < dataN; i++) {
-    delta = h_ResultCPU[i] - h_ResultGPU[i];
-    ref = h_ResultCPU[i];
-    sum_delta2 += delta * delta;
-    sum_ref2 += ref * ref;
-  }
-
-  L2norm = sqrt(sum_delta2 / sum_ref2);
-
-  printf("Shutting down...\n");
-  sdkDeleteTimer(&hTimer);
-  HIPCHECK(hipFree(d_Data));
-  HIPCHECK(hipFree(d_Kernel));
-  free(h_ResultGPU);
-  free(h_ResultCPU);
-  free(h_Data);
-  free(h_Kernel);
-
-  printf("L2 norm: %E\n", L2norm);
-  printf(L2norm < 1e-6 ? "Test passed\n" : "Test failed!\n");
-}
diff --git a/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip
index bf8c811..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/fluidsD3D9/fluidsD3D9_kernels.cu.hip
@@ -1,335 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <hip/hip_runtime.h>
-#include <builtin_types.h>
-#include <hipfft.h>
-
-#include <helper_cuda.h>
-#include "fluidsD3D9_kernels.h"
-
-// Texture object for reading velocity field
-hipTextureObject_t texObj;
-static hipArray *array = NULL;
-
-void setupTexture(int x, int y) {
-  hipChannelFormatDesc desc = hipCreateChannelDesc<float2>();
-
-  hipMallocArray(&array, &desc, y, x);
-  getLastCudaError("hipMalloc failed");
-
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = array;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&texObj, &texRes, &texDescr, NULL));
-}
-
-void updateTexture(cData *data, size_t wib, size_t h, size_t pitch) {
-  HIPCHECK(hipMemcpy2DToArray(array, 0, 0, data, pitch, wib, h,
-                                      hipMemcpyDeviceToDevice));
-}
-
-void deleteTexture(void) {
-  HIPCHECK(hipDestroyTextureObject(texObj));
-  HIPCHECK(hipFreeArray(array));
-}
-
-// Note that these kernels are designed to work with arbitrary
-// domain sizes, not just domains that are multiples of the tile
-// size. Therefore, we have extra code that checks to make sure
-// a given thread location falls within the domain boundaries in
-// both X and Y. Also, the domain is covered by looping over
-// multiple elements in the Y direction, while there is a one-to-one
-// mapping between threads in X and the tile size in X.
-// Nolan Goodnight 9/22/06
-
-// This method adds constant force vectors to the velocity field
-// stored in 'v' according to v(x,t+1) = v(x,t) + dt * f.
-__global__ void addForces_k(cData *v, int dx, int dy, int spx, int spy,
-                            float fx, float fy, int r, size_t pitch) {
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-  cData *fj = (cData *)((char *)v + (ty + spy) * pitch) + tx + spx;
-
-  cData vterm = *fj;
-  tx -= r;
-  ty -= r;
-  float s = 1.f / (1.f + tx * tx * tx * tx + ty * ty * ty * ty);
-  vterm.x += s * fx;
-  vterm.y += s * fy;
-  *fj = vterm;
-}
-
-// This method performs the velocity advection step, where we
-// trace velocity vectors back in time to update each grid cell.
-// That is, v(x,t+1) = v(p(x,-dt),t). Here we perform bilinear
-// interpolation in the velocity space.
-__global__ void advectVelocity_k(cData *v, float *vx, float *vy, int dx,
-                                 int pdx, int dy, float dt, int lb,
-                                 hipTextureObject_t texObject) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  cData vterm, ploc;
-  float vxterm, vyterm;
-
-  // gtidx is the domain location in x for this thread
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fj = fi * pdx + gtidx;
-        vterm = tex2D<cData>(texObject, (float)gtidx, (float)fi);
-        ploc.x = (gtidx + 0.5f) - (dt * vterm.x * dx);
-        ploc.y = (fi + 0.5f) - (dt * vterm.y * dy);
-        vterm = tex2D<cData>(texObject, ploc.x, ploc.y);
-        vxterm = vterm.x;
-        vyterm = vterm.y;
-        vx[fj] = vxterm;
-        vy[fj] = vyterm;
-      }
-    }
-  }
-}
-
-// This method performs velocity diffusion and forces mass conservation
-// in the frequency domain. The inputs 'vx' and 'vy' are complex-valued
-// arrays holding the Fourier coefficients of the velocity field in
-// X and Y. Diffusion in this space takes a simple form described as:
-// v(k,t) = v(k,t) / (1 + visc * dt * k^2), where visc is the viscosity,
-// and k is the wavenumber. The projection step forces the Fourier
-// velocity vectors to be orthogonal to the vectors for each
-// wavenumber: v(k,t) = v(k,t) - ((k dot v(k,t) * k) / k^2.
-__global__ void diffuseProject_k(cData *vx, cData *vy, int dx, int dy, float dt,
-                                 float visc, int lb) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  cData xterm, yterm;
-
-  // gtidx is the domain location in x for this thread
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fj = fi * dx + gtidx;
-        xterm = vx[fj];
-        yterm = vy[fj];
-
-        // Compute the index of the wavenumber based on the
-        // data order produced by a standard NN FFT.
-        int iix = gtidx;
-        int iiy = (fi > dy / 2) ? (fi - (dy)) : fi;
-
-        // Velocity diffusion
-        float kk = (float)(iix * iix + iiy * iiy);  // k^2
-        float diff = 1.f / (1.f + visc * dt * kk);
-        xterm.x *= diff;
-        xterm.y *= diff;
-        yterm.x *= diff;
-        yterm.y *= diff;
-
-        // Velocity projection
-        if (kk > 0.f) {
-          float rkk = 1.f / kk;
-          // Real portion of velocity projection
-          float rkp = (iix * xterm.x + iiy * yterm.x);
-          // Imaginary portion of velocity projection
-          float ikp = (iix * xterm.y + iiy * yterm.y);
-          xterm.x -= rkk * rkp * iix;
-          xterm.y -= rkk * ikp * iix;
-          yterm.x -= rkk * rkp * iiy;
-          yterm.y -= rkk * ikp * iiy;
-        }
-
-        vx[fj] = xterm;
-        vy[fj] = yterm;
-      }
-    }
-  }
-}
-
-// This method updates the velocity field 'v' using the two complex
-// arrays from the previous step: 'vx' and 'vy'. Here we scale the
-// real components by 1/(dx*dy) to account for an unnormalized FFT.
-__global__ void updateVelocity_k(cData *v, float *vx, float *vy, int dx,
-                                 int pdx, int dy, int lb, size_t pitch) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  float vxterm, vyterm;
-  cData nvterm;
-
-  // gtidx is the domain location in x for this thread
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fjr = fi * pdx + gtidx;
-        vxterm = vx[fjr];
-        vyterm = vy[fjr];
-
-        // Normalize the result of the inverse FFT
-        float scale = 1.f / (dx * dy);
-        nvterm.x = vxterm * scale;
-        nvterm.y = vyterm * scale;
-
-        cData *fj = (cData *)((char *)v + fi * pitch) + gtidx;
-        *fj = nvterm;
-      }
-    }  // If this thread is inside the domain in Y
-  }    // If this thread is inside the domain in X
-}
-
-// This method updates the particles by moving particle positions
-// according to the velocity field and time step. That is, for each
-// particle: p(t+1) = p(t) + dt * v(p(t)).
-__global__ void advectParticles_k(Vertex *part, cData *v, int dx, int dy,
-                                  float dt, int lb, size_t pitch) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  // gtidx is the domain location in x for this thread
-  cData vterm;
-  Vertex pterm;
-
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fj = fi * dx + gtidx;
-        pterm = part[fj];
-
-        int xvi = ((int)(pterm.x * dx));
-        int yvi = ((int)(pterm.y * dy));
-        vterm = *((cData *)((char *)v + yvi * pitch) + xvi);
-
-        pterm.x += dt * vterm.x;
-        pterm.x = pterm.x - (int)pterm.x;
-        pterm.x += 1.f;
-        pterm.x = pterm.x - (int)pterm.x;
-        pterm.y += dt * vterm.y;
-        pterm.y = pterm.y - (int)pterm.y;
-        pterm.y += 1.f;
-        pterm.y = pterm.y - (int)pterm.y;
-
-        part[fj] = pterm;
-      }
-    }  // If this thread is inside the domain in Y
-  }    // If this thread is inside the domain in X
-}
-
-extern "C" void addForces(cData *v, int dx, int dy, int spx, int spy, float fx,
-                          float fy, int r, size_t tPitch) {
-  dim3 tids(2 * r + 1, 2 * r + 1);
-
-  addForces_k<<<1, tids>>>(v, dx, dy, spx, spy, fx, fy, r, tPitch);
-  getLastCudaError("addForces_k failed.");
-}
-
-extern "C" void advectVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
-                               int dy, float dt, size_t tPitch) {
-  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
-
-  dim3 tids(TIDSX, TIDSY);
-
-  updateTexture(v, DIM * sizeof(cData), DIM, tPitch);
-  advectVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, dt, TILEY / TIDSY,
-                                   texObj);
-
-  getLastCudaError("advectVelocity_k failed.");
-}
-
-extern "C" void diffuseProject(cData *vx, cData *vy, int dx, int dy, float dt,
-                               float visc, size_t tPitch) {
-  // Forward FFT
-  //    cufftExecR2C(planr2c, (cufftReal*)vx, (cufftComplex*)vx);
-  //    cufftExecR2C(planr2c, (cufftReal*)vy, (cufftComplex*)vy);
-
-  uint3 grid = make_uint3((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-                          (dy / TILEY) + (!(dy % TILEY) ? 0 : 1), 1);
-
-  uint3 tids = make_uint3(TIDSX, TIDSY, 1);
-
-  diffuseProject_k<<<grid, tids>>>(vx, vy, dx, dy, dt, visc, TILEY / TIDSY);
-  getLastCudaError("diffuseProject_k failed.");
-
-  // Inverse FFT
-  //    cufftExecC2R(planc2r, (cufftComplex*)vx, (cufftReal*)vx);
-  //    cufftExecC2R(planc2r, (cufftComplex*)vy, (cufftReal*)vy);
-}
-
-extern "C" void updateVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
-                               int dy, size_t tPitch) {
-  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
-
-  dim3 tids(TIDSX, TIDSY);
-
-  updateVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, TILEY / TIDSY,
-                                   tPitch);
-  getLastCudaError("updateVelocity_k failed.");
-}
-
-extern "C" void advectParticles(Vertex *p, cData *v, int dx, int dy, float dt,
-                                size_t tPitch) {
-  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
-
-  dim3 tids(TIDSX, TIDSY);
-
-  advectParticles_k<<<grid, tids>>>(p, v, dx, dy, dt, TILEY / TIDSY, tPitch);
-  getLastCudaError("advectParticles_k failed.");
-}
diff --git a/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip b/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip
index 6a88710..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/fluidsGL/fluidsGL_kernels.cu.hip
@@ -1,362 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-
-#include <hip/hip_runtime.h>
-#include <hipfft.h>        // CUDA FFT Libraries
-#include <helper_cuda.h>  // Helper functions for CUDA Error handling
-
-// OpenGL Graphics includes
-#define HELPERGL_EXTERN_GL_FUNC_IMPLEMENTATION
-#include <helper_gl.h>
-
-// FluidsGL CUDA kernel definitions
-#include "fluidsGL_kernels.cuh"
-
-// Texture object for reading velocity field
-hipTextureObject_t texObj;
-static hipArray *array = NULL;
-
-// Particle data
-extern GLuint vbo;  // OpenGL vertex buffer object
-extern struct hipGraphicsResource
-    *cuda_vbo_resource;  // handles OpenGL-CUDA exchange
-
-// Texture pitch
-extern size_t tPitch;
-extern hipfftHandle planr2c;
-extern hipfftHandle planc2r;
-cData *vxfield = NULL;
-cData *vyfield = NULL;
-
-void setupTexture(int x, int y) {
-  hipChannelFormatDesc desc = hipCreateChannelDesc<float2>();
-
-  hipMallocArray(&array, &desc, y, x);
-  getLastCudaError("hipMalloc failed");
-
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = array;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModeLinear;
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(hipCreateTextureObject(&texObj, &texRes, &texDescr, NULL));
-}
-
-void updateTexture(cData *data, size_t wib, size_t h, size_t pitch) {
-  HIPCHECK(hipMemcpy2DToArray(array, 0, 0, data, pitch, wib, h,
-                                      hipMemcpyDeviceToDevice));
-}
-
-void deleteTexture(void) {
-  HIPCHECK(hipDestroyTextureObject(texObj));
-  HIPCHECK(hipFreeArray(array));
-}
-
-// Note that these kernels are designed to work with arbitrary
-// domain sizes, not just domains that are multiples of the tile
-// size. Therefore, we have extra code that checks to make sure
-// a given thread location falls within the domain boundaries in
-// both X and Y. Also, the domain is covered by looping over
-// multiple elements in the Y direction, while there is a one-to-one
-// mapping between threads in X and the tile size in X.
-// Nolan Goodnight 9/22/06
-
-// This method adds constant force vectors to the velocity field
-// stored in 'v' according to v(x,t+1) = v(x,t) + dt * f.
-__global__ void addForces_k(cData *v, int dx, int dy, int spx, int spy,
-                            float fx, float fy, int r, size_t pitch) {
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-  cData *fj = (cData *)((char *)v + (ty + spy) * pitch) + tx + spx;
-
-  cData vterm = *fj;
-  tx -= r;
-  ty -= r;
-  float s = 1.f / (1.f + tx * tx * tx * tx + ty * ty * ty * ty);
-  vterm.x += s * fx;
-  vterm.y += s * fy;
-  *fj = vterm;
-}
-
-// This method performs the velocity advection step, where we
-// trace velocity vectors back in time to update each grid cell.
-// That is, v(x,t+1) = v(p(x,-dt),t). Here we perform bilinear
-// interpolation in the velocity space.
-__global__ void advectVelocity_k(cData *v, float *vx, float *vy, int dx,
-                                 int pdx, int dy, float dt, int lb,
-                                 hipTextureObject_t texObject) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  cData vterm, ploc;
-  float vxterm, vyterm;
-
-  // gtidx is the domain location in x for this thread
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fj = fi * pdx + gtidx;
-        vterm = tex2D<cData>(texObject, (float)gtidx, (float)fi);
-        ploc.x = (gtidx + 0.5f) - (dt * vterm.x * dx);
-        ploc.y = (fi + 0.5f) - (dt * vterm.y * dy);
-        vterm = tex2D<cData>(texObject, ploc.x, ploc.y);
-        vxterm = vterm.x;
-        vyterm = vterm.y;
-        vx[fj] = vxterm;
-        vy[fj] = vyterm;
-      }
-    }
-  }
-}
-
-// This method performs velocity diffusion and forces mass conservation
-// in the frequency domain. The inputs 'vx' and 'vy' are complex-valued
-// arrays holding the Fourier coefficients of the velocity field in
-// X and Y. Diffusion in this space takes a simple form described as:
-// v(k,t) = v(k,t) / (1 + visc * dt * k^2), where visc is the viscosity,
-// and k is the wavenumber. The projection step forces the Fourier
-// velocity vectors to be orthogonal to the vectors for each
-// wavenumber: v(k,t) = v(k,t) - ((k dot v(k,t) * k) / k^2.
-__global__ void diffuseProject_k(cData *vx, cData *vy, int dx, int dy, float dt,
-                                 float visc, int lb) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  cData xterm, yterm;
-
-  // gtidx is the domain location in x for this thread
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fj = fi * dx + gtidx;
-        xterm = vx[fj];
-        yterm = vy[fj];
-
-        // Compute the index of the wavenumber based on the
-        // data order produced by a standard NN FFT.
-        int iix = gtidx;
-        int iiy = (fi > dy / 2) ? (fi - (dy)) : fi;
-
-        // Velocity diffusion
-        float kk = (float)(iix * iix + iiy * iiy);  // k^2
-        float diff = 1.f / (1.f + visc * dt * kk);
-        xterm.x *= diff;
-        xterm.y *= diff;
-        yterm.x *= diff;
-        yterm.y *= diff;
-
-        // Velocity projection
-        if (kk > 0.f) {
-          float rkk = 1.f / kk;
-          // Real portion of velocity projection
-          float rkp = (iix * xterm.x + iiy * yterm.x);
-          // Imaginary portion of velocity projection
-          float ikp = (iix * xterm.y + iiy * yterm.y);
-          xterm.x -= rkk * rkp * iix;
-          xterm.y -= rkk * ikp * iix;
-          yterm.x -= rkk * rkp * iiy;
-          yterm.y -= rkk * ikp * iiy;
-        }
-
-        vx[fj] = xterm;
-        vy[fj] = yterm;
-      }
-    }
-  }
-}
-
-// This method updates the velocity field 'v' using the two complex
-// arrays from the previous step: 'vx' and 'vy'. Here we scale the
-// real components by 1/(dx*dy) to account for an unnormalized FFT.
-__global__ void updateVelocity_k(cData *v, float *vx, float *vy, int dx,
-                                 int pdx, int dy, int lb, size_t pitch) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  float vxterm, vyterm;
-  cData nvterm;
-
-  // gtidx is the domain location in x for this thread
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fjr = fi * pdx + gtidx;
-        vxterm = vx[fjr];
-        vyterm = vy[fjr];
-
-        // Normalize the result of the inverse FFT
-        float scale = 1.f / (dx * dy);
-        nvterm.x = vxterm * scale;
-        nvterm.y = vyterm * scale;
-
-        cData *fj = (cData *)((char *)v + fi * pitch) + gtidx;
-        *fj = nvterm;
-      }
-    }  // If this thread is inside the domain in Y
-  }    // If this thread is inside the domain in X
-}
-
-// This method updates the particles by moving particle positions
-// according to the velocity field and time step. That is, for each
-// particle: p(t+1) = p(t) + dt * v(p(t)).
-__global__ void advectParticles_k(cData *part, cData *v, int dx, int dy,
-                                  float dt, int lb, size_t pitch) {
-  int gtidx = blockIdx.x * blockDim.x + threadIdx.x;
-  int gtidy = blockIdx.y * (lb * blockDim.y) + threadIdx.y * lb;
-  int p;
-
-  // gtidx is the domain location in x for this thread
-  cData pterm, vterm;
-
-  if (gtidx < dx) {
-    for (p = 0; p < lb; p++) {
-      // fi is the domain location in y for this thread
-      int fi = gtidy + p;
-
-      if (fi < dy) {
-        int fj = fi * dx + gtidx;
-        pterm = part[fj];
-
-        int xvi = ((int)(pterm.x * dx));
-        int yvi = ((int)(pterm.y * dy));
-        vterm = *((cData *)((char *)v + yvi * pitch) + xvi);
-
-        pterm.x += dt * vterm.x;
-        pterm.x = pterm.x - (int)pterm.x;
-        pterm.x += 1.f;
-        pterm.x = pterm.x - (int)pterm.x;
-        pterm.y += dt * vterm.y;
-        pterm.y = pterm.y - (int)pterm.y;
-        pterm.y += 1.f;
-        pterm.y = pterm.y - (int)pterm.y;
-
-        part[fj] = pterm;
-      }
-    }  // If this thread is inside the domain in Y
-  }    // If this thread is inside the domain in X
-}
-
-// These are the external function calls necessary for launching fluid
-// simulation
-extern "C" void addForces(cData *v, int dx, int dy, int spx, int spy, float fx,
-                          float fy, int r) {
-  dim3 tids(2 * r + 1, 2 * r + 1);
-
-  addForces_k<<<1, tids>>>(v, dx, dy, spx, spy, fx, fy, r, tPitch);
-  getLastCudaError("addForces_k failed.");
-}
-
-extern "C" void advectVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
-                               int dy, float dt) {
-  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
-
-  dim3 tids(TIDSX, TIDSY);
-
-  updateTexture(v, DIM * sizeof(cData), DIM, tPitch);
-  advectVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, dt, TILEY / TIDSY,
-                                   texObj);
-
-  getLastCudaError("advectVelocity_k failed.");
-}
-
-extern "C" void diffuseProject(cData *vx, cData *vy, int dx, int dy, float dt,
-                               float visc) {
-  // Forward FFT
-  HIPCHECK(hipfftExecR2C(planr2c, (hipfftReal *)vx, (hipfftComplex *)vx));
-  HIPCHECK(hipfftExecR2C(planr2c, (hipfftReal *)vy, (hipfftComplex *)vy));
-
-  uint3 grid = make_uint3((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-                          (dy / TILEY) + (!(dy % TILEY) ? 0 : 1), 1);
-  uint3 tids = make_uint3(TIDSX, TIDSY, 1);
-
-  diffuseProject_k<<<grid, tids>>>(vx, vy, dx, dy, dt, visc, TILEY / TIDSY);
-  getLastCudaError("diffuseProject_k failed.");
-
-  // Inverse FFT
-  HIPCHECK(hipfftExecC2R(planc2r, (hipfftComplex *)vx, (hipfftReal *)vx));
-  HIPCHECK(hipfftExecC2R(planc2r, (hipfftComplex *)vy, (hipfftReal *)vy));
-}
-
-extern "C" void updateVelocity(cData *v, float *vx, float *vy, int dx, int pdx,
-                               int dy) {
-  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
-  dim3 tids(TIDSX, TIDSY);
-
-  updateVelocity_k<<<grid, tids>>>(v, vx, vy, dx, pdx, dy, TILEY / TIDSY,
-                                   tPitch);
-  getLastCudaError("updateVelocity_k failed.");
-}
-
-extern "C" void advectParticles(GLuint vbo, cData *v, int dx, int dy,
-                                float dt) {
-  dim3 grid((dx / TILEX) + (!(dx % TILEX) ? 0 : 1),
-            (dy / TILEY) + (!(dy % TILEY) ? 0 : 1));
-  dim3 tids(TIDSX, TIDSY);
-
-  cData *p;
-  hipGraphicsMapResources(1, &cuda_vbo_resource, 0);
-  getLastCudaError("hipGraphicsMapResources failed");
-
-  size_t num_bytes;
-  hipGraphicsResourceGetMappedPointer((void **)&p, &num_bytes,
-                                       cuda_vbo_resource);
-  getLastCudaError("hipGraphicsResourceGetMappedPointer failed");
-
-  advectParticles_k<<<grid, tids>>>(p, v, dx, dy, dt, TILEY / TIDSY, tPitch);
-  getLastCudaError("advectParticles_k failed.");
-
-  hipGraphicsUnmapResources(1, &cuda_vbo_resource, 0);
-  getLastCudaError("hipGraphicsUnmapResources failed");
-}
diff --git a/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu.hip b/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu.hip
index d8bb742..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest.cu.hip
@@ -1,697 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <cstdio>
-#include <vector>
-
-#include <helper_cuda.h>
-#include <helper_timer.h>
-
-using namespace std;
-
-const char *sSampleName = "P2P (Peer-to-Peer) GPU Bandwidth Latency Test";
-
-typedef enum {
-  P2P_WRITE = 0,
-  P2P_READ = 1,
-} P2PDataTransfer;
-
-typedef enum {
-  CE = 0,
-  SM = 1,
-} P2PEngine;
-
-P2PEngine p2p_mechanism = CE;  // By default use Copy Engine
-
-// Macro for checking cuda errors following a cuda launch or api call
-#define cudaCheckError()                                       \
-  {                                                            \
-    hipError_t e = hipGetLastError();                        \
-    if (e != hipSuccess) {                                    \
-      printf("Cuda failure %s:%d: '%s'\n", __FILE__, __LINE__, \
-             hipGetErrorString(e));                           \
-      exit(EXIT_FAILURE);                                      \
-    }                                                          \
-  }
-__global__ void delay(volatile int *flag,
-                      unsigned long long timeout_clocks = 10000000) {
-  // Wait until the application notifies us that it has completed queuing up the
-  // experiment, or timeout and exit, allowing the application to make progress
-  long long int start_clock, sample_clock;
-  start_clock = clock64();
-
-  while (!*flag) {
-    sample_clock = clock64();
-
-    if (sample_clock - start_clock > timeout_clocks) {
-      break;
-    }
-  }
-}
-
-// This kernel is for demonstration purposes only, not a performant kernel for
-// p2p transfers.
-__global__ void copyp2p(int4 *__restrict__ dest, int4 const *__restrict__ src,
-                        size_t num_elems) {
-  size_t globalId = blockIdx.x * blockDim.x + threadIdx.x;
-  size_t gridSize = blockDim.x * gridDim.x;
-
-#pragma unroll(5)
-  for (size_t i = globalId; i < num_elems; i += gridSize) {
-    dest[i] = src[i];
-  }
-}
-
-///////////////////////////////////////////////////////////////////////////
-// Print help screen
-///////////////////////////////////////////////////////////////////////////
-void printHelp(void) {
-  printf("Usage:  p2pBandwidthLatencyTest [OPTION]...\n");
-  printf("Tests bandwidth/latency of GPU pairs using P2P and without P2P\n");
-  printf("\n");
-
-  printf("Options:\n");
-  printf("--help\t\tDisplay this help menu\n");
-  printf(
-      "--p2p_read\tUse P2P reads for data transfers between GPU pairs and show "
-      "corresponding results.\n \t\tDefault used is P2P write operation.\n");
-  printf("--sm_copy                      Use SM intiated p2p transfers instead of Copy Engine\n");
-  printf("--numElems=<NUM_OF_INT_ELEMS>  Number of integer elements to be used in p2p copy.\n");
-}
-
-void checkP2Paccess(int numGPUs) {
-  for (int i = 0; i < numGPUs; i++) {
-    hipSetDevice(i);
-    cudaCheckError();
-
-    for (int j = 0; j < numGPUs; j++) {
-      int access;
-      if (i != j) {
-        hipDeviceCanAccessPeer(&access, i, j);
-        cudaCheckError();
-        printf("Device=%d %s Access Peer Device=%d\n", i,
-               access ? "CAN" : "CANNOT", j);
-      }
-    }
-  }
-  printf(
-      "\n***NOTE: In case a device doesn't have P2P access to other one, it "
-      "falls back to normal memcopy procedure.\nSo you can see lesser "
-      "Bandwidth (GB/s) and unstable Latency (us) in those cases.\n\n");
-}
-
-void performP2PCopy(int *dest, int destDevice, int *src, int srcDevice,
-                    int num_elems, int repeat, bool p2paccess,
-                    hipStream_t streamToRun) {
-  int blockSize = 0;
-  int numBlocks = 0;
-
-  hipOccupancyMaxPotentialBlockSize(&numBlocks, &blockSize, copyp2p);
-  cudaCheckError();
-
-  if (p2p_mechanism == SM && p2paccess) {
-    for (int r = 0; r < repeat; r++) {
-      copyp2p<<<numBlocks, blockSize, 0, streamToRun>>>(
-          (int4 *)dest, (int4 *)src, num_elems / 4);
-    }
-  } else {
-    for (int r = 0; r < repeat; r++) {
-      hipMemcpyPeerAsync(dest, destDevice, src, srcDevice,
-                          sizeof(int) * num_elems, streamToRun);
-    }
-  }
-}
-
-void outputBandwidthMatrix(int numElems, int numGPUs, bool p2p, P2PDataTransfer p2p_method) {
-  int repeat = 5;
-  volatile int *flag = NULL;
-  vector<int *> buffers(numGPUs);
-  vector<int *> buffersD2D(numGPUs);  // buffer for D2D, that is, intra-GPU copy
-  vector<hipEvent_t> start(numGPUs);
-  vector<hipEvent_t> stop(numGPUs);
-  vector<hipStream_t> stream(numGPUs);
-
-  hipHostAlloc((void **)&flag, sizeof(*flag), hipHostMallocPortable);
-  cudaCheckError();
-
-  for (int d = 0; d < numGPUs; d++) {
-    hipSetDevice(d);
-    hipStreamCreateWithFlags(&stream[d], hipStreamNonBlocking);
-    hipMalloc(&buffers[d], numElems * sizeof(int));
-    cudaCheckError();
-    hipMemset(buffers[d], 0, numElems * sizeof(int));
-    cudaCheckError();
-    hipMalloc(&buffersD2D[d], numElems * sizeof(int));
-    cudaCheckError();
-    hipMemset(buffersD2D[d], 0, numElems * sizeof(int));
-    cudaCheckError();
-    hipEventCreate(&start[d]);
-    cudaCheckError();
-    hipEventCreate(&stop[d]);
-    cudaCheckError();
-  }
-
-  vector<double> bandwidthMatrix(numGPUs * numGPUs);
-
-  for (int i = 0; i < numGPUs; i++) {
-    hipSetDevice(i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      int access = 0;
-      if (p2p) {
-        hipDeviceCanAccessPeer(&access, i, j);
-        if (access) {
-          hipDeviceEnablePeerAccess(j, 0);
-          cudaCheckError();
-          hipSetDevice(j);
-          cudaCheckError();
-          hipDeviceEnablePeerAccess(i, 0);
-          cudaCheckError();
-          hipSetDevice(i);
-          cudaCheckError();
-        }
-      }
-
-      hipStreamSynchronize(stream[i]);
-      cudaCheckError();
-
-      // Block the stream until all the work is queued up
-      // DANGER! - cudaMemcpy*Async may infinitely block waiting for
-      // room to push the operation, so keep the number of repeatitions
-      // relatively low.  Higher repeatitions will cause the delay kernel
-      // to timeout and lead to unstable results.
-      *flag = 0;
-      delay<<<1, 1, 0, stream[i]>>>(flag);
-      cudaCheckError();
-      hipEventRecord(start[i], stream[i]);
-      cudaCheckError();
-
-      if (i == j) {
-        // Perform intra-GPU, D2D copies
-        performP2PCopy(buffers[i], i, buffersD2D[i], i, numElems, repeat,
-                       access, stream[i]);
-
-      } else {
-        if (p2p_method == P2P_WRITE) {
-          performP2PCopy(buffers[j], j, buffers[i], i, numElems, repeat, access,
-                         stream[i]);
-        } else {
-          performP2PCopy(buffers[i], i, buffers[j], j, numElems, repeat, access,
-                         stream[i]);
-        }
-      }
-
-      hipEventRecord(stop[i], stream[i]);
-      cudaCheckError();
-
-      // Release the queued events
-      *flag = 1;
-      hipStreamSynchronize(stream[i]);
-      cudaCheckError();
-
-      float time_ms;
-      hipEventElapsedTime(&time_ms, start[i], stop[i]);
-      double time_s = time_ms / 1e3;
-
-      double gb = numElems * sizeof(int) * repeat / (double)1e9;
-      if (i == j) {
-        gb *= 2;  // must count both the read and the write here
-      }
-      bandwidthMatrix[i * numGPUs + j] = gb / time_s;
-      if (p2p && access) {
-        hipDeviceDisablePeerAccess(j);
-        hipSetDevice(j);
-        hipDeviceDisablePeerAccess(i);
-        hipSetDevice(i);
-        cudaCheckError();
-      }
-    }
-  }
-
-  printf("   D\\D");
-
-  for (int j = 0; j < numGPUs; j++) {
-    printf("%6d ", j);
-  }
-
-  printf("\n");
-
-  for (int i = 0; i < numGPUs; i++) {
-    printf("%6d ", i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      printf("%6.02f ", bandwidthMatrix[i * numGPUs + j]);
-    }
-
-    printf("\n");
-  }
-
-  for (int d = 0; d < numGPUs; d++) {
-    hipSetDevice(d);
-    hipFree(buffers[d]);
-    hipFree(buffersD2D[d]);
-    cudaCheckError();
-    hipEventDestroy(start[d]);
-    cudaCheckError();
-    hipEventDestroy(stop[d]);
-    cudaCheckError();
-    hipStreamDestroy(stream[d]);
-    cudaCheckError();
-  }
-
-  hipHostFree((void *)flag);
-  cudaCheckError();
-}
-
-void outputBidirectionalBandwidthMatrix(int numElems, int numGPUs, bool p2p) {
-  int repeat = 5;
-  volatile int *flag = NULL;
-  vector<int *> buffers(numGPUs);
-  vector<int *> buffersD2D(numGPUs);
-  vector<hipEvent_t> start(numGPUs);
-  vector<hipEvent_t> stop(numGPUs);
-  vector<hipStream_t> stream0(numGPUs);
-  vector<hipStream_t> stream1(numGPUs);
-
-  hipHostAlloc((void **)&flag, sizeof(*flag), hipHostMallocPortable);
-  cudaCheckError();
-
-  for (int d = 0; d < numGPUs; d++) {
-    hipSetDevice(d);
-    hipMalloc(&buffers[d], numElems * sizeof(int));
-    hipMemset(buffers[d], 0, numElems * sizeof(int));
-    hipMalloc(&buffersD2D[d], numElems * sizeof(int));
-    hipMemset(buffersD2D[d], 0, numElems * sizeof(int));
-    cudaCheckError();
-    hipEventCreate(&start[d]);
-    cudaCheckError();
-    hipEventCreate(&stop[d]);
-    cudaCheckError();
-    hipStreamCreateWithFlags(&stream0[d], hipStreamNonBlocking);
-    cudaCheckError();
-    hipStreamCreateWithFlags(&stream1[d], hipStreamNonBlocking);
-    cudaCheckError();
-  }
-
-  vector<double> bandwidthMatrix(numGPUs * numGPUs);
-
-  for (int i = 0; i < numGPUs; i++) {
-    hipSetDevice(i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      int access = 0;
-      if (p2p) {
-        hipDeviceCanAccessPeer(&access, i, j);
-        if (access) {
-          hipSetDevice(i);
-          hipDeviceEnablePeerAccess(j, 0);
-          cudaCheckError();
-          hipSetDevice(j);
-          hipDeviceEnablePeerAccess(i, 0);
-          cudaCheckError();
-        }
-      }
-
-      hipSetDevice(i);
-      hipStreamSynchronize(stream0[i]);
-      hipStreamSynchronize(stream1[j]);
-      cudaCheckError();
-
-      // Block the stream until all the work is queued up
-      // DANGER! - cudaMemcpy*Async may infinitely block waiting for
-      // room to push the operation, so keep the number of repeatitions
-      // relatively low.  Higher repeatitions will cause the delay kernel
-      // to timeout and lead to unstable results.
-      *flag = 0;
-      hipSetDevice(i);
-      // No need to block stream1 since it'll be blocked on stream0's event
-      delay<<<1, 1, 0, stream0[i]>>>(flag);
-      cudaCheckError();
-
-      // Force stream1 not to start until stream0 does, in order to ensure
-      // the events on stream0 fully encompass the time needed for all
-      // operations
-      hipEventRecord(start[i], stream0[i]);
-      hipStreamWaitEvent(stream1[j], start[i], 0);
-
-      if (i == j) {
-        // For intra-GPU perform 2 memcopies buffersD2D <-> buffers
-        performP2PCopy(buffers[i], i, buffersD2D[i], i, numElems, repeat,
-                       access, stream0[i]);
-        performP2PCopy(buffersD2D[i], i, buffers[i], i, numElems, repeat,
-                       access, stream1[i]);
-      } else {
-        if (access && p2p_mechanism == SM) {
-          hipSetDevice(j);
-        }
-        performP2PCopy(buffers[i], i, buffers[j], j, numElems, repeat, access,
-                       stream1[j]);
-        if (access && p2p_mechanism == SM) {
-          hipSetDevice(i);
-        }
-        performP2PCopy(buffers[j], j, buffers[i], i, numElems, repeat, access,
-                       stream0[i]);
-      }
-
-      // Notify stream0 that stream1 is complete and record the time of
-      // the total transaction
-      hipEventRecord(stop[j], stream1[j]);
-      hipStreamWaitEvent(stream0[i], stop[j], 0);
-      hipEventRecord(stop[i], stream0[i]);
-
-      // Release the queued operations
-      *flag = 1;
-      hipStreamSynchronize(stream0[i]);
-      hipStreamSynchronize(stream1[j]);
-      cudaCheckError();
-
-      float time_ms;
-      hipEventElapsedTime(&time_ms, start[i], stop[i]);
-      double time_s = time_ms / 1e3;
-
-      double gb = 2.0 * numElems * sizeof(int) * repeat / (double)1e9;
-      if (i == j) {
-        gb *= 2;  // must count both the read and the write here
-      }
-      bandwidthMatrix[i * numGPUs + j] = gb / time_s;
-      if (p2p && access) {
-        hipSetDevice(i);
-        hipDeviceDisablePeerAccess(j);
-        hipSetDevice(j);
-        hipDeviceDisablePeerAccess(i);
-      }
-    }
-  }
-
-  printf("   D\\D");
-
-  for (int j = 0; j < numGPUs; j++) {
-    printf("%6d ", j);
-  }
-
-  printf("\n");
-
-  for (int i = 0; i < numGPUs; i++) {
-    printf("%6d ", i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      printf("%6.02f ", bandwidthMatrix[i * numGPUs + j]);
-    }
-
-    printf("\n");
-  }
-
-  for (int d = 0; d < numGPUs; d++) {
-    hipSetDevice(d);
-    hipFree(buffers[d]);
-    hipFree(buffersD2D[d]);
-    cudaCheckError();
-    hipEventDestroy(start[d]);
-    cudaCheckError();
-    hipEventDestroy(stop[d]);
-    cudaCheckError();
-    hipStreamDestroy(stream0[d]);
-    cudaCheckError();
-    hipStreamDestroy(stream1[d]);
-    cudaCheckError();
-  }
-
-  hipHostFree((void *)flag);
-  cudaCheckError();
-}
-
-void outputLatencyMatrix(int numGPUs, bool p2p, P2PDataTransfer p2p_method) {
-  int repeat = 100;
-  int numElems = 4;  // perform 1-int4 transfer.
-  volatile int *flag = NULL;
-  StopWatchInterface *stopWatch = NULL;
-  vector<int *> buffers(numGPUs);
-  vector<int *> buffersD2D(numGPUs);  // buffer for D2D, that is, intra-GPU copy
-  vector<hipStream_t> stream(numGPUs);
-  vector<hipEvent_t> start(numGPUs);
-  vector<hipEvent_t> stop(numGPUs);
-
-  hipHostAlloc((void **)&flag, sizeof(*flag), hipHostMallocPortable);
-  cudaCheckError();
-
-  if (!sdkCreateTimer(&stopWatch)) {
-    printf("Failed to create stop watch\n");
-    exit(EXIT_FAILURE);
-  }
-  sdkStartTimer(&stopWatch);
-
-  for (int d = 0; d < numGPUs; d++) {
-    hipSetDevice(d);
-    hipStreamCreateWithFlags(&stream[d], hipStreamNonBlocking);
-    hipMalloc(&buffers[d], sizeof(int) * numElems);
-    hipMemset(buffers[d], 0, sizeof(int) * numElems);
-    hipMalloc(&buffersD2D[d], sizeof(int) * numElems);
-    hipMemset(buffersD2D[d], 0, sizeof(int) * numElems);
-    cudaCheckError();
-    hipEventCreate(&start[d]);
-    cudaCheckError();
-    hipEventCreate(&stop[d]);
-    cudaCheckError();
-  }
-
-  vector<double> gpuLatencyMatrix(numGPUs * numGPUs);
-  vector<double> cpuLatencyMatrix(numGPUs * numGPUs);
-
-  for (int i = 0; i < numGPUs; i++) {
-    hipSetDevice(i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      int access = 0;
-      if (p2p) {
-        hipDeviceCanAccessPeer(&access, i, j);
-        if (access) {
-          hipDeviceEnablePeerAccess(j, 0);
-          cudaCheckError();
-          hipSetDevice(j);
-          hipDeviceEnablePeerAccess(i, 0);
-          hipSetDevice(i);
-          cudaCheckError();
-        }
-      }
-      hipStreamSynchronize(stream[i]);
-      cudaCheckError();
-
-      // Block the stream until all the work is queued up
-      // DANGER! - cudaMemcpy*Async may infinitely block waiting for
-      // room to push the operation, so keep the number of repeatitions
-      // relatively low.  Higher repeatitions will cause the delay kernel
-      // to timeout and lead to unstable results.
-      *flag = 0;
-      delay<<<1, 1, 0, stream[i]>>>(flag);
-      cudaCheckError();
-      hipEventRecord(start[i], stream[i]);
-
-      sdkResetTimer(&stopWatch);
-      if (i == j) {
-        // Perform intra-GPU, D2D copies
-        performP2PCopy(buffers[i], i, buffersD2D[i], i, numElems, repeat,
-                       access, stream[i]);
-      } else {
-        if (p2p_method == P2P_WRITE) {
-          performP2PCopy(buffers[j], j, buffers[i], i, numElems, repeat, access,
-                         stream[i]);
-        } else {
-          performP2PCopy(buffers[i], i, buffers[j], j, numElems, repeat, access,
-                         stream[i]);
-        }
-      }
-      float cpu_time_ms = sdkGetTimerValue(&stopWatch);
-
-      hipEventRecord(stop[i], stream[i]);
-      // Now that the work has been queued up, release the stream
-      *flag = 1;
-      hipStreamSynchronize(stream[i]);
-      cudaCheckError();
-
-      float gpu_time_ms;
-      hipEventElapsedTime(&gpu_time_ms, start[i], stop[i]);
-
-      gpuLatencyMatrix[i * numGPUs + j] = gpu_time_ms * 1e3 / repeat;
-      cpuLatencyMatrix[i * numGPUs + j] = cpu_time_ms * 1e3 / repeat;
-      if (p2p && access) {
-        hipDeviceDisablePeerAccess(j);
-        hipSetDevice(j);
-        hipDeviceDisablePeerAccess(i);
-        hipSetDevice(i);
-        cudaCheckError();
-      }
-    }
-  }
-
-  printf("   GPU");
-
-  for (int j = 0; j < numGPUs; j++) {
-    printf("%6d ", j);
-  }
-
-  printf("\n");
-
-  for (int i = 0; i < numGPUs; i++) {
-    printf("%6d ", i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      printf("%6.02f ", gpuLatencyMatrix[i * numGPUs + j]);
-    }
-
-    printf("\n");
-  }
-
-  printf("\n   CPU");
-
-  for (int j = 0; j < numGPUs; j++) {
-    printf("%6d ", j);
-  }
-
-  printf("\n");
-
-  for (int i = 0; i < numGPUs; i++) {
-    printf("%6d ", i);
-
-    for (int j = 0; j < numGPUs; j++) {
-      printf("%6.02f ", cpuLatencyMatrix[i * numGPUs + j]);
-    }
-
-    printf("\n");
-  }
-
-  for (int d = 0; d < numGPUs; d++) {
-    hipSetDevice(d);
-    hipFree(buffers[d]);
-    hipFree(buffersD2D[d]);
-    cudaCheckError();
-    hipEventDestroy(start[d]);
-    cudaCheckError();
-    hipEventDestroy(stop[d]);
-    cudaCheckError();
-    hipStreamDestroy(stream[d]);
-    cudaCheckError();
-  }
-
-  sdkDeleteTimer(&stopWatch);
-
-  hipHostFree((void *)flag);
-  cudaCheckError();
-}
-
-int main(int argc, char **argv) {
-  int numGPUs, numElems = 40000000;
-  P2PDataTransfer p2p_method = P2P_WRITE;
-
-  hipGetDeviceCount(&numGPUs);
-  cudaCheckError();
-
-  // process command line args
-  if (checkCmdLineFlag(argc, (const char **)argv, "help")) {
-    printHelp();
-    return 0;
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "p2p_read")) {
-    p2p_method = P2P_READ;
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "sm_copy")) {
-    p2p_mechanism = SM;
-  }
-
-  // number of elements of int to be used in copy.
-  if (checkCmdLineFlag(argc, (const char **)argv, "numElems")) {
-    numElems = getCmdLineArgumentInt(argc, (const char **)argv, "numElems");
-  }
-
-  printf("[%s]\n", sSampleName);
-
-  // output devices
-  for (int i = 0; i < numGPUs; i++) {
-    hipDeviceProp_t prop;
-    hipGetDeviceProperties(&prop, i);
-    cudaCheckError();
-    printf("Device: %d, %s, pciBusID: %x, pciDeviceID: %x, pciDomainID:%x\n", i,
-           prop.name, prop.pciBusID, prop.pciDeviceID, prop.pciDomainID);
-  }
-
-  checkP2Paccess(numGPUs);
-
-  // Check peer-to-peer connectivity
-  printf("P2P Connectivity Matrix\n");
-  printf("     D\\D");
-
-  for (int j = 0; j < numGPUs; j++) {
-    printf("%6d", j);
-  }
-  printf("\n");
-
-  for (int i = 0; i < numGPUs; i++) {
-    printf("%6d\t", i);
-    for (int j = 0; j < numGPUs; j++) {
-      if (i != j) {
-        int access;
-        hipDeviceCanAccessPeer(&access, i, j);
-        cudaCheckError();
-        printf("%6d", (access) ? 1 : 0);
-      } else {
-        printf("%6d", 1);
-      }
-    }
-    printf("\n");
-  }
-
-  printf("Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n");
-  outputBandwidthMatrix(numElems, numGPUs, false, P2P_WRITE);
-  printf("Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n");
-  outputBandwidthMatrix(numElems, numGPUs, true, P2P_WRITE);
-  if (p2p_method == P2P_READ) {
-    printf("Unidirectional P2P=Enabled Bandwidth (P2P Reads) Matrix (GB/s)\n");
-    outputBandwidthMatrix(numElems, numGPUs, true, p2p_method);
-  }
-  printf("Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n");
-  outputBidirectionalBandwidthMatrix(numElems, numGPUs, false);
-  printf("Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n");
-  outputBidirectionalBandwidthMatrix(numElems, numGPUs, true);
-
-  printf("P2P=Disabled Latency Matrix (us)\n");
-  outputLatencyMatrix(numGPUs, false, P2P_WRITE);
-  printf("P2P=Enabled Latency (P2P Writes) Matrix (us)\n");
-  outputLatencyMatrix(numGPUs, true, P2P_WRITE);
-  if (p2p_method == P2P_READ) {
-    printf("P2P=Enabled Latency (P2P Reads) Matrix (us)\n");
-    outputLatencyMatrix(numGPUs, true, p2p_method);
-  }
-
-  printf(
-      "\nNOTE: The CUDA Samples are not meant for performance measurements. "
-      "Results may vary when GPU Boost is enabled.\n");
-
-  exit(EXIT_SUCCESS);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator/quasirandomGenerator_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator/quasirandomGenerator_kernel.cu.hip
index 80e373f..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/quasirandomGenerator/quasirandomGenerator_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/quasirandomGenerator/quasirandomGenerator_kernel.cu.hip
@@ -1,180 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef QUASIRANDOMGENERATOR_KERNEL_CUH
-#define QUASIRANDOMGENERATOR_KERNEL_CUH
-
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <helper_cuda.h>
-#include "quasirandomGenerator_common.h"
-
-// Fast integer multiplication
-#define MUL(a, b) __umul24(a, b)
-
-////////////////////////////////////////////////////////////////////////////////
-// Niederreiter quasirandom number generation kernel
-////////////////////////////////////////////////////////////////////////////////
-static __constant__ unsigned int c_Table[QRNG_DIMENSIONS][QRNG_RESOLUTION];
-
-static __global__ void quasirandomGeneratorKernel(float *d_Output,
-                                                  unsigned int seed,
-                                                  unsigned int N) {
-  unsigned int *dimBase = &c_Table[threadIdx.y][0];
-  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
-  unsigned int threadN = MUL(blockDim.x, gridDim.x);
-
-  for (unsigned int pos = tid; pos < N; pos += threadN) {
-    unsigned int result = 0;
-    unsigned int data = seed + pos;
-
-    for (int bit = 0; bit < QRNG_RESOLUTION; bit++, data >>= 1)
-      if (data & 1) {
-        result ^= dimBase[bit];
-      }
-
-    d_Output[MUL(threadIdx.y, N) + pos] = (float)(result + 1) * INT_SCALE;
-  }
-}
-
-// Table initialization routine
-extern "C" void initTableGPU(
-    unsigned int tableCPU[QRNG_DIMENSIONS][QRNG_RESOLUTION]) {
-  HIP_SYMBOL(c_Table)(hipMemcpyToSymbol(
-      c_Table, tableCPU,
-      QRNG_DIMENSIONS * QRNG_RESOLUTION * sizeof(unsigned int)));
-}
-
-// Host-side interface
-extern "C" void quasirandomGeneratorGPU(float *d_Output, unsigned int seed,
-                                        unsigned int N) {
-  dim3 threads(128, QRNG_DIMENSIONS);
-  quasirandomGeneratorKernel<<<128, threads>>>(d_Output, seed, N);
-  getLastCudaError("quasirandomGeneratorKernel() execution failed.\n");
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Moro's Inverse Cumulative Normal Distribution function approximation
-////////////////////////////////////////////////////////////////////////////////
-__device__ inline float MoroInvCNDgpu(unsigned int x) {
-  const float a1 = 2.50662823884f;
-  const float a2 = -18.61500062529f;
-  const float a3 = 41.39119773534f;
-  const float a4 = -25.44106049637f;
-  const float b1 = -8.4735109309f;
-  const float b2 = 23.08336743743f;
-  const float b3 = -21.06224101826f;
-  const float b4 = 3.13082909833f;
-  const float c1 = 0.337475482272615f;
-  const float c2 = 0.976169019091719f;
-  const float c3 = 0.160797971491821f;
-  const float c4 = 2.76438810333863E-02f;
-  const float c5 = 3.8405729373609E-03f;
-  const float c6 = 3.951896511919E-04f;
-  const float c7 = 3.21767881768E-05f;
-  const float c8 = 2.888167364E-07f;
-  const float c9 = 3.960315187E-07f;
-
-  float z;
-
-  bool negate = false;
-
-  // Ensure the conversion to floating point will give a value in the
-  // range (0,0.5] by restricting the input to the bottom half of the
-  // input domain. We will later reflect the result if the input was
-  // originally in the top half of the input domain
-  if (x >= 0x80000000UL) {
-    x = 0xffffffffUL - x;
-    negate = true;
-  }
-
-  // x is now in the range [0,0x80000000) (i.e. [0,0x7fffffff])
-  // Convert to floating point in (0,0.5]
-  const float x1 = 1.0f / static_cast<float>(0xffffffffUL);
-  const float x2 = x1 / 2.0f;
-  float p1 = x * x1 + x2;
-  // Convert to floating point in (-0.5,0]
-  float p2 = p1 - 0.5f;
-
-  // The input to the Moro inversion is p2 which is in the range
-  // (-0.5,0]. This means that our output will be the negative side
-  // of the bell curve (which we will reflect if "negate" is true).
-
-  // Main body of the bell curve for |p| < 0.42
-  if (p2 > -0.42f) {
-    z = p2 * p2;
-    z = p2 * (((a4 * z + a3) * z + a2) * z + a1) /
-        ((((b4 * z + b3) * z + b2) * z + b1) * z + 1.0f);
-  }
-  // Special case (Chebychev) for tail
-  else {
-    z = __logf(-__logf(p1));
-    z = -(c1 + z * (c2 + z * (c3 + z * (c4 + z * (c5 + z * (c6 + z * (c7 + z 
-        * (c8 + z * c9))))))));
-  }
-
-  // If the original input (x) was in the top half of the range, reflect
-  // to get the positive side of the bell curve
-  return negate ? -z : z;
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Main kernel. Choose between transforming
-// input sequence and uniform ascending (0, 1) sequence
-////////////////////////////////////////////////////////////////////////////////
-static __global__ void inverseCNDKernel(float *d_Output, unsigned int *d_Input,
-                                        unsigned int pathN) {
-  unsigned int distance = ((unsigned int)-1) / (pathN + 1);
-  unsigned int tid = MUL(blockDim.x, blockIdx.x) + threadIdx.x;
-  unsigned int threadN = MUL(blockDim.x, gridDim.x);
-
-  // Transform input number sequence if it's supplied
-  if (d_Input) {
-    for (unsigned int pos = tid; pos < pathN; pos += threadN) {
-      unsigned int d = d_Input[pos];
-      d_Output[pos] = (float)MoroInvCNDgpu(d);
-    }
-  }
-  // Else generate input uniformly placed samples on the fly
-  // and write to destination
-  else {
-    for (unsigned int pos = tid; pos < pathN; pos += threadN) {
-      unsigned int d = (pos + 1) * distance;
-      d_Output[pos] = (float)MoroInvCNDgpu(d);
-    }
-  }
-}
-
-extern "C" void inverseCNDgpu(float *d_Output, unsigned int *d_Input,
-                              unsigned int N) {
-  inverseCNDKernel<<<128, 128>>>(d_Output, d_Input, N);
-  getLastCudaError("inverseCNDKernel() execution failed.\n");
-}
-
-#endif
diff --git a/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip b/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip
index c0d9ca4..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/recursiveGaussian/recursiveGaussian_cuda.cu.hip
@@ -1,162 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
-  Recursive Gaussian filter
-  sgreen 8/1/08
-
-  This code sample implements a Gaussian blur using Deriche's recursive method:
-  http://citeseer.ist.psu.edu/deriche93recursively.html
-
-  This is similar to the box filter sample in the SDK, but it uses the previous
-  outputs of the filter as well as the previous inputs. This is also known as an
-  IIR (infinite impulse response) filter, since its response to an input impulse
-  can last forever.
-
-  The main advantage of this method is that the execution time is independent of
-  the filter width.
-
-  The GPU processes columns of the image in parallel. To avoid uncoalesced reads
-  for the row pass we transpose the image and then transpose it back again
-  afterwards.
-
-  The implementation is based on code from the CImg library:
-  http://cimg.sourceforge.net/
-  Thanks to David Tschumperl and all the CImg contributors!
-*/
-
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-#include <helper_math.h>
-
-#include "recursiveGaussian_kernel.cuh"
-
-#define USE_SIMPLE_FILTER 0
-
-// Round a / b to nearest higher integer value
-int iDivUp(int a, int b) { return (a % b != 0) ? (a / b + 1) : (a / b); }
-
-/*
-  Transpose a 2D array (see SDK transpose example)
-*/
-extern "C" void transpose(uint *d_src, uint *d_dest, uint width, int height) {
-  dim3 grid(iDivUp(width, BLOCK_DIM), iDivUp(height, BLOCK_DIM), 1);
-  dim3 threads(BLOCK_DIM, BLOCK_DIM, 1);
-  d_transpose<<<grid, threads>>>(d_dest, d_src, width, height);
-  getLastCudaError("Kernel execution failed");
-}
-
-/*
-  Perform Gaussian filter on a 2D image using CUDA
-
-  Parameters:
-  d_src  - pointer to input image in device memory
-  d_dest - pointer to destination image in device memory
-  d_temp - pointer to temporary storage in device memory
-  width  - image width
-  height - image height
-  sigma  - sigma of Gaussian
-  order  - filter order (0, 1 or 2)
-*/
-
-// 8-bit RGBA version
-extern "C" void gaussianFilterRGBA(uint *d_src, uint *d_dest, uint *d_temp,
-                                   int width, int height, float sigma,
-                                   int order, int nthreads) {
-  // compute filter coefficients
-  const float nsigma = sigma < 0.1f ? 0.1f : sigma, alpha = 1.695f / nsigma,
-              ema = (float)std::exp(-alpha), ema2 = (float)std::exp(-2 * alpha),
-              b1 = -2 * ema, b2 = ema2;
-
-  float a0 = 0, a1 = 0, a2 = 0, a3 = 0, coefp = 0, coefn = 0;
-
-  switch (order) {
-    case 0: {
-      const float k = (1 - ema) * (1 - ema) / (1 + 2 * alpha * ema - ema2);
-      a0 = k;
-      a1 = k * (alpha - 1) * ema;
-      a2 = k * (alpha + 1) * ema;
-      a3 = -k * ema2;
-    } break;
-
-    case 1: {
-      const float k = (1 - ema) * (1 - ema) / ema;
-      a0 = k * ema;
-      a1 = a3 = 0;
-      a2 = -a0;
-    } break;
-
-    case 2: {
-      const float ea = (float)std::exp(-alpha),
-                  k = -(ema2 - 1) / (2 * alpha * ema),
-                  kn = (-2 * (-1 + 3 * ea - 3 * ea * ea + ea * ea * ea) /
-                        (3 * ea + 1 + 3 * ea * ea + ea * ea * ea));
-      a0 = kn;
-      a1 = -kn * (1 + k * alpha) * ema;
-      a2 = kn * (1 - k * alpha) * ema;
-      a3 = -kn * ema2;
-    } break;
-
-    default:
-      fprintf(stderr, "gaussianFilter: invalid order parameter!\n");
-      return;
-  }
-
-  coefp = (a0 + a1) / (1 + b1 + b2);
-  coefn = (a2 + a3) / (1 + b1 + b2);
-
-// process columns
-#if USE_SIMPLE_FILTER
-  d_simpleRecursive_rgba<<<iDivUp(width, nthreads), nthreads>>>(
-      d_src, d_temp, width, height, ema);
-#else
-  d_recursiveGaussian_rgba<<<iDivUp(width, nthreads), nthreads>>>(
-      d_src, d_temp, width, height, a0, a1, a2, a3, b1, b2, coefp, coefn);
-#endif
-  getLastCudaError("Kernel execution failed");
-
-  transpose(d_temp, d_dest, width, height);
-  getLastCudaError("transpose: Kernel execution failed");
-
-// process rows
-#if USE_SIMPLE_FILTER
-  d_simpleRecursive_rgba<<<iDivUp(height, nthreads), nthreads>>>(
-      d_dest, d_temp, height, width, ema);
-#else
-  d_recursiveGaussian_rgba<<<iDivUp(height, nthreads), nthreads>>>(
-      d_dest, d_temp, height, width, a0, a1, a2, a3, b1, b2, coefp, coefn);
-#endif
-  getLastCudaError("Kernel execution failed");
-
-  transpose(d_temp, d_dest, height, width);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip
index b22a868..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleD3D10RenderTarget/simpleD3D10RenderTarget_kernel.cu.hip
@@ -1,226 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* This example demonstrates how to use the CUDA Direct3D bindings with the
- * runtime API.
- * Device code.
- */
-
-#ifndef SIMPLED3D10RENDERTARGET_KERNEL_CU
-#define SIMPLED3D10RENDERTARGET_KERNEL_CU
-
-// includes, C string library
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <stdlib.h>
-#include <string.h>
-
-// includes, cuda
-#include <hip/hip_runtime.h>
-#include <builtin_types.h>
-#include <hip/hip_runtime_api.h>
-
-// includes, project
-#include <helper_cuda.h>  // includes cuda.h and cuda_runtime_api.h
-//#include "HIPCHECK"
-
-#define BIN_COUNT 256
-#define HISTOGRAM_SIZE (BIN_COUNT * sizeof(unsigned int))
-
-texture<uchar4, 2, hipReadModeElementType> colorTex;
-
-////////////////////////////////////////////////////////////////////////////////
-// GPU-specific definitions
-////////////////////////////////////////////////////////////////////////////////
-// Fast mul on G8x / G9x / G100
-#define IMUL(a, b) __mul24(a, b)
-
-// Machine warp size
-// G80's warp size is 32 threads
-#define WARP_LOG2SIZE 5
-
-// Warps in thread block for histogram256Kernel()
-#define WARP_N 6
-
-// Corresponding thread block size in threads for histogram256Kernel()
-#define THREAD_N (WARP_N << WARP_LOG2SIZE)
-
-// Total histogram size (in counters) per thread block for histogram256Kernel()
-#define BLOCK_MEMORY (WARP_N * BIN_COUNT)
-
-// Thread block count for histogram256Kernel()
-#define BLOCK_N 64
-
-////////////////////////////////////////////////////////////////////////////////
-// If threadPos == threadIdx.x, there are always  4-way bank conflicts,
-// since each group of 16 threads (half-warp) accesses different bytes,
-// but only within 4 shared memory banks. Having shuffled bits of threadIdx.x
-// as in histogram64GPU(), each half-warp accesses different shared memory banks
-// avoiding any bank conflicts at all.
-// Refer to the supplied whitepaper for detailed explanations.
-////////////////////////////////////////////////////////////////////////////////
-__device__ inline void addData256(volatile unsigned int *s_WarpHist,
-                                  unsigned int data, unsigned int threadTag) {
-  unsigned int count;
-
-  do {
-    count = s_WarpHist[data] & 0x07FFFFFFU;
-    count = threadTag | (count + 1);
-    s_WarpHist[data] = count;
-  } while (s_WarpHist[data] != count);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Main histogram calculation kernel
-////////////////////////////////////////////////////////////////////////////////
-static __global__ void histogramTex256Kernel(unsigned int *d_Result,
-                                             unsigned int width,
-                                             unsigned int height, int dataN) {
-  // Current global thread index
-  const int globalTid = IMUL(blockIdx.x, blockDim.x) + threadIdx.x;
-  // Total number of threads in the compute grid
-  const int numThreads = IMUL(blockDim.x, gridDim.x);
-
-  // Thread tag for addData256()
-  // WARP_LOG2SIZE higher bits of counter values are tagged
-  // by lower WARP_LOG2SIZE threadID bits
-  const unsigned int threadTag = threadIdx.x << (32 - WARP_LOG2SIZE);
-
-  // Shared memory storage for each warp
-  volatile __shared__ unsigned int s_Hist[BLOCK_MEMORY];
-
-  // Current warp shared memory base
-  const int warpBase = (threadIdx.x >> WARP_LOG2SIZE) * BIN_COUNT;
-
-  // Clear shared memory buffer for current thread block before processing
-  for (int pos = threadIdx.x; pos < BLOCK_MEMORY; pos += blockDim.x)
-    s_Hist[pos] = 0;
-
-  // Cycle through the entire data set, update subhistograms for each warp
-  __syncthreads();
-
-  for (int pos = globalTid; pos < dataN; pos += numThreads) {
-    // NOTE: check this... Not sure this is what needs to be done
-    int py = pos / width;
-    int px = pos - (py * width);
-    uchar4 data4 = tex2D(colorTex, px, py);
-
-    addData256(s_Hist + warpBase, (data4.x), threadTag);
-    addData256(s_Hist + warpBase, (data4.y), threadTag);
-    addData256(s_Hist + warpBase, (data4.z), threadTag);
-    addData256(s_Hist + warpBase, (data4.w), threadTag);
-  }
-
-  __syncthreads();
-
-  // Merge per-warp histograms into per-block and write to global memory
-  for (int pos = threadIdx.x; pos < BIN_COUNT; pos += blockDim.x) {
-    unsigned int sum = 0;
-
-    for (int base = 0; base < BLOCK_MEMORY; base += BIN_COUNT)
-      sum += s_Hist[base + pos] & 0x07FFFFFFU;
-
-    d_Result[blockIdx.x * BIN_COUNT + pos] = sum;
-  }
-}
-
-///////////////////////////////////////////////////////////////////////////////
-// Merge BLOCK_N subhistograms of BIN_COUNT bins into final histogram
-///////////////////////////////////////////////////////////////////////////////
-// gridDim.x   == BIN_COUNT
-// blockDim.x  == BLOCK_N
-// blockIdx.x  == bin counter processed by current block
-// threadIdx.x == subhistogram index
-static __global__ void mergeHistogramTex256Kernel(unsigned int *d_Result) {
-  __shared__ unsigned int data[BLOCK_N];
-
-  // Reads are uncoalesced, but this final stage takes
-  // only a fraction of total processing time
-  data[threadIdx.x] = d_Result[threadIdx.x * BIN_COUNT + blockIdx.x];
-
-  for (int stride = BLOCK_N / 2; stride > 0; stride >>= 1) {
-    __syncthreads();
-
-    if (threadIdx.x < stride) data[threadIdx.x] += data[threadIdx.x + stride];
-  }
-
-  if (threadIdx.x == 0) d_Result[blockIdx.x] = data[0];
-}
-
-////////////////////////////////////////////////////////////////////////////////
-// Host interface to GPU histogram
-////////////////////////////////////////////////////////////////////////////////
-
-extern "C" void checkCudaError() {
-  hipError_t err = hipGetLastError();
-
-  if (hipSuccess != err) {
-    fprintf(stderr, "Cuda error: %s.\n", hipGetErrorString(err));
-    exit(2);
-  }
-}
-
-// Maximum block count for histogram64kernel()
-// Limits input data size to 756MB
-// const int MAX_BLOCK_N = 16384;
-
-// Internal memory allocation
-// const int BLOCK_N2 = 32;
-
-extern "C" void createHistogramTex(unsigned int *h_Result, unsigned int width,
-                                   unsigned int height, hipArray *colorArray) {
-  hipBindTextureToArray(colorTex, colorArray);
-  checkCudaError();
-
-  histogramTex256Kernel<<<BLOCK_N, THREAD_N>>>(h_Result, width, height,
-                                               width * height / 4);
-  checkCudaError();
-
-  mergeHistogramTex256Kernel<<<BIN_COUNT, BLOCK_N>>>(h_Result);
-  checkCudaError();
-
-  hipUnbindTexture(colorTex);
-  checkCudaError();
-
-#if 0
-    // Dummy fill test
-    unsigned int toto[256];
-
-    for (int i=0; i<256; i++)
-    {
-        toto[i] = i * 100;
-    }
-    hipMemcpy(h_Result, toto, HISTOGRAM_SIZE, hipMemcpyHostToDevice);
-#endif
-  checkCudaError();
-}
-
-extern "C" void bindArrayToTexture(hipArray *pArray) {}
-
-#endif  // #ifndef SIMPLED3D10RENDERTARGET_KERNEL_CU
-NEL_CU
diff --git a/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip b/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip
index c7f6a9e..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/simpleVulkan/SineWaveSimulation.cu.hip
@@ -1,136 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include "SineWaveSimulation.h"
-#include <algorithm>
-#include <helper_cuda.h>
-
-__global__ void sinewave(float *heightMap, unsigned int width,
-                         unsigned int height, float time) {
-  const float freq = 4.0f;
-  const size_t stride = gridDim.x * blockDim.x;
-
-  // Iterate through the entire array in a way that is
-  // independent of the grid configuration
-  for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < width * height;
-       tid += stride) {
-    // Calculate the x, y coordinates
-    const size_t y = tid / width;
-    const size_t x = tid - y * width;
-    // Normalize x, y to [0,1]
-    const float u = ((2.0f * x) / width) - 1.0f;
-    const float v = ((2.0f * y) / height) - 1.0f;
-    // Calculate the new height value
-    const float w = 0.5f * sinf(u * freq + time) * cosf(v * freq + time);
-    // Store this new height value
-    heightMap[tid] = w;
-  }
-}
-
-SineWaveSimulation::SineWaveSimulation(size_t width, size_t height)
-    : m_heightMap(nullptr), m_width(width), m_height(height) {}
-
-void SineWaveSimulation::initCudaLaunchConfig(int device) {
-  hipDeviceProp_t prop = {};
-  HIPCHECK(hipSetDevice(device));
-  HIPCHECK(hipGetDeviceProperties(&prop, device));
-
-  // We don't need large block sizes, since there's not much inter-thread
-  // communication
-  m_threads = prop.warpSize;
-
-  // Use the occupancy calculator and fill the gpu as best as we can
-  HIPCHECK(hipOccupancyMaxActiveBlocksPerMultiprocessor(
-      &m_blocks, sinewave, prop.warpSize, 0));
-  m_blocks *= prop.multiProcessorCount;
-
-  // Go ahead and the clamp the blocks to the minimum needed for this
-  // height/width
-  m_blocks = std::min(m_blocks,
-                      (int)((m_width * m_height + m_threads - 1) / m_threads));
-}
-
-int SineWaveSimulation::initCuda(uint8_t *vkDeviceUUID, size_t UUID_SIZE) {
-  int current_device = 0;
-  int device_count = 0;
-  int devices_prohibited = 0;
-
-  hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDeviceCount(&device_count));
-
-  if (device_count == 0) {
-    fprintf(stderr, "CUDA error: no devices supporting CUDA.\n");
-    exit(EXIT_FAILURE);
-  }
-
-  // Find the GPU which is selected by Vulkan
-  while (current_device < device_count) {
-    hipGetDeviceProperties(&deviceProp, current_device);
-
-    if ((deviceProp.computeMode != hipComputeModeProhibited)) {
-      // Compare the cuda device UUID with vulkan UUID
-      int ret = memcmp((void *)&deviceProp.uuid, vkDeviceUUID, UUID_SIZE);
-      if (ret == 0) {
-        HIPCHECK(hipSetDevice(current_device));
-        HIPCHECK(hipGetDeviceProperties(&deviceProp, current_device));
-        printf("GPU Device %d: \"%s\" with compute capability %d.%d\n\n",
-               current_device, deviceProp.name, deviceProp.major,
-               deviceProp.minor);
-
-        return current_device;
-      }
-
-    } else {
-      devices_prohibited++;
-    }
-
-    current_device++;
-  }
-
-  if (devices_prohibited == device_count) {
-    fprintf(stderr,
-            "CUDA error:"
-            " No Vulkan-CUDA Interop capable GPU found.\n");
-    exit(EXIT_FAILURE);
-  }
-
-  return -1;
-}
-
-SineWaveSimulation::~SineWaveSimulation() { m_heightMap = NULL; }
-
-void SineWaveSimulation::initSimulation(float *heights) {
-  m_heightMap = heights;
-}
-
-void SineWaveSimulation::stepSimulation(float time, hipStream_t stream) {
-  sinewave<<<m_blocks, m_threads, 0, stream>>>(m_heightMap, m_width, m_height,
-                                               time);
-  getLastCudaError("Failed to launch CUDA simulation");
-}
diff --git a/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip b/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip
index 8fc6487..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/stereoDisparity/stereoDisparity.cu.hip
@@ -1,284 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/* A CUDA program that demonstrates how to compute a stereo disparity map using
- * SIMD SAD (Sum of Absolute Difference) intrinsics
- */
-
-// includes, system
-#include <stdlib.h>
-#include <stdio.h>
-#include "rocprofiler.h"
-#include "HIPCHECK.h"
-#include <string.h>
-#include <math.h>
-
-// includes, kernels
-#include <hip/hip_runtime.h>
-#include "stereoDisparity_kernel.cuh"
-
-// includes, project
-#include <helper_functions.h>  // helper for shared that are common to CUDA Samples
-#include <helper_cuda.h>  // helper for checking cuda initialization and error checking
-#include <helper_string.h>  // helper functions for string parsing
-
-static const char *sSDKsample = "[stereoDisparity]\0";
-
-int iDivUp(int a, int b) { return ((a % b) != 0) ? (a / b + 1) : (a / b); }
-
-////////////////////////////////////////////////////////////////////////////////
-// declaration, forward
-void runTest(int argc, char **argv);
-
-////////////////////////////////////////////////////////////////////////////////
-// Program main
-////////////////////////////////////////////////////////////////////////////////
-int main(int argc, char **argv) {
-  printf("%s Starting...\n\n", sSDKsample);
-  runTest(argc, argv);
-}
-
-////////////////////////////////////////////////////////////////////////////////
-//! CUDA Sample for calculating depth maps
-////////////////////////////////////////////////////////////////////////////////
-void runTest(int argc, char **argv) {
-  hipDeviceProp_t deviceProp;
-  deviceProp.major = 0;
-  deviceProp.minor = 0;
-  int dev = 0;
-
-  // This will pick the best possible CUDA capable device
-  dev = findCudaDevice(argc, (const char **)argv);
-
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, dev));
-
-  // Statistics about the GPU device
-  printf(
-      "> GPU device has %d Multi-Processors, SM %d.%d compute capabilities\n\n",
-      deviceProp.multiProcessorCount, deviceProp.major, deviceProp.minor);
-
-  StopWatchInterface *timer;
-  sdkCreateTimer(&timer);
-
-  // Search parameters
-  int minDisp = -16;
-  int maxDisp = 0;
-
-  // Load image data
-  // allocate mem for the images on host side
-  // initialize pointers to NULL to request lib call to allocate as needed
-  // PPM images are loaded into 4 byte/pixel memory (RGBX)
-  unsigned char *h_img0 = NULL;
-  unsigned char *h_img1 = NULL;
-  unsigned int w, h;
-  char *fname0 = sdkFindFilePath("stereo.im0.640x533.ppm", argv[0]);
-  char *fname1 = sdkFindFilePath("stereo.im1.640x533.ppm", argv[0]);
-
-  printf("Loaded <%s> as image 0\n", fname0);
-
-  if (!sdkLoadPPM4ub(fname0, &h_img0, &w, &h)) {
-    fprintf(stderr, "Failed to load <%s>\n", fname0);
-  }
-
-  printf("Loaded <%s> as image 1\n", fname1);
-
-  if (!sdkLoadPPM4ub(fname1, &h_img1, &w, &h)) {
-    fprintf(stderr, "Failed to load <%s>\n", fname1);
-  }
-
-  dim3 numThreads = dim3(blockSize_x, blockSize_y, 1);
-  dim3 numBlocks = dim3(iDivUp(w, numThreads.x), iDivUp(h, numThreads.y));
-  unsigned int numData = w * h;
-  unsigned int memSize = sizeof(int) * numData;
-
-  // allocate mem for the result on host side
-  unsigned int *h_odata = (unsigned int *)malloc(memSize);
-
-  // initialize the memory
-  for (unsigned int i = 0; i < numData; i++) h_odata[i] = 0;
-
-  // allocate device memory for result
-  unsigned int *d_odata, *d_img0, *d_img1;
-
-  HIPCHECK(hipMalloc((void **)&d_odata, memSize));
-  HIPCHECK(hipMalloc((void **)&d_img0, memSize));
-  HIPCHECK(hipMalloc((void **)&d_img1, memSize));
-
-  // copy host memory to device to initialize to zeros
-  HIPCHECK(hipMemcpy(d_img0, h_img0, memSize, hipMemcpyHostToDevice));
-  HIPCHECK(hipMemcpy(d_img1, h_img1, memSize, hipMemcpyHostToDevice));
-  HIPCHECK(
-      hipMemcpy(d_odata, h_odata, memSize, hipMemcpyHostToDevice));
-
-  hipChannelFormatDesc ca_desc0 = hipCreateChannelDesc<unsigned int>();
-  hipChannelFormatDesc ca_desc1 = hipCreateChannelDesc<unsigned int>();
-
-  hipTextureObject_t tex2Dleft, tex2Dright;
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypePitch2D;
-  texRes.res.pitch2D.devPtr = d_img0;
-  texRes.res.pitch2D.desc = ca_desc0;
-  texRes.res.pitch2D.width = w;
-  texRes.res.pitch2D.height = h;
-  texRes.res.pitch2D.pitchInBytes = w * 4;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModePoint;
-  texDescr.addressMode[0] = hipAddressModeClamp;
-  texDescr.addressMode[1] = hipAddressModeClamp;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&tex2Dleft, &texRes, &texDescr, NULL));
-
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypePitch2D;
-  texRes.res.pitch2D.devPtr = d_img1;
-  texRes.res.pitch2D.desc = ca_desc1;
-  texRes.res.pitch2D.width = w;
-  texRes.res.pitch2D.height = h;
-  texRes.res.pitch2D.pitchInBytes = w * 4;
-
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = false;
-  texDescr.filterMode = hipFilterModePoint;
-  texDescr.addressMode[0] = hipAddressModeClamp;
-  texDescr.addressMode[1] = hipAddressModeClamp;
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&tex2Dright, &texRes, &texDescr, NULL));
-
-  // First run the warmup kernel (which we'll use to get the GPU in the correct
-  // max power state
-  stereoDisparityKernel<<<numBlocks, numThreads>>>(
-      d_img0, d_img1, d_odata, w, h, minDisp, maxDisp, tex2Dleft, tex2Dright);
-  hipDeviceSynchronize();
-
-  // Allocate CUDA events that we'll use for timing
-  hipEvent_t start, stop;
-  HIPCHECK(hipEventCreate(&start));
-  HIPCHECK(hipEventCreate(&stop));
-
-  printf("Launching CUDA stereoDisparityKernel()\n");
-
-  // Record the start event
-  HIPCHECK(hipEventRecord(start, NULL));
-
-  // launch the stereoDisparity kernel
-  stereoDisparityKernel<<<numBlocks, numThreads>>>(
-      d_img0, d_img1, d_odata, w, h, minDisp, maxDisp, tex2Dleft, tex2Dright);
-
-  // Record the stop event
-  HIPCHECK(hipEventRecord(stop, NULL));
-
-  // Wait for the stop event to complete
-  HIPCHECK(hipEventSynchronize(stop));
-
-  // Check to make sure the kernel didn't fail
-  getLastCudaError("Kernel execution failed");
-
-  float msecTotal = 0.0f;
-  HIPCHECK(hipEventElapsedTime(&msecTotal, start, stop));
-
-  // Copy result from device to host for verification
-  HIPCHECK(
-      hipMemcpy(h_odata, d_odata, memSize, hipMemcpyDeviceToHost));
-
-  printf("Input Size  [%dx%d], ", w, h);
-  printf("Kernel size [%dx%d], ", (2 * RAD + 1), (2 * RAD + 1));
-  printf("Disparities [%d:%d]\n", minDisp, maxDisp);
-
-  printf("GPU processing time : %.4f (ms)\n", msecTotal);
-  printf("Pixel throughput    : %.3f Mpixels/sec\n",
-         ((float)(w * h * 1000.f) / msecTotal) / 1000000);
-
-  // calculate sum of resultant GPU image
-  unsigned int checkSum = 0;
-
-  for (unsigned int i = 0; i < w * h; i++) {
-    checkSum += h_odata[i];
-  }
-
-  printf("GPU Checksum = %u, ", checkSum);
-
-  // write out the resulting disparity image.
-  unsigned char *dispOut = (unsigned char *)malloc(numData);
-  int mult = 20;
-  const char *fnameOut = "output_GPU.pgm";
-
-  for (unsigned int i = 0; i < numData; i++) {
-    dispOut[i] = (int)h_odata[i] * mult;
-  }
-
-  printf("GPU image: <%s>\n", fnameOut);
-  sdkSavePGM(fnameOut, dispOut, w, h);
-
-  // compute reference solution
-  printf("Computing CPU reference...\n");
-  cpu_gold_stereo((unsigned int *)h_img0, (unsigned int *)h_img1,
-                  (unsigned int *)h_odata, w, h, minDisp, maxDisp);
-  unsigned int cpuCheckSum = 0;
-
-  for (unsigned int i = 0; i < w * h; i++) {
-    cpuCheckSum += h_odata[i];
-  }
-
-  printf("CPU Checksum = %u, ", cpuCheckSum);
-  const char *cpuFnameOut = "output_CPU.pgm";
-
-  for (unsigned int i = 0; i < numData; i++) {
-    dispOut[i] = (int)h_odata[i] * mult;
-  }
-
-  printf("CPU image: <%s>\n", cpuFnameOut);
-  sdkSavePGM(cpuFnameOut, dispOut, w, h);
-
-  // cleanup memory
-  HIPCHECK(hipFree(d_odata));
-  HIPCHECK(hipFree(d_img0));
-  HIPCHECK(hipFree(d_img1));
-
-  if (h_odata != NULL) free(h_odata);
-
-  if (h_img0 != NULL) free(h_img0);
-
-  if (h_img1 != NULL) free(h_img1);
-
-  if (dispOut != NULL) free(dispOut);
-
-  sdkDeleteTimer(&timer);
-
-  exit((checkSum == cpuCheckSum) ? EXIT_SUCCESS : EXIT_FAILURE);
-}
diff --git a/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip b/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip
index 42a0b31..e69de29 100644
--- a/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip
+++ b/src/samples/Samples/5_Domain_Specific/volumeRender/volumeRender_kernel.cu.hip
@@ -1,313 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-// Simple 3D volume renderer
-
-#ifndef _VOLUMERENDER_KERNEL_CU_
-#define _VOLUMERENDER_KERNEL_CU_
-
-#include <helper_cuda.h>
-#include <helper_math.h>
-
-typedef unsigned int uint;
-typedef unsigned char uchar;
-
-hipArray *d_volumeArray = 0;
-hipArray *d_transferFuncArray;
-
-typedef unsigned char VolumeType;
-// typedef unsigned short VolumeType;
-
-hipTextureObject_t texObject;    // For 3D texture
-hipTextureObject_t transferTex;  // For 1D transfer function texture
-
-typedef struct { float4 m[3]; } float3x4;
-
-__constant__ float3x4 c_invViewMatrix;  // inverse view matrix
-
-struct Ray {
-  float3 o;  // origin
-  float3 d;  // direction
-};
-
-// intersect ray with a box
-// http://www.siggraph.org/education/materials/HyperGraph/raytrace/rtinter3.htm
-
-__device__ int intersectBox(Ray r, float3 boxmin, float3 boxmax, float *tnear,
-                            float *tfar) {
-  // compute intersection of ray with all six bbox planes
-  float3 invR = make_float3(1.0f) / r.d;
-  float3 tbot = invR * (boxmin - r.o);
-  float3 ttop = invR * (boxmax - r.o);
-
-  // re-order intersections to find smallest and largest on each axis
-  float3 tmin = fminf(ttop, tbot);
-  float3 tmax = fmaxf(ttop, tbot);
-
-  // find the largest tmin and the smallest tmax
-  float largest_tmin = fmaxf(fmaxf(tmin.x, tmin.y), fmaxf(tmin.x, tmin.z));
-  float smallest_tmax = fminf(fminf(tmax.x, tmax.y), fminf(tmax.x, tmax.z));
-
-  *tnear = largest_tmin;
-  *tfar = smallest_tmax;
-
-  return smallest_tmax > largest_tmin;
-}
-
-// transform vector by matrix (no translation)
-__device__ float3 mul(const float3x4 &M, const float3 &v) {
-  float3 r;
-  r.x = dot(v, make_float3(M.m[0]));
-  r.y = dot(v, make_float3(M.m[1]));
-  r.z = dot(v, make_float3(M.m[2]));
-  return r;
-}
-
-// transform vector by matrix with translation
-__device__ float4 mul(const float3x4 &M, const float4 &v) {
-  float4 r;
-  r.x = dot(v, M.m[0]);
-  r.y = dot(v, M.m[1]);
-  r.z = dot(v, M.m[2]);
-  r.w = 1.0f;
-  return r;
-}
-
-__device__ uint rgbaFloatToInt(float4 rgba) {
-  rgba.x = __saturatef(rgba.x);  // clamp to [0.0, 1.0]
-  rgba.y = __saturatef(rgba.y);
-  rgba.z = __saturatef(rgba.z);
-  rgba.w = __saturatef(rgba.w);
-  return (uint(rgba.w * 255) << 24) | (uint(rgba.z * 255) << 16) |
-         (uint(rgba.y * 255) << 8) | uint(rgba.x * 255);
-}
-
-__global__ void d_render(uint *d_output, uint imageW, uint imageH,
-                         float density, float brightness, float transferOffset,
-                         float transferScale, hipTextureObject_t tex,
-                         hipTextureObject_t transferTex) {
-  const int maxSteps = 500;
-  const float tstep = 0.01f;
-  const float opacityThreshold = 0.95f;
-  const float3 boxMin = make_float3(-1.0f, -1.0f, -1.0f);
-  const float3 boxMax = make_float3(1.0f, 1.0f, 1.0f);
-
-  uint x = blockIdx.x * blockDim.x + threadIdx.x;
-  uint y = blockIdx.y * blockDim.y + threadIdx.y;
-
-  if ((x >= imageW) || (y >= imageH)) return;
-
-  float u = (x / (float)imageW) * 2.0f - 1.0f;
-  float v = (y / (float)imageH) * 2.0f - 1.0f;
-
-  // calculate eye ray in world space
-  Ray eyeRay;
-  eyeRay.o =
-      make_float3(mul(c_invViewMatrix, make_float4(0.0f, 0.0f, 0.0f, 1.0f)));
-  eyeRay.d = normalize(make_float3(u, v, -2.0f));
-  eyeRay.d = mul(c_invViewMatrix, eyeRay.d);
-
-  // find intersection with box
-  float tnear, tfar;
-  int hit = intersectBox(eyeRay, boxMin, boxMax, &tnear, &tfar);
-
-  if (!hit) return;
-
-  if (tnear < 0.0f) tnear = 0.0f;  // clamp to near plane
-
-  // march along ray from front to back, accumulating color
-  float4 sum = make_float4(0.0f);
-  float t = tnear;
-  float3 pos = eyeRay.o + eyeRay.d * tnear;
-  float3 step = eyeRay.d * tstep;
-
-  for (int i = 0; i < maxSteps; i++) {
-    // read from 3D texture
-    // remap position to [0, 1] coordinates
-    float sample = tex3D<float>(tex, pos.x * 0.5f + 0.5f, pos.y * 0.5f + 0.5f,
-                                pos.z * 0.5f + 0.5f);
-    // sample *= 64.0f;    // scale for 10-bit data
-
-    // lookup in transfer function texture
-    float4 col =
-        tex1D<float4>(transferTex, (sample - transferOffset) * transferScale);
-    col.w *= density;
-
-    // "under" operator for back-to-front blending
-    // sum = lerp(sum, col, col.w);
-
-    // pre-multiply alpha
-    col.x *= col.w;
-    col.y *= col.w;
-    col.z *= col.w;
-    // "over" operator for front-to-back blending
-    sum = sum + col * (1.0f - sum.w);
-
-    // exit early if opaque
-    if (sum.w > opacityThreshold) break;
-
-    t += tstep;
-
-    if (t > tfar) break;
-
-    pos += step;
-  }
-
-  sum *= brightness;
-
-  // write output color
-  d_output[y * imageW + x] = rgbaFloatToInt(sum);
-}
-
-extern "C" void setTextureFilterMode(bool bLinearFilter) {
-  if (texObject) {
-    HIPCHECK(hipDestroyTextureObject(texObject));
-  }
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = d_volumeArray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords = true;
-  texDescr.filterMode =
-      bLinearFilter ? hipFilterModeLinear : hipFilterModePoint;
-
-  texDescr.addressMode[0] = hipAddressModeWrap;
-  texDescr.addressMode[1] = hipAddressModeWrap;
-  texDescr.addressMode[2] = hipAddressModeWrap;
-
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texObject, &texRes, &texDescr, NULL));
-}
-
-extern "C" void initCuda(void *h_volume, hipExtent volumeSize) {
-  // create 3D array
-  hipChannelFormatDesc channelDesc = hipCreateChannelDesc<VolumeType>();
-  HIPCHECK(hipMalloc3DArray(&d_volumeArray, &channelDesc, volumeSize));
-
-  // copy data to 3D array
-  hipMemcpy3DParms copyParams = {0};
-  copyParams.srcPtr =
-      make_hipPitchedPtr(h_volume, volumeSize.width * sizeof(VolumeType),
-                          volumeSize.width, volumeSize.height);
-  copyParams.dstArray = d_volumeArray;
-  copyParams.extent = volumeSize;
-  copyParams.kind = hipMemcpyHostToDevice;
-  HIPCHECK(hipMemcpy3D(&copyParams));
-
-  hipResourceDesc texRes;
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = d_volumeArray;
-
-  hipTextureDesc texDescr;
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords =
-      true;  // access with normalized texture coordinates
-  texDescr.filterMode = hipFilterModeLinear;  // linear interpolation
-
-  texDescr.addressMode[0] = hipAddressModeClamp;  // clamp texture coordinates
-  texDescr.addressMode[1] = hipAddressModeClamp;
-  texDescr.addressMode[2] = hipAddressModeClamp;
-
-  texDescr.readMode = hipReadModeNormalizedFloat;
-
-  HIPCHECK(
-      hipCreateTextureObject(&texObject, &texRes, &texDescr, NULL));
-
-  // create transfer function texture
-  float4 transferFunc[] = {
-    {  0.0, 0.0, 0.0, 0.0, },
-    {  1.0, 0.0, 0.0, 1.0, },
-    {  1.0, 0.5, 0.0, 1.0, },
-    {  1.0, 1.0, 0.0, 1.0, },
-    {  0.0, 1.0, 0.0, 1.0, },
-    {  0.0, 1.0, 1.0, 1.0, },
-    {  0.0, 0.0, 1.0, 1.0, },
-    {  1.0, 0.0, 1.0, 1.0, },
-    {  0.0, 0.0, 0.0, 0.0, },
-  };
-
-  hipChannelFormatDesc channelDesc2 = hipCreateChannelDesc<float4>();
-  hipArray *d_transferFuncArray;
-  HIPCHECK(hipMallocArray(&d_transferFuncArray, &channelDesc2,
-                                  sizeof(transferFunc) / sizeof(float4), 1));
-  HIPCHECK(hipMemcpy2DToArray(d_transferFuncArray, 0, 0, transferFunc,
-                                      0, sizeof(transferFunc), 1,
-                                      hipMemcpyHostToDevice));
-
-  memset(&texRes, 0, sizeof(hipResourceDesc));
-
-  texRes.resType = hipResourceTypeArray;
-  texRes.res.array.array = d_transferFuncArray;
-
-  memset(&texDescr, 0, sizeof(hipTextureDesc));
-
-  texDescr.normalizedCoords =
-      true;  // access with normalized texture coordinates
-  texDescr.filterMode = hipFilterModeLinear;
-
-  texDescr.addressMode[0] = hipAddressModeClamp;  // wrap texture coordinates
-
-  texDescr.readMode = hipReadModeElementType;
-
-  HIPCHECK(
-      hipCreateTextureObject(&transferTex, &texRes, &texDescr, NULL));
-}
-
-extern "C" void freeCudaBuffers() {
-  HIPCHECK(hipDestroyTextureObject(texObject));
-  HIPCHECK(hipDestroyTextureObject(transferTex));
-  HIPCHECK(hipFreeArray(d_volumeArray));
-  HIPCHECK(hipFreeArray(d_transferFuncArray));
-}
-
-extern "C" void render_kernel(dim3 gridSize, dim3 blockSize, uint *d_output,
-                              uint imageW, uint imageH, float density,
-                              float brightness, float transferOffset,
-                              float transferScale) {
-  d_render<<<gridSize, blockSize>>>(d_output, imageW, imageH, density,
-                                    brightness, transferOffset, transferScale,
-                                    texObject, transferTex);
-}
-
-extern "C" void copyInvViewMatrix(float *invViewMatrix, size_t sizeofMatrix) {
-  HIP_SYMBOL(c_invViewMatrix)(
-      hipMemcpyToSymbol(c_invViewMatrix, invViewMatrix, sizeofMatrix));
-}
-
-#endif  // #ifndef _VOLUMERENDER_KERNEL_CU_
-trix, invViewMatrix, sizeofMatrix));
-}
diff --git a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip
index 08cdbe4..e69de29 100644
--- a/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip
+++ b/src/samples/Samples/6_Performance/UnifiedMemoryPerf/matrixMultiplyPerf.cu.hip
@@ -1,707 +0,0 @@
-/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *  * Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- *  * Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *  * Neither the name of NVIDIA CORPORATION nor the names of its
- *    contributors may be used to endorse or promote products derived
- *    from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
- * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
- * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
- * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
- * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
- * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
- * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
- * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-
-#include <hip/hip_runtime.h>
-#include <helper_cuda.h>
-#include <helper_timer.h>
-#include "commonDefs.hpp"
-#include "commonKernels.hpp"
-
-#define VERIFY_GPU_CORRECTNESS 0
-
-size_t maxSampleSizeInMb = 64;
-int numKernelRuns = 20;
-int verboseResults = 0;
-
-const char *memAllocTypeStr[MEMALLOC_TYPE_COUNT] = {
-    "Managed_Memory_With_Hints",
-    "Managed_Memory_With_Hints_FullyAsync",
-    "Managed_Memory_NoHints",
-    "Zero_Copy",
-    "Memcpy_HostMalloc_DeviceCudaMalloc",
-    "MemcpyAsync_HostMalloc_DeviceCudaMalloc",
-    "Memcpy_HostCudaHostAlloc_DeviceCudaMalloc",
-    "MemcpyAsync_HostCudaHostAlloc_DeviceCudaMalloc"};
-
-const char *memAllocTypeShortStr[MEMALLOC_TYPE_COUNT] = {
-    "UMhint",   // Managed Memory With Hints
-    "UMhntAs",  // Managed Memory With_Hints Async
-    "UMeasy",   // Managed_Memory with No Hints
-    "0Copy",    // Zero Copy
-    "MemCopy",  // USE HOST PAGEABLE AND DEVICE_MEMORY
-    "CpAsync",  // USE HOST PAGEABLE AND DEVICE_MEMORY ASYNC
-    "CpHpglk",  // USE HOST PAGELOCKED AND DEVICE MEMORY
-    "CpPglAs"   // USE HOST PAGELOCKED AND DEVICE MEMORY ASYNC
-};
-
-static float RandFloat(float low, float high) {
-  float t = (float)rand() / (float)RAND_MAX;
-  return (1.0f - t) * low + t * high;
-}
-
-void fillMatrixWithRandomValues(float *matrix, unsigned int matrixDim) {
-  unsigned int i, j;
-  for (i = 0; i < matrixDim; ++i) {
-    for (j = 0; j < matrixDim; ++j) {
-      matrix[j + i * matrixDim] = RandFloat(0.0f, 10.0f);
-    }
-  }
-}
-
-#if VERIFY_GPU_CORRECTNESS
-void verifyMatrixMultiplyCorrectness(float *C, float *A, float *B,
-                                     unsigned int matrixDim) {
-  unsigned int i, j, k, numErrors = 0;
-  for (i = 0; i < matrixDim; ++i) {
-    for (j = 0; j < matrixDim; ++j) {
-      float result = 0.0f;
-      for (k = 0; k < matrixDim; ++k) {
-        result += A[k + i * matrixDim] * B[j + k * matrixDim];
-      }
-      if (fabs(C[j + i * matrixDim] - result) > 0.001 * matrixDim) {
-        printf("At [%u, %u]: Expected %f, Found %f\n", i, j, result,
-               C[j + i * matrixDim]);
-        ++numErrors;
-      }
-    }
-  }
-  if (numErrors != 0) {
-    printf("%d value mismatches occured\n", numErrors);
-    fflush(stdout);
-    exit(EXIT_FAILURE);  // exit since value mismatches occured
-  }
-}
-#endif
-
-void copyMatrix(float *dstMatrix, float *srcMatrix, unsigned int matrixDim) {
-  size_t size = matrixDim * matrixDim * sizeof(float);
-  memcpy(dstMatrix, srcMatrix, size);
-}
-
-void verifyMatrixData(float *expectedData, float *observedData,
-                      unsigned int matrixDim) {
-  unsigned int i, j, numErrors = 0;
-  for (i = 0; i < matrixDim; ++i) {
-    for (j = 0; j < matrixDim; ++j) {
-      if (expectedData[j + i * matrixDim] != observedData[j + i * matrixDim]) {
-        ++numErrors;
-        if (verboseResults) {
-          printf("At [%u, %u]: Expected %f, Found %f\n", i, j,
-                 expectedData[j + i * matrixDim],
-                 observedData[j + i * matrixDim]);
-        }
-      }
-    }
-  }
-  if (numErrors != 0) {
-    printf("%d value mismatches occured\n", numErrors);
-    fflush(stdout);
-    exit(EXIT_FAILURE);  // exit since value mismatches occured
-  }
-}
-
-#define BLOCK_SIZE 32
-__global__ void matrixMultiplyKernel(float *C, float *A, float *B,
-                                     unsigned int matrixDim) {
-  // Block index
-  int bx = blockIdx.x;
-  int by = blockIdx.y;
-
-  // Thread index
-  int tx = threadIdx.x;
-  int ty = threadIdx.y;
-
-  unsigned int wA = matrixDim;
-  unsigned int wB = matrixDim;
-
-  // Index of the first sub-matrix of A processed by the block
-  int aBegin = matrixDim * BLOCK_SIZE * by;
-
-  // Index of the last sub-matrix of A processed by the block
-  int aEnd = aBegin + wA - 1;
-
-  // Step size used to iterate through the sub-matrices of A
-  int aStep = BLOCK_SIZE;
-
-  // Index of the first sub-matrix of B processed by the block
-  int bBegin = BLOCK_SIZE * bx;
-
-  // Step size used to iterate through the sub-matrices of B
-  int bStep = BLOCK_SIZE * wB;
-
-  // Csub is used to store the element of the block sub-matrix
-  // that is computed by the thread
-  float Csub = 0;
-
-  // Loop over all the sub-matrices of A and B
-  // required to compute the block sub-matrix
-  for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {
-    // Declaration of the shared memory array As used to
-    // store the sub-matrix of A
-    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
-
-    // Declaration of the shared memory array Bs used to
-    // store the sub-matrix of B
-    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
-
-    // Load the matrices from device memory
-    // to shared memory; each thread loads
-    // one element of each matrix
-    As[ty][tx] = A[a + wA * ty + tx];
-    Bs[ty][tx] = B[b + wB * ty + tx];
-
-    // Synchronize to make sure the matrices are loaded
-    __syncthreads();
-
-    // Multiply the two matrices together;
-    // each thread computes one element
-    // of the block sub-matrix
-#pragma unroll
-
-    for (int k = 0; k < BLOCK_SIZE; ++k) {
-      Csub += As[ty][k] * Bs[k][tx];
-    }
-
-    // Synchronize to make sure that the preceding
-    // computation is done before loading two new
-    // sub-matrices of A and B in the next iteration
-    __syncthreads();
-  }
-
-  // Write the block sub-matrix to device memory;
-  // each thread writes one element
-  int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;
-  C[c + wB * ty + tx] = Csub;
-}
-
-void runMatrixMultiplyKernel(unsigned int matrixDim, int allocType,
-                             unsigned int numLoops, double *gpuLaunchCallsTimes,
-                             double *gpuTransferToCallsTimes,
-                             double *gpuTransferFromCallsTimes,
-                             double *gpuLaunchAndTransferCallsTimes,
-                             double *gpuLaunchTransferSyncTimes,
-                             double *cpuAccessTimes, double *overallTimes,
-                             int device_id) {
-  float *dptrA = NULL, *hptrA = NULL;
-  float *dptrB = NULL, *hptrB = NULL;
-  float *dptrC = NULL, *hptrC = NULL;
-  float *randValuesX = NULL, *randValuesY = NULL;
-  float *randValuesVerifyXmulY = NULL, *randValuesVerifyYmulX = NULL;
-  bool copyRequired = false, hintsRequired = false;
-  bool someTransferOpRequired;
-  bool isAsync = false;
-  hipStream_t streamToRunOn;
-  unsigned int *latch;
-  size_t size = matrixDim * matrixDim * sizeof(float);
-  dim3 threads(32, 32);
-  dim3 grid(matrixDim / threads.x, matrixDim / threads.y);
-  StopWatchInterface *gpuLaunchCallsTimer = 0, *gpuTransferCallsTimer = 0;
-  StopWatchInterface *gpuSyncTimer = 0, *cpuAccessTimer = 0;
-  sdkCreateTimer(&gpuLaunchCallsTimer);
-  sdkCreateTimer(&gpuTransferCallsTimer);
-  sdkCreateTimer(&gpuSyncTimer);
-  sdkCreateTimer(&cpuAccessTimer);
-  unsigned int i;
-
-  hipDeviceProp_t deviceProp;
-  HIPCHECK(hipGetDeviceProperties(&deviceProp, device_id));
-  HIPCHECK(hipStreamCreate(&streamToRunOn));
-
-  randValuesX = (float *)malloc(size);
-  if (!randValuesX) {
-    exit(EXIT_FAILURE);  // exit since memory allocation error
-  }
-  randValuesY = (float *)malloc(size);
-  if (!randValuesY) {
-    exit(EXIT_FAILURE);  // exit since memory allocation error
-  }
-  randValuesVerifyXmulY = (float *)malloc(size);
-  if (!randValuesVerifyXmulY) {
-    exit(EXIT_FAILURE);  // exit since memory allocation error
-  }
-  randValuesVerifyYmulX = (float *)malloc(size);
-  if (!randValuesVerifyYmulX) {
-    exit(EXIT_FAILURE);  // exit since memory allocation error
-  }
-  HIPCHECK(hipMalloc(&dptrA, size));
-  HIPCHECK(hipMalloc(&dptrB, size));
-  HIPCHECK(hipMalloc(&dptrC, size));
-
-  fillMatrixWithRandomValues(randValuesX, matrixDim);
-  fillMatrixWithRandomValues(randValuesY, matrixDim);
-
-  HIPCHECK(
-      hipMemcpyAsync(dptrA, randValuesX, size, hipMemcpyHostToDevice));
-  HIPCHECK(
-      hipMemcpyAsync(dptrB, randValuesY, size, hipMemcpyHostToDevice));
-  matrixMultiplyKernel<<<grid, threads>>>(dptrC, dptrA, dptrB, matrixDim);
-  HIPCHECK(hipMemcpyAsync(randValuesVerifyXmulY, dptrC, size,
-                                  hipMemcpyDeviceToHost));
-  HIPCHECK(hipStreamSynchronize(NULL));
-  matrixMultiplyKernel<<<grid, threads>>>(dptrC, dptrB, dptrA, matrixDim);
-  HIPCHECK(hipMemcpyAsync(randValuesVerifyYmulX, dptrC, size,
-                                  hipMemcpyDeviceToHost));
-  HIPCHECK(hipStreamSynchronize(NULL));
-#if VERIFY_GPU_CORRECTNESS
-  verifyMatrixMultiplyCorrectness(randValuesVerifyXmulY, randValuesX,
-                                  randValuesY, matrixDim);
-  verifyMatrixMultiplyCorrectness(randValuesVerifyYmulX, randValuesY,
-                                  randValuesX, matrixDim);
-#endif
-  HIPCHECK(hipFree(dptrA));
-  HIPCHECK(hipFree(dptrB));
-  HIPCHECK(hipFree(dptrC));
-
-  HIPCHECK(hipHostMalloc(&latch, sizeof(unsigned int)));
-
-  switch (allocType) {
-    case USE_HOST_PAGEABLE_AND_DEVICE_MEMORY:
-    case USE_HOST_PAGEABLE_AND_DEVICE_MEMORY_ASYNC:
-      hptrA = (float *)malloc(size);
-      if (!hptrA) {
-        exit(EXIT_FAILURE);  // exit since memory allocation error
-      }
-      hptrB = (float *)malloc(size);
-      if (!hptrB) {
-        exit(EXIT_FAILURE);  // exit since memory allocation error
-      }
-      hptrC = (float *)malloc(size);
-      if (!hptrC) {
-        exit(EXIT_FAILURE);  // exit since memory allocation error
-      }
-      HIPCHECK(hipMalloc(&dptrA, size));
-      HIPCHECK(hipMalloc(&dptrB, size));
-      HIPCHECK(hipMalloc(&dptrC, size));
-      copyRequired = true;
-      break;
-
-    case USE_HOST_PAGELOCKED_AND_DEVICE_MEMORY:
-    case USE_HOST_PAGELOCKED_AND_DEVICE_MEMORY_ASYNC:
-      HIPCHECK(hipHostMalloc(&hptrA, size));
-      HIPCHECK(hipHostMalloc(&hptrB, size));
-      HIPCHECK(hipHostMalloc(&hptrC, size));
-      HIPCHECK(hipMalloc(&dptrA, size));
-      HIPCHECK(hipMalloc(&dptrB, size));
-      HIPCHECK(hipMalloc(&dptrC, size));
-      copyRequired = true;
-      break;
-
-    case USE_ZERO_COPY:
-      HIPCHECK(hipHostMalloc(&hptrA, size));
-      HIPCHECK(hipHostMalloc(&hptrB, size));
-      HIPCHECK(hipHostMalloc(&hptrC, size));
-      HIPCHECK(hipHostGetDevicePointer(&dptrA, hptrA, 0));
-      HIPCHECK(hipHostGetDevicePointer(&dptrB, hptrB, 0));
-      HIPCHECK(hipHostGetDevicePointer(&dptrC, hptrC, 0));
-      break;
-
-    case USE_MANAGED_MEMORY:
-      HIPCHECK(hipMallocManaged(&dptrA, size));
-      HIPCHECK(hipMallocManaged(&dptrB, size));
-      HIPCHECK(hipMallocManaged(&dptrC, size));
-      hptrA = dptrA;
-      hptrB = dptrB;
-      hptrC = dptrC;
-      break;
-
-    case USE_MANAGED_MEMORY_WITH_HINTS:
-    case USE_MANAGED_MEMORY_WITH_HINTS_ASYNC:
-      if (deviceProp.concurrentManagedAccess) {
-        HIPCHECK(hipMallocManaged(&dptrA, size));
-        HIPCHECK(hipMallocManaged(&dptrB, size));
-        HIPCHECK(hipMallocManaged(&dptrC, size));
-        HIPCHECK(hipMemPrefetchAsync(dptrA, size, hipCpuDeviceId));
-        HIPCHECK(hipMemPrefetchAsync(dptrB, size, hipCpuDeviceId));
-        HIPCHECK(hipMemPrefetchAsync(dptrC, size, hipCpuDeviceId));
-      } else {
-        HIPCHECK(hipMallocManaged(&dptrA, size, hipMemAttachHost));
-        HIPCHECK(hipMallocManaged(&dptrB, size, hipMemAttachHost));
-        HIPCHECK(hipMallocManaged(&dptrC, size, hipMemAttachHost));
-      }
-      hptrA = dptrA;
-      hptrB = dptrB;
-      hptrC = dptrC;
-      hintsRequired = true;
-      break;
-
-    default:
-      exit(EXIT_FAILURE);  // exit with error
-  }
-
-  if (allocType == USE_HOST_PAGEABLE_AND_DEVICE_MEMORY_ASYNC ||
-      allocType == USE_HOST_PAGELOCKED_AND_DEVICE_MEMORY_ASYNC ||
-      allocType == USE_MANAGED_MEMORY_WITH_HINTS_ASYNC) {
-    isAsync = true;
-  }
-
-  someTransferOpRequired = copyRequired || hintsRequired;
-
-  // fill buffers with 0 to avoid any first access page-fault overheads.
-  memset(hptrA, 0, size);
-  memset(hptrB, 0, size);
-  memset(hptrC, 0, size);
-
-  for (i = 0; i < numLoops; i++) {
-    cpuAccessTimes[i] = 0.0;
-    gpuLaunchCallsTimes[i] = 0.0;
-    gpuTransferToCallsTimes[i] = 0.0;
-    gpuTransferFromCallsTimes[i] = 0.0;
-
-    sdkStartTimer(&cpuAccessTimer);
-    {
-      copyMatrix(hptrA, (i & 0x1 == 0) ? randValuesX : randValuesY, matrixDim);
-      copyMatrix(hptrB, (i & 0x1 == 0) ? randValuesY : randValuesX, matrixDim);
-    }
-    sdkStopTimer(&cpuAccessTimer);
-    cpuAccessTimes[i] += sdkGetAverageTimerValue(&cpuAccessTimer);
-    sdkResetTimer(&cpuAccessTimer);
-
-    if (isAsync && hintsRequired) {
-      *latch = 0;
-      // Prevent any work on stream from starting until all work is pushed
-      spinWhileLessThanOne<<<1, 1, 0, streamToRunOn>>>(latch);
-    }
-
-    if (someTransferOpRequired) {
-      sdkStartTimer(&gpuTransferCallsTimer);
-      if (copyRequired) {
-        if (isAsync) {
-          HIPCHECK(hipMemcpyAsync(
-              dptrA, hptrA, size, hipMemcpyHostToDevice, streamToRunOn));
-          HIPCHECK(hipMemcpyAsync(
-              dptrB, hptrB, size, hipMemcpyHostToDevice, streamToRunOn));
-        } else {
-          HIPCHECK(
-              hipMemcpy(dptrA, hptrA, size, hipMemcpyHostToDevice));
-          HIPCHECK(
-              hipMemcpy(dptrB, hptrB, size, hipMemcpyHostToDevice));
-        }
-      }
-      if (hintsRequired) {
-        if (deviceProp.concurrentManagedAccess) {
-          HIPCHECK(
-              hipMemPrefetchAsync(dptrA, size, device_id, streamToRunOn));
-          HIPCHECK(
-              hipMemPrefetchAsync(dptrB, size, device_id, streamToRunOn));
-          HIPCHECK(
-              hipMemPrefetchAsync(dptrC, size, device_id, streamToRunOn));
-        } else {
-          HIPCHECK(hipStreamAttachMemAsync(streamToRunOn, dptrA, 0,
-                                                   hipMemAttachGlobal));
-          HIPCHECK(hipStreamAttachMemAsync(streamToRunOn, dptrB, 0,
-                                                   hipMemAttachGlobal));
-          HIPCHECK(hipStreamAttachMemAsync(streamToRunOn, dptrC, 0,
-                                                   hipMemAttachGlobal));
-        }
-        if (!isAsync) {
-          HIPCHECK(hipStreamSynchronize(streamToRunOn));
-        }
-      }
-
-      sdkStopTimer(&gpuTransferCallsTimer);
-      gpuTransferToCallsTimes[i] +=
-          sdkGetAverageTimerValue(&gpuTransferCallsTimer);
-      sdkResetTimer(&gpuTransferCallsTimer);
-    }
-
-    sdkStartTimer(&gpuLaunchCallsTimer);
-    {
-      matrixMultiplyKernel<<<grid, threads, 0, streamToRunOn>>>(
-          dptrC, dptrA, dptrB, matrixDim);
-      if (!isAsync) {
-        HIPCHECK(hipStreamSynchronize(streamToRunOn));
-      }
-    }
-    sdkStopTimer(&gpuLaunchCallsTimer);
-
-    gpuLaunchCallsTimes[i] += sdkGetAverageTimerValue(&gpuLaunchCallsTimer);
-    sdkResetTimer(&gpuLaunchCallsTimer);
-
-    if (someTransferOpRequired) {
-      sdkStartTimer(&gpuTransferCallsTimer);
-      if (hintsRequired) {
-        if (deviceProp.concurrentManagedAccess) {
-          HIPCHECK(hipMemPrefetchAsync(dptrA, size, hipCpuDeviceId));
-          HIPCHECK(hipMemPrefetchAsync(dptrB, size, hipCpuDeviceId));
-          HIPCHECK(hipMemPrefetchAsync(dptrC, size, hipCpuDeviceId));
-        } else {
-          HIPCHECK(hipStreamAttachMemAsync(streamToRunOn, dptrA, 0,
-                                                   hipMemAttachHost));
-          HIPCHECK(hipStreamAttachMemAsync(streamToRunOn, dptrB, 0,
-                                                   hipMemAttachHost));
-          HIPCHECK(hipStreamAttachMemAsync(streamToRunOn, dptrC, 0,
-                                                   hipMemAttachHost));
-        }
-        if (!isAsync) {
-          HIPCHECK(hipStreamSynchronize(streamToRunOn));
-        }
-      }
-      if (copyRequired) {
-        if (isAsync) {
-          HIPCHECK(hipMemcpyAsync(
-              hptrC, dptrC, size, hipMemcpyDeviceToHost, streamToRunOn));
-        } else {
-          HIPCHECK(
-              hipMemcpy(hptrC, dptrC, size, hipMemcpyDeviceToHost));
-        }
-      }
-      sdkStopTimer(&gpuTransferCallsTimer);
-      gpuTransferFromCallsTimes[i] +=
-          sdkGetAverageTimerValue(&gpuTransferCallsTimer);
-      sdkResetTimer(&gpuTransferCallsTimer);
-    }
-    gpuLaunchAndTransferCallsTimes[i] = gpuLaunchCallsTimes[i] +
-                                        gpuTransferToCallsTimes[i] +
-                                        gpuTransferFromCallsTimes[i];
-    gpuLaunchTransferSyncTimes[i] = gpuLaunchAndTransferCallsTimes[i];
-    if (isAsync) {
-      sdkStartTimer(&gpuSyncTimer);
-      {
-        if (hintsRequired) {
-          *latch = 1;
-        }
-        HIPCHECK(hipStreamSynchronize(streamToRunOn));
-      }
-      sdkStopTimer(&gpuSyncTimer);
-      gpuLaunchTransferSyncTimes[i] += sdkGetAverageTimerValue(&gpuSyncTimer);
-      sdkResetTimer(&gpuSyncTimer);
-    }
-
-    sdkStartTimer(&cpuAccessTimer);
-    {
-      verifyMatrixData(
-          (i & 0x1 == 0) ? randValuesVerifyXmulY : randValuesVerifyYmulX, hptrC,
-          matrixDim);
-    }
-    sdkStopTimer(&cpuAccessTimer);
-    cpuAccessTimes[i] += sdkGetAverageTimerValue(&cpuAccessTimer);
-    sdkResetTimer(&cpuAccessTimer);
-    overallTimes[i] = cpuAccessTimes[i] + gpuLaunchTransferSyncTimes[i];
-  }
-
-  switch (allocType) {
-    case USE_HOST_PAGEABLE_AND_DEVICE_MEMORY:
-    case USE_HOST_PAGEABLE_AND_DEVICE_MEMORY_ASYNC:
-      free(hptrA);
-      free(hptrB);
-      free(hptrC);
-      HIPCHECK(hipFree(dptrA));
-      HIPCHECK(hipFree(dptrB));
-      HIPCHECK(hipFree(dptrC));
-      break;
-
-    case USE_HOST_PAGELOCKED_AND_DEVICE_MEMORY:
-    case USE_HOST_PAGELOCKED_AND_DEVICE_MEMORY_ASYNC:
-      HIPCHECK(hipHostFree(hptrA));
-      HIPCHECK(hipHostFree(hptrB));
-      HIPCHECK(hipHostFree(hptrC));
-      HIPCHECK(hipFree(dptrA));
-      HIPCHECK(hipFree(dptrB));
-      HIPCHECK(hipFree(dptrC));
-      break;
-
-    case USE_ZERO_COPY:
-      HIPCHECK(hipHostFree(hptrA));
-      HIPCHECK(hipHostFree(hptrB));
-      HIPCHECK(hipHostFree(hptrC));
-      break;
-
-    case USE_MANAGED_MEMORY:
-    case USE_MANAGED_MEMORY_WITH_HINTS:
-    case USE_MANAGED_MEMORY_WITH_HINTS_ASYNC:
-      HIPCHECK(hipFree(dptrA));
-      HIPCHECK(hipFree(dptrB));
-      HIPCHECK(hipFree(dptrC));
-      break;
-
-    default:
-      exit(EXIT_FAILURE);  // exit due to error
-  }
-
-  HIPCHECK(hipStreamDestroy(streamToRunOn));
-  HIPCHECK(hipHostFree(latch));
-  free(randValuesX);
-  free(randValuesY);
-  free(randValuesVerifyXmulY);
-  free(randValuesVerifyYmulX);
-  sdkDeleteTimer(&gpuLaunchCallsTimer);
-  sdkDeleteTimer(&gpuTransferCallsTimer);
-  sdkDeleteTimer(&gpuSyncTimer);
-  sdkDeleteTimer(&cpuAccessTimer);
-}
-
-void matrixMultiplyPerfRunner(bool reportAsBandwidth,
-                              bool print_launch_transfer_results,
-                              bool print_std_deviation, int device_id) {
-  int i;
-  unsigned int minMatrixDim = 32;
-  unsigned int multiplierDim = 2;
-  unsigned int matrixDim;
-  unsigned int minSize = minMatrixDim * minMatrixDim * sizeof(float);
-  unsigned int maxSize =
-      (maxSampleSizeInMb * ONE_MB) /
-      4;  // 3 buffers are used, but dividing by 4 (power of 2)
-  unsigned int multiplier = multiplierDim * multiplierDim;
-  unsigned int numSizesToTest;
-
-  struct testResults *results;
-  struct resultsData *gpuLaunchCallsTimes;
-  struct resultsData *gpuTransferToCallsTimes;
-  struct resultsData *gpuTransferFromCallsTimes;
-  struct resultsData *gpuLaunchAndTransferCallsTimes;
-  struct resultsData *gpuLaunchTransferSyncTimes;
-  struct resultsData *cpuAccessTimes;
-  struct resultsData *overallTimes;
-  unsigned long *sizesToTest;
-  unsigned int j;
-
-  numSizesToTest = findNumSizesToTest(minSize, maxSize, multiplier);
-
-  createAndInitTestResults(&results, "matrixMultiplyPerf", numKernelRuns,
-                           numSizesToTest);
-
-  sizesToTest = getPtrSizesToTest(results);
-
-  createResultDataAndAddToTestResults(&gpuLaunchCallsTimes, results,
-                                      "GPU Kernel Launch Call Time", false,
-                                      reportAsBandwidth);
-  createResultDataAndAddToTestResults(&gpuTransferToCallsTimes, results,
-                                      "CPU to GPU Transfer Calls Time", false,
-                                      reportAsBandwidth);
-  createResultDataAndAddToTestResults(&gpuTransferFromCallsTimes, results,
-                                      "GPU to CPU Transfer Calls Time", false,
-                                      reportAsBandwidth);
-  createResultDataAndAddToTestResults(&gpuLaunchAndTransferCallsTimes, results,
-                                      "GPU Launch and Transfer Calls Time",
-                                      false, reportAsBandwidth);
-  createResultDataAndAddToTestResults(&gpuLaunchTransferSyncTimes, results,
-                                      "GPU Launch Transfer and Sync Time",
-                                      false, reportAsBandwidth);
-  createResultDataAndAddToTestResults(
-      &cpuAccessTimes, results, "CPU Access Time", false, reportAsBandwidth);
-  createResultDataAndAddToTestResults(&overallTimes, results, "Overall Time",
-                                      false, reportAsBandwidth);
-
-  printf("Running ");
-  for (matrixDim = minMatrixDim, j = 0;
-       matrixDim * matrixDim <= maxSize / sizeof(float);
-       matrixDim *= multiplierDim, ++j) {
-    sizesToTest[j] = matrixDim * matrixDim * sizeof(float);
-    for (i = MEMALLOC_TYPE_START; i <= MEMALLOC_TYPE_END; i++) {
-      printf(".");
-      fflush(stdout);
-      runMatrixMultiplyKernel(
-          matrixDim, i, numKernelRuns,
-          getPtrRunTimesInMs(gpuLaunchCallsTimes, i, j),
-          getPtrRunTimesInMs(gpuTransferToCallsTimes, i, j),
-          getPtrRunTimesInMs(gpuTransferFromCallsTimes, i, j),
-          getPtrRunTimesInMs(gpuLaunchAndTransferCallsTimes, i, j),
-          getPtrRunTimesInMs(gpuLaunchTransferSyncTimes, i, j),
-          getPtrRunTimesInMs(cpuAccessTimes, i, j),
-          getPtrRunTimesInMs(overallTimes, i, j), device_id);
-    }
-  }
-  printf("\n");
-  printResults(results, print_launch_transfer_results, print_std_deviation);
-  freeTestResultsAndAllResultsData(results);
-}
-
-static void usage() {
-  printf(
-      "./cudaMemoryTypesPerf [-device=<device_id>] [-reportAsBandwidth] "
-      "[-print-launch-transfer-results] [-print-std-deviation] [-verbose]\n");
-  printf("Options:\n");
-  printf(
-      "-reportAsBandwidth:             By default time taken is printed, this "
-      "option allows to instead print bandwidth.\n");
-  printf(
-      "-print-launch-transfer-results: By default overall results are printed, "
-      "this option allows to print data transfers and kernel time as well.\n");
-  printf(
-      "-print-std-deviation:           Prints std deviation of the results.\n");
-  printf(
-      "-kernel-iterations=<num>:       Number of times the kernel tests should "
-      "be run[default is 100 iterations].\n");
-  printf(
-      "-device=<device_id>:            Allows to pass GPU Device ID on which "
-      "the tests will be run.\n");
-  printf("-verbose:                       Prints highly verbose output.\n");
-}
-
-int main(int argc, char **argv) {
-  bool reportAsBandwidth = false;
-  bool print_launch_transfer_results = false;
-  bool print_std_deviation = false;
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "help") ||
-      checkCmdLineFlag(argc, (const char **)argv, "h")) {
-    usage();
-    printf("&&&& %s WAIVED\n", argv[0]);
-    exit(EXIT_WAIVED);
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "reportAsBandwidth")) {
-    reportAsBandwidth = true;
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv,
-                       "print-launch-transfer-results")) {
-    print_launch_transfer_results = true;
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "print-std-deviation")) {
-    print_std_deviation = true;
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "kernel-iterations")) {
-    numKernelRuns =
-        getCmdLineArgumentInt(argc, (const char **)argv, "kernel-iterations");
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "verbose")) {
-    verboseResults = 1;
-  }
-
-  int device_id = findCudaDevice(argc, (const char **)argv);
-
-  matrixMultiplyPerfRunner(reportAsBandwidth, print_launch_transfer_results,
-                           print_std_deviation, device_id);
-
-  printf(
-      "\nNOTE: The CUDA Samples are not meant for performance measurements. "
-      "Results may vary when GPU Boost is enabled.\n");
-  exit(EXIT_SUCCESS);
-}
-{
-    numKernelRuns =
-        getCmdLineArgumentInt(argc, (const char **)argv, "kernel-iterations");
-  }
-
-  if (checkCmdLineFlag(argc, (const char **)argv, "verbose")) {
-    verboseResults = 1;
-  }
